nohup: ignoring input
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:45:41  loss: 0.5769 (0.5769)  acc1: 89.5833 (89.5833)  acc5: 97.9167 (97.9167)  time: 5.2628  data: 1.1128  max mem: 16225
Test:  [ 10/521]  eta: 0:30:04  loss: 0.5769 (0.6858)  acc1: 89.5833 (86.5530)  acc5: 97.9167 (96.9697)  time: 3.5321  data: 0.1014  max mem: 16226
Test:  [ 20/521]  eta: 0:28:49  loss: 0.8459 (0.8663)  acc1: 81.2500 (81.6468)  acc5: 95.8333 (95.3869)  time: 3.3622  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:01  loss: 1.0805 (0.9779)  acc1: 76.0417 (78.7634)  acc5: 92.7083 (94.2204)  time: 3.3669  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:27:20  loss: 1.1603 (1.0473)  acc1: 70.8333 (76.3211)  acc5: 91.6667 (93.8262)  time: 3.3681  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:26:44  loss: 0.7606 (0.9742)  acc1: 84.3750 (78.6152)  acc5: 95.8333 (94.2810)  time: 3.3758  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:08  loss: 0.7259 (0.9649)  acc1: 85.4167 (79.2691)  acc5: 95.8333 (94.2623)  time: 3.3825  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:33  loss: 0.8552 (0.9655)  acc1: 82.2917 (79.1960)  acc5: 94.7917 (94.1608)  time: 3.3817  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:24:58  loss: 0.7517 (0.9415)  acc1: 82.2917 (79.8225)  acc5: 95.8333 (94.3930)  time: 3.3840  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:24  loss: 1.0319 (0.9798)  acc1: 76.0417 (78.5829)  acc5: 94.7917 (94.1735)  time: 3.3917  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:23:50  loss: 1.2399 (1.0124)  acc1: 68.7500 (77.4546)  acc5: 92.7083 (94.0903)  time: 3.3981  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:16  loss: 1.1789 (1.0176)  acc1: 69.7917 (77.1866)  acc5: 94.7917 (94.1817)  time: 3.3996  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:42  loss: 1.0264 (1.0239)  acc1: 73.9583 (77.0489)  acc5: 93.7500 (94.0685)  time: 3.3992  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:08  loss: 1.1433 (1.0319)  acc1: 72.9167 (76.4790)  acc5: 93.7500 (94.1794)  time: 3.3963  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:21:34  loss: 1.0055 (1.0232)  acc1: 75.0000 (76.7287)  acc5: 95.8333 (94.2819)  time: 3.3962  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:00  loss: 0.9547 (1.0332)  acc1: 79.1667 (76.4073)  acc5: 94.7917 (94.2743)  time: 3.3963  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:26  loss: 1.0760 (1.0246)  acc1: 77.0833 (76.6887)  acc5: 94.7917 (94.3129)  time: 3.3984  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:19:52  loss: 0.8463 (1.0168)  acc1: 81.2500 (76.8640)  acc5: 94.7917 (94.3348)  time: 3.4003  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:18  loss: 0.8907 (1.0098)  acc1: 81.2500 (77.1179)  acc5: 94.7917 (94.4003)  time: 3.4017  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:44  loss: 0.9013 (1.0074)  acc1: 79.1667 (77.1161)  acc5: 94.7917 (94.4372)  time: 3.4010  data: 0.0003  max mem: 16226
Test:  [200/521]  eta: 0:18:10  loss: 1.0076 (1.0148)  acc1: 76.0417 (76.9486)  acc5: 93.7500 (94.3253)  time: 3.3989  data: 0.0003  max mem: 16226
Test:  [210/521]  eta: 0:17:36  loss: 1.0140 (1.0141)  acc1: 75.0000 (76.9994)  acc5: 92.7083 (94.2684)  time: 3.4018  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:03  loss: 1.2063 (1.0371)  acc1: 69.7917 (76.4753)  acc5: 90.6250 (93.9480)  time: 3.4033  data: 0.0003  max mem: 16226
Test:  [230/521]  eta: 0:16:29  loss: 1.3119 (1.0532)  acc1: 68.7500 (76.0552)  acc5: 89.5833 (93.7094)  time: 3.4069  data: 0.0003  max mem: 16226
Test:  [240/521]  eta: 0:15:55  loss: 1.3527 (1.0720)  acc1: 64.5833 (75.6051)  acc5: 89.5833 (93.4820)  time: 3.4084  data: 0.0003  max mem: 16226
Test:  [250/521]  eta: 0:15:21  loss: 1.4808 (1.0950)  acc1: 64.5833 (75.2158)  acc5: 87.5000 (93.1316)  time: 3.4071  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:47  loss: 1.4920 (1.1153)  acc1: 62.5000 (74.7126)  acc5: 83.3333 (92.8560)  time: 3.4084  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.4920 (1.1371)  acc1: 61.4583 (74.1236)  acc5: 87.5000 (92.6084)  time: 3.4108  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.4679 (1.1479)  acc1: 63.5417 (73.9101)  acc5: 87.5000 (92.4488)  time: 3.4121  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.4526 (1.1579)  acc1: 67.7083 (73.7257)  acc5: 86.4583 (92.2752)  time: 3.4122  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.1088 (1.1548)  acc1: 73.9583 (73.8684)  acc5: 91.6667 (92.2792)  time: 3.4126  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.1105 (1.1704)  acc1: 70.8333 (73.5095)  acc5: 91.6667 (92.0954)  time: 3.4113  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.4197 (1.1748)  acc1: 69.7917 (73.5040)  acc5: 88.5417 (92.0009)  time: 3.4110  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3688 (1.1935)  acc1: 69.7917 (73.0488)  acc5: 90.6250 (91.7516)  time: 3.4111  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.6057 (1.2041)  acc1: 63.5417 (72.8311)  acc5: 85.4167 (91.5903)  time: 3.4105  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.6057 (1.2133)  acc1: 63.5417 (72.5487)  acc5: 85.4167 (91.5005)  time: 3.4164  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5211 (1.2254)  acc1: 64.5833 (72.3194)  acc5: 85.4167 (91.3089)  time: 3.4172  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.5110 (1.2329)  acc1: 66.6667 (72.1474)  acc5: 86.4583 (91.2539)  time: 3.4126  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.4058 (1.2377)  acc1: 69.7917 (72.1484)  acc5: 89.5833 (91.1445)  time: 3.4135  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.4058 (1.2480)  acc1: 66.6667 (71.8750)  acc5: 88.5417 (90.9580)  time: 3.4158  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.5398 (1.2543)  acc1: 65.6250 (71.7893)  acc5: 85.4167 (90.8588)  time: 3.4171  data: 0.0003  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.4818 (1.2608)  acc1: 68.7500 (71.6900)  acc5: 85.4167 (90.7619)  time: 3.4154  data: 0.0003  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.4818 (1.2650)  acc1: 68.7500 (71.6746)  acc5: 85.4167 (90.6992)  time: 3.4152  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.6131 (1.2725)  acc1: 67.7083 (71.4835)  acc5: 88.5417 (90.6298)  time: 3.4162  data: 0.0005  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.7444 (1.2845)  acc1: 59.3750 (71.2042)  acc5: 84.3750 (90.4644)  time: 3.4163  data: 0.0004  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.5783 (1.2893)  acc1: 63.5417 (71.0989)  acc5: 85.4167 (90.3987)  time: 3.4164  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.4870 (1.2922)  acc1: 66.6667 (70.9960)  acc5: 87.5000 (90.3674)  time: 3.4164  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.4062 (1.2978)  acc1: 69.7917 (70.8953)  acc5: 89.5833 (90.2999)  time: 3.4172  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.4062 (1.3024)  acc1: 69.7917 (70.7489)  acc5: 90.6250 (90.2612)  time: 3.4188  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1035 (1.2975)  acc1: 70.8333 (70.8333)  acc5: 92.7083 (90.3386)  time: 3.4182  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0038 (1.2909)  acc1: 77.0833 (70.9851)  acc5: 93.7500 (90.4005)  time: 3.4174  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1576 (1.2960)  acc1: 71.8750 (70.8252)  acc5: 92.7083 (90.3518)  time: 3.4165  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1894 (1.2871)  acc1: 70.8333 (71.0520)  acc5: 91.6667 (90.4320)  time: 3.3930  data: 0.0001  max mem: 16226
Test: Total time: 0:29:34 (3.4068 s / it)
* Acc@1 71.052 Acc@5 90.432 loss 1.287
Accuracy of the network on the 50000 test images: 71.1%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.04, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.04
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:37  loss: 0.5600 (0.5600)  acc1: 89.5833 (89.5833)  acc5: 96.8750 (96.8750)  time: 5.0238  data: 0.9945  max mem: 16225
Test:  [ 10/521]  eta: 0:30:20  loss: 0.5720 (0.6832)  acc1: 89.5833 (86.6477)  acc5: 97.9167 (96.8750)  time: 3.5621  data: 0.0905  max mem: 16226
Test:  [ 20/521]  eta: 0:29:09  loss: 0.8592 (0.8638)  acc1: 82.2917 (81.4980)  acc5: 96.8750 (95.5853)  time: 3.4162  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:23  loss: 1.0664 (0.9733)  acc1: 72.9167 (78.4610)  acc5: 92.7083 (94.4220)  time: 3.4183  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:43  loss: 1.1898 (1.0452)  acc1: 69.7917 (75.9400)  acc5: 91.6667 (93.9278)  time: 3.4221  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:04  loss: 0.7478 (0.9724)  acc1: 85.4167 (78.3088)  acc5: 95.8333 (94.3627)  time: 3.4199  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:28  loss: 0.7312 (0.9653)  acc1: 85.4167 (78.9276)  acc5: 94.7917 (94.3135)  time: 3.4190  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:25:52  loss: 0.8798 (0.9653)  acc1: 83.3333 (79.0053)  acc5: 94.7917 (94.3075)  time: 3.4201  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:16  loss: 0.7499 (0.9407)  acc1: 84.3750 (79.6811)  acc5: 95.8333 (94.5216)  time: 3.4183  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:41  loss: 1.0132 (0.9802)  acc1: 77.0833 (78.4112)  acc5: 94.7917 (94.2537)  time: 3.4179  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:24:05  loss: 1.2327 (1.0139)  acc1: 66.6667 (77.3102)  acc5: 91.6667 (94.0182)  time: 3.4183  data: 0.0001  max mem: 16226
Test:  [110/521]  eta: 0:23:30  loss: 1.1312 (1.0204)  acc1: 69.7917 (77.0458)  acc5: 92.7083 (94.0503)  time: 3.4177  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:22:56  loss: 1.0635 (1.0279)  acc1: 75.0000 (76.8337)  acc5: 92.7083 (93.8964)  time: 3.4179  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:21  loss: 1.1075 (1.0351)  acc1: 71.8750 (76.3200)  acc5: 93.7500 (93.9965)  time: 3.4195  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:46  loss: 1.0052 (1.0261)  acc1: 77.0833 (76.5440)  acc5: 95.8333 (94.1120)  time: 3.4208  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:12  loss: 0.9641 (1.0355)  acc1: 77.0833 (76.2141)  acc5: 95.8333 (94.1018)  time: 3.4212  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:37  loss: 1.0618 (1.0268)  acc1: 76.0417 (76.4752)  acc5: 94.7917 (94.1706)  time: 3.4208  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:03  loss: 0.8067 (1.0184)  acc1: 81.2500 (76.6691)  acc5: 94.7917 (94.2312)  time: 3.4202  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:28  loss: 0.8842 (1.0117)  acc1: 80.2083 (76.8992)  acc5: 94.7917 (94.2507)  time: 3.4196  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:54  loss: 0.9147 (1.0087)  acc1: 80.2083 (76.9579)  acc5: 93.7500 (94.2736)  time: 3.4203  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:20  loss: 0.9671 (1.0163)  acc1: 76.0417 (76.8087)  acc5: 93.7500 (94.1801)  time: 3.4192  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:45  loss: 1.0172 (1.0144)  acc1: 76.0417 (76.8118)  acc5: 91.6667 (94.1301)  time: 3.4178  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:11  loss: 1.1444 (1.0369)  acc1: 69.7917 (76.2349)  acc5: 91.6667 (93.8584)  time: 3.4200  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:37  loss: 1.4054 (1.0528)  acc1: 63.5417 (75.8433)  acc5: 90.6250 (93.6418)  time: 3.4229  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:16:02  loss: 1.4054 (1.0715)  acc1: 63.5417 (75.4106)  acc5: 88.5417 (93.3956)  time: 3.4214  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:28  loss: 1.4773 (1.0941)  acc1: 63.5417 (75.0249)  acc5: 88.5417 (93.0403)  time: 3.4182  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:14:54  loss: 1.6301 (1.1148)  acc1: 63.5417 (74.5370)  acc5: 86.4583 (92.8041)  time: 3.4175  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:19  loss: 1.5499 (1.1370)  acc1: 61.4583 (73.9852)  acc5: 86.4583 (92.5315)  time: 3.4187  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:45  loss: 1.4893 (1.1480)  acc1: 64.5833 (73.7359)  acc5: 86.4583 (92.3784)  time: 3.4199  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:11  loss: 1.4720 (1.1585)  acc1: 68.7500 (73.5646)  acc5: 88.5417 (92.2036)  time: 3.4197  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:36  loss: 1.1565 (1.1551)  acc1: 72.9167 (73.7438)  acc5: 89.5833 (92.2031)  time: 3.4181  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:12:02  loss: 1.1746 (1.1708)  acc1: 72.9167 (73.4023)  acc5: 90.6250 (92.0050)  time: 3.4175  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:28  loss: 1.3774 (1.1752)  acc1: 67.7083 (73.3904)  acc5: 89.5833 (91.8841)  time: 3.4194  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:54  loss: 1.4137 (1.1940)  acc1: 69.7917 (72.9481)  acc5: 89.5833 (91.6383)  time: 3.4202  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:19  loss: 1.6145 (1.2039)  acc1: 64.5833 (72.7670)  acc5: 87.5000 (91.5017)  time: 3.4206  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:45  loss: 1.5911 (1.2130)  acc1: 65.6250 (72.5220)  acc5: 87.5000 (91.4293)  time: 3.4209  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:11  loss: 1.5255 (1.2250)  acc1: 64.5833 (72.2934)  acc5: 86.4583 (91.2540)  time: 3.4205  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:36  loss: 1.5151 (1.2324)  acc1: 66.6667 (72.1474)  acc5: 86.4583 (91.1781)  time: 3.4210  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:02  loss: 1.4006 (1.2370)  acc1: 69.7917 (72.1429)  acc5: 89.5833 (91.0679)  time: 3.4211  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:28  loss: 1.4103 (1.2477)  acc1: 65.6250 (71.8857)  acc5: 87.5000 (90.9047)  time: 3.4224  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:54  loss: 1.5189 (1.2538)  acc1: 66.6667 (71.8205)  acc5: 85.4167 (90.8146)  time: 3.4231  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:20  loss: 1.4781 (1.2603)  acc1: 67.7083 (71.7052)  acc5: 85.4167 (90.7086)  time: 3.4216  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:45  loss: 1.4885 (1.2644)  acc1: 67.7083 (71.6647)  acc5: 85.4167 (90.6448)  time: 3.4221  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:11  loss: 1.5232 (1.2719)  acc1: 65.6250 (71.4786)  acc5: 87.5000 (90.5742)  time: 3.4210  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:37  loss: 1.6919 (1.2841)  acc1: 61.4583 (71.2089)  acc5: 84.3750 (90.3982)  time: 3.4193  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:03  loss: 1.5435 (1.2883)  acc1: 65.6250 (71.1197)  acc5: 86.4583 (90.3432)  time: 3.4202  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:28  loss: 1.4732 (1.2910)  acc1: 65.6250 (71.0005)  acc5: 87.5000 (90.3222)  time: 3.4196  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:54  loss: 1.3493 (1.2967)  acc1: 66.6667 (70.8908)  acc5: 89.5833 (90.2446)  time: 3.4206  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.3493 (1.3013)  acc1: 69.7917 (70.7467)  acc5: 89.5833 (90.2244)  time: 3.4214  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:46  loss: 1.1496 (1.2967)  acc1: 70.8333 (70.8312)  acc5: 92.7083 (90.2940)  time: 3.4209  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0404 (1.2902)  acc1: 78.1250 (70.9810)  acc5: 93.7500 (90.3630)  time: 3.4209  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1443 (1.2953)  acc1: 69.7917 (70.8068)  acc5: 92.7083 (90.3233)  time: 3.4191  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2096 (1.2863)  acc1: 70.8333 (71.0440)  acc5: 91.6667 (90.4080)  time: 3.3959  data: 0.0001  max mem: 16226
Test: Total time: 0:29:42 (3.4222 s / it)
* Acc@1 71.044 Acc@5 90.408 loss 1.286
Accuracy of the network on the 50000 test images: 71.0%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.05, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.05
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:59  loss: 0.5493 (0.5493)  acc1: 89.5833 (89.5833)  acc5: 97.9167 (97.9167)  time: 5.0667  data: 1.1461  max mem: 16225
Test:  [ 10/521]  eta: 0:30:21  loss: 0.5851 (0.6819)  acc1: 89.5833 (86.3636)  acc5: 97.9167 (97.0644)  time: 3.5640  data: 0.1043  max mem: 16226
Test:  [ 20/521]  eta: 0:29:11  loss: 0.8179 (0.8630)  acc1: 83.3333 (81.7460)  acc5: 94.7917 (95.2381)  time: 3.4167  data: 0.0001  max mem: 16226
Test:  [ 30/521]  eta: 0:28:24  loss: 1.0553 (0.9769)  acc1: 73.9583 (78.7634)  acc5: 91.6667 (94.0860)  time: 3.4194  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:43  loss: 1.2247 (1.0478)  acc1: 69.7917 (76.5498)  acc5: 91.6667 (93.4959)  time: 3.4191  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:04  loss: 0.7467 (0.9765)  acc1: 86.4583 (78.8399)  acc5: 94.7917 (93.8930)  time: 3.4181  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:27  loss: 0.7506 (0.9690)  acc1: 84.3750 (79.2350)  acc5: 95.8333 (93.8866)  time: 3.4163  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:25:51  loss: 0.8612 (0.9684)  acc1: 83.3333 (79.2547)  acc5: 94.7917 (93.8967)  time: 3.4160  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:15  loss: 0.7337 (0.9420)  acc1: 83.3333 (80.0026)  acc5: 95.8333 (94.2130)  time: 3.4162  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:24:40  loss: 1.0338 (0.9814)  acc1: 76.0417 (78.6973)  acc5: 94.7917 (93.9675)  time: 3.4151  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:05  loss: 1.2777 (1.0155)  acc1: 66.6667 (77.5681)  acc5: 92.7083 (93.8016)  time: 3.4150  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:30  loss: 1.1276 (1.0221)  acc1: 68.7500 (77.3836)  acc5: 93.7500 (93.8438)  time: 3.4204  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:22:55  loss: 1.0376 (1.0277)  acc1: 76.0417 (77.2297)  acc5: 93.7500 (93.7500)  time: 3.4209  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:21  loss: 1.1338 (1.0357)  acc1: 73.9583 (76.5903)  acc5: 94.7917 (93.8772)  time: 3.4182  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:21:46  loss: 1.0116 (1.0266)  acc1: 76.0417 (76.7435)  acc5: 95.8333 (94.0086)  time: 3.4177  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:11  loss: 0.9690 (1.0362)  acc1: 80.2083 (76.4211)  acc5: 95.8333 (93.9983)  time: 3.4174  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:37  loss: 1.0429 (1.0273)  acc1: 77.0833 (76.6951)  acc5: 94.7917 (94.0994)  time: 3.4180  data: 0.0004  max mem: 16226
Test:  [170/521]  eta: 0:20:03  loss: 0.8095 (1.0192)  acc1: 82.2917 (76.9128)  acc5: 94.7917 (94.1399)  time: 3.4199  data: 0.0005  max mem: 16226
Test:  [180/521]  eta: 0:19:28  loss: 0.8505 (1.0126)  acc1: 80.2083 (77.0661)  acc5: 94.7917 (94.2162)  time: 3.4199  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:54  loss: 0.9372 (1.0102)  acc1: 78.1250 (77.0833)  acc5: 94.7917 (94.2463)  time: 3.4171  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:19  loss: 1.0058 (1.0168)  acc1: 76.0417 (76.9486)  acc5: 92.7083 (94.1542)  time: 3.4178  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:45  loss: 1.0196 (1.0165)  acc1: 76.0417 (76.9204)  acc5: 91.6667 (94.1005)  time: 3.4199  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:11  loss: 1.1520 (1.0392)  acc1: 67.7083 (76.3716)  acc5: 91.6667 (93.8113)  time: 3.4191  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:36  loss: 1.3546 (1.0554)  acc1: 66.6667 (75.9560)  acc5: 89.5833 (93.5877)  time: 3.4155  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:02  loss: 1.3546 (1.0742)  acc1: 66.6667 (75.5144)  acc5: 89.5833 (93.3394)  time: 3.4151  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:27  loss: 1.4778 (1.0967)  acc1: 66.6667 (75.0830)  acc5: 88.5417 (93.0196)  time: 3.4180  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:53  loss: 1.5345 (1.1178)  acc1: 62.5000 (74.6089)  acc5: 85.4167 (92.7522)  time: 3.4192  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:19  loss: 1.5002 (1.1389)  acc1: 62.5000 (74.1121)  acc5: 85.4167 (92.4700)  time: 3.4182  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:45  loss: 1.4868 (1.1503)  acc1: 62.5000 (73.8508)  acc5: 86.4583 (92.3006)  time: 3.4175  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:10  loss: 1.4556 (1.1599)  acc1: 69.7917 (73.6648)  acc5: 86.4583 (92.1428)  time: 3.4182  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:36  loss: 1.0932 (1.1571)  acc1: 72.9167 (73.8199)  acc5: 90.6250 (92.1200)  time: 3.4176  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:02  loss: 1.1967 (1.1734)  acc1: 71.8750 (73.4593)  acc5: 90.6250 (91.9246)  time: 3.4170  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:27  loss: 1.3837 (1.1783)  acc1: 68.7500 (73.4424)  acc5: 88.5417 (91.8192)  time: 3.4162  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:53  loss: 1.3837 (1.1974)  acc1: 68.7500 (72.9859)  acc5: 89.5833 (91.5786)  time: 3.4148  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:19  loss: 1.6282 (1.2081)  acc1: 63.5417 (72.7456)  acc5: 86.4583 (91.4131)  time: 3.4154  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:45  loss: 1.6282 (1.2168)  acc1: 63.5417 (72.4834)  acc5: 87.5000 (91.3402)  time: 3.4159  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:10  loss: 1.4855 (1.2288)  acc1: 63.5417 (72.2068)  acc5: 86.4583 (91.1790)  time: 3.4181  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:36  loss: 1.4984 (1.2359)  acc1: 65.6250 (72.0547)  acc5: 87.5000 (91.1079)  time: 3.4177  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:08:02  loss: 1.3322 (1.2403)  acc1: 69.7917 (72.0582)  acc5: 89.5833 (91.0105)  time: 3.4177  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:28  loss: 1.4211 (1.2505)  acc1: 65.6250 (71.7871)  acc5: 86.4583 (90.8355)  time: 3.4181  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:54  loss: 1.5115 (1.2571)  acc1: 66.6667 (71.7321)  acc5: 85.4167 (90.7393)  time: 3.4179  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:19  loss: 1.5472 (1.2635)  acc1: 67.7083 (71.6266)  acc5: 85.4167 (90.6225)  time: 3.4182  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:45  loss: 1.5472 (1.2677)  acc1: 67.7083 (71.6177)  acc5: 86.4583 (90.5532)  time: 3.4183  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:11  loss: 1.5632 (1.2758)  acc1: 67.7083 (71.4061)  acc5: 87.5000 (90.4824)  time: 3.4201  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:37  loss: 1.7814 (1.2883)  acc1: 59.3750 (71.1050)  acc5: 84.3750 (90.3108)  time: 3.4173  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:02  loss: 1.6216 (1.2924)  acc1: 63.5417 (71.0066)  acc5: 86.4583 (90.2555)  time: 3.4159  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:28  loss: 1.4478 (1.2953)  acc1: 65.6250 (70.9056)  acc5: 88.5417 (90.2296)  time: 3.4185  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:54  loss: 1.3444 (1.3009)  acc1: 66.6667 (70.8068)  acc5: 88.5417 (90.1495)  time: 3.4178  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.3444 (1.3055)  acc1: 69.7917 (70.6558)  acc5: 89.5833 (90.1117)  time: 3.4181  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:46  loss: 1.1229 (1.3006)  acc1: 70.8333 (70.7739)  acc5: 92.7083 (90.1965)  time: 3.4172  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0198 (1.2939)  acc1: 77.0833 (70.9352)  acc5: 94.7917 (90.2715)  time: 3.4158  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1344 (1.2991)  acc1: 72.9167 (70.7640)  acc5: 92.7083 (90.2132)  time: 3.4157  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1649 (1.2896)  acc1: 71.8750 (71.0020)  acc5: 90.6250 (90.3080)  time: 3.3947  data: 0.0001  max mem: 16226
Test: Total time: 0:29:41 (3.4200 s / it)
* Acc@1 71.002 Acc@5 90.308 loss 1.290
Accuracy of the network on the 50000 test images: 71.0%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.06, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.06
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:45:05  loss: 0.5033 (0.5033)  acc1: 91.6667 (91.6667)  acc5: 97.9167 (97.9167)  time: 5.1936  data: 1.1633  max mem: 16225
Test:  [ 10/521]  eta: 0:30:29  loss: 0.5695 (0.6642)  acc1: 91.6667 (87.7841)  acc5: 96.8750 (96.6856)  time: 3.5809  data: 0.1061  max mem: 16226
Test:  [ 20/521]  eta: 0:29:15  loss: 0.8489 (0.8619)  acc1: 83.3333 (82.1429)  acc5: 95.8333 (95.2877)  time: 3.4191  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:26  loss: 1.0913 (0.9737)  acc1: 72.9167 (79.0659)  acc5: 92.7083 (94.1868)  time: 3.4178  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:45  loss: 1.2292 (1.0461)  acc1: 71.8750 (76.4736)  acc5: 91.6667 (93.6230)  time: 3.4179  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:06  loss: 0.7510 (0.9754)  acc1: 87.5000 (78.8399)  acc5: 95.8333 (94.0564)  time: 3.4181  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:29  loss: 0.7365 (0.9662)  acc1: 86.4583 (79.4911)  acc5: 95.8333 (94.0232)  time: 3.4192  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:53  loss: 0.8577 (0.9688)  acc1: 84.3750 (79.4308)  acc5: 94.7917 (94.0581)  time: 3.4193  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:17  loss: 0.7375 (0.9434)  acc1: 84.3750 (80.1440)  acc5: 96.8750 (94.3287)  time: 3.4186  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:41  loss: 1.0265 (0.9842)  acc1: 76.0417 (78.7431)  acc5: 93.7500 (94.0018)  time: 3.4192  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:06  loss: 1.2650 (1.0186)  acc1: 65.6250 (77.5887)  acc5: 92.7083 (93.8635)  time: 3.4194  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:31  loss: 1.1547 (1.0251)  acc1: 67.7083 (77.3086)  acc5: 93.7500 (93.9189)  time: 3.4205  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:22:57  loss: 1.0595 (1.0328)  acc1: 75.0000 (77.1264)  acc5: 93.7500 (93.7758)  time: 3.4219  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:22  loss: 1.1425 (1.0405)  acc1: 71.8750 (76.5585)  acc5: 94.7917 (93.8772)  time: 3.4198  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:47  loss: 0.9834 (1.0309)  acc1: 77.0833 (76.8174)  acc5: 95.8333 (94.0086)  time: 3.4204  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:13  loss: 0.9590 (1.0402)  acc1: 78.1250 (76.4694)  acc5: 95.8333 (94.0052)  time: 3.4224  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:38  loss: 1.0264 (1.0305)  acc1: 77.0833 (76.7792)  acc5: 94.7917 (94.1059)  time: 3.4194  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:20:03  loss: 0.8286 (1.0228)  acc1: 83.3333 (76.9615)  acc5: 94.7917 (94.1277)  time: 3.4193  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:29  loss: 0.8830 (1.0164)  acc1: 82.2917 (77.2042)  acc5: 94.7917 (94.1644)  time: 3.4202  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:54  loss: 0.9370 (1.0140)  acc1: 80.2083 (77.2197)  acc5: 94.7917 (94.2027)  time: 3.4192  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:20  loss: 0.9937 (1.0213)  acc1: 76.0417 (77.0626)  acc5: 93.7500 (94.0765)  time: 3.4189  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:46  loss: 1.0170 (1.0193)  acc1: 76.0417 (77.0340)  acc5: 92.7083 (94.0462)  time: 3.4191  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:11  loss: 1.1525 (1.0421)  acc1: 67.7083 (76.4470)  acc5: 92.7083 (93.7500)  time: 3.4197  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:37  loss: 1.3444 (1.0587)  acc1: 63.5417 (75.9921)  acc5: 88.5417 (93.5336)  time: 3.4201  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:02  loss: 1.3686 (1.0786)  acc1: 63.5417 (75.4884)  acc5: 88.5417 (93.2573)  time: 3.4192  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:28  loss: 1.5070 (1.1010)  acc1: 65.6250 (75.0996)  acc5: 87.5000 (92.9075)  time: 3.4201  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:54  loss: 1.5486 (1.1216)  acc1: 62.5000 (74.5889)  acc5: 86.4583 (92.6525)  time: 3.4234  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:20  loss: 1.4893 (1.1430)  acc1: 61.4583 (74.0275)  acc5: 86.4583 (92.3970)  time: 3.4223  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:45  loss: 1.4782 (1.1543)  acc1: 61.4583 (73.7396)  acc5: 87.5000 (92.2450)  time: 3.4195  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:11  loss: 1.4462 (1.1636)  acc1: 67.7083 (73.5574)  acc5: 87.5000 (92.1105)  time: 3.4191  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:37  loss: 1.1143 (1.1601)  acc1: 75.0000 (73.7507)  acc5: 91.6667 (92.1131)  time: 3.4176  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:12:02  loss: 1.1438 (1.1757)  acc1: 72.9167 (73.3789)  acc5: 91.6667 (91.9279)  time: 3.4172  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:28  loss: 1.3456 (1.1800)  acc1: 67.7083 (73.3385)  acc5: 89.5833 (91.8289)  time: 3.4190  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:54  loss: 1.3456 (1.1996)  acc1: 68.7500 (72.9167)  acc5: 89.5833 (91.5817)  time: 3.4192  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:19  loss: 1.6572 (1.2107)  acc1: 62.5000 (72.6662)  acc5: 86.4583 (91.4376)  time: 3.4214  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:45  loss: 1.6572 (1.2193)  acc1: 62.5000 (72.4240)  acc5: 88.5417 (91.3551)  time: 3.4223  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:11  loss: 1.5012 (1.2313)  acc1: 64.5833 (72.1549)  acc5: 86.4583 (91.1877)  time: 3.4194  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:37  loss: 1.5279 (1.2385)  acc1: 66.6667 (72.0182)  acc5: 88.5417 (91.1360)  time: 3.4192  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:08:02  loss: 1.3944 (1.2429)  acc1: 66.6667 (71.9980)  acc5: 90.6250 (91.0488)  time: 3.4201  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:28  loss: 1.4260 (1.2534)  acc1: 62.5000 (71.7125)  acc5: 88.5417 (90.8808)  time: 3.4201  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:54  loss: 1.5074 (1.2598)  acc1: 66.6667 (71.6568)  acc5: 86.4583 (90.7861)  time: 3.4197  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:20  loss: 1.4921 (1.2662)  acc1: 67.7083 (71.5481)  acc5: 86.4583 (90.6732)  time: 3.4215  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:45  loss: 1.4921 (1.2707)  acc1: 65.6250 (71.5138)  acc5: 85.4167 (90.5879)  time: 3.4225  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:11  loss: 1.5687 (1.2793)  acc1: 65.6250 (71.3095)  acc5: 85.4167 (90.4776)  time: 3.4221  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:37  loss: 1.7802 (1.2916)  acc1: 59.3750 (71.0317)  acc5: 84.3750 (90.2896)  time: 3.4212  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:03  loss: 1.6208 (1.2960)  acc1: 62.5000 (70.9303)  acc5: 87.5000 (90.2508)  time: 3.4190  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:28  loss: 1.4824 (1.2985)  acc1: 65.6250 (70.8107)  acc5: 89.5833 (90.2296)  time: 3.4190  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:54  loss: 1.3748 (1.3044)  acc1: 67.7083 (70.7029)  acc5: 88.5417 (90.1561)  time: 3.4201  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.3748 (1.3093)  acc1: 69.7917 (70.5345)  acc5: 90.6250 (90.1421)  time: 3.4183  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:46  loss: 1.1125 (1.3044)  acc1: 70.8333 (70.6275)  acc5: 93.7500 (90.2219)  time: 3.4175  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0299 (1.2978)  acc1: 75.0000 (70.7834)  acc5: 93.7500 (90.2903)  time: 3.4185  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1605 (1.3031)  acc1: 72.9167 (70.6071)  acc5: 92.7083 (90.2418)  time: 3.4172  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2446 (1.2937)  acc1: 69.7917 (70.8440)  acc5: 91.6667 (90.3340)  time: 3.3943  data: 0.0001  max mem: 16226
Test: Total time: 0:29:43 (3.4223 s / it)
* Acc@1 70.844 Acc@5 90.334 loss 1.294
Accuracy of the network on the 50000 test images: 70.8%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.07, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.07
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:52  loss: 0.5134 (0.5134)  acc1: 90.6250 (90.6250)  acc5: 98.9583 (98.9583)  time: 5.0519  data: 1.0847  max mem: 16225
Test:  [ 10/521]  eta: 0:30:21  loss: 0.5476 (0.6800)  acc1: 90.6250 (86.9318)  acc5: 97.9167 (97.0644)  time: 3.5653  data: 0.0987  max mem: 16226
Test:  [ 20/521]  eta: 0:29:10  loss: 0.8152 (0.8700)  acc1: 83.3333 (81.2996)  acc5: 95.8333 (95.6845)  time: 3.4159  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:23  loss: 1.0671 (0.9798)  acc1: 73.9583 (78.4610)  acc5: 91.6667 (94.3212)  time: 3.4158  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:42  loss: 1.2520 (1.0530)  acc1: 69.7917 (76.1179)  acc5: 91.6667 (93.7500)  time: 3.4165  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:04  loss: 0.7685 (0.9813)  acc1: 83.3333 (78.4518)  acc5: 94.7917 (94.1176)  time: 3.4190  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:28  loss: 0.7565 (0.9716)  acc1: 83.3333 (78.9788)  acc5: 94.7917 (94.1598)  time: 3.4224  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:52  loss: 0.8430 (0.9741)  acc1: 83.3333 (78.8732)  acc5: 94.7917 (94.0874)  time: 3.4253  data: 0.0003  max mem: 16226
Test:  [ 80/521]  eta: 0:25:16  loss: 0.7262 (0.9480)  acc1: 84.3750 (79.6811)  acc5: 95.8333 (94.3287)  time: 3.4240  data: 0.0003  max mem: 16226
Test:  [ 90/521]  eta: 0:24:41  loss: 0.9628 (0.9869)  acc1: 77.0833 (78.4799)  acc5: 93.7500 (94.0133)  time: 3.4208  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:06  loss: 1.2430 (1.0208)  acc1: 67.7083 (77.3824)  acc5: 91.6667 (93.8119)  time: 3.4217  data: 0.0001  max mem: 16226
Test:  [110/521]  eta: 0:23:31  loss: 1.1854 (1.0284)  acc1: 71.8750 (77.1584)  acc5: 93.7500 (93.8814)  time: 3.4245  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:22:57  loss: 1.0856 (1.0355)  acc1: 73.9583 (76.9456)  acc5: 93.7500 (93.7070)  time: 3.4237  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:22  loss: 1.1903 (1.0437)  acc1: 71.8750 (76.3836)  acc5: 94.7917 (93.8534)  time: 3.4245  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:47  loss: 0.9935 (1.0356)  acc1: 79.1667 (76.6031)  acc5: 94.7917 (93.9642)  time: 3.4240  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:13  loss: 0.9697 (1.0444)  acc1: 79.1667 (76.2693)  acc5: 94.7917 (93.9846)  time: 3.4222  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:38  loss: 1.0519 (1.0350)  acc1: 76.0417 (76.4687)  acc5: 94.7917 (94.0929)  time: 3.4233  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:04  loss: 0.7854 (1.0265)  acc1: 81.2500 (76.6204)  acc5: 95.8333 (94.1399)  time: 3.4215  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:29  loss: 0.8423 (1.0200)  acc1: 81.2500 (76.8531)  acc5: 94.7917 (94.1874)  time: 3.4212  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:18:55  loss: 0.9420 (1.0178)  acc1: 79.1667 (76.8815)  acc5: 94.7917 (94.2408)  time: 3.4205  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:20  loss: 1.0280 (1.0251)  acc1: 76.0417 (76.7154)  acc5: 93.7500 (94.1439)  time: 3.4205  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:46  loss: 1.0280 (1.0240)  acc1: 75.0000 (76.6736)  acc5: 92.7083 (94.0906)  time: 3.4217  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:12  loss: 1.1696 (1.0469)  acc1: 66.6667 (76.0935)  acc5: 90.6250 (93.7877)  time: 3.4248  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:37  loss: 1.3472 (1.0629)  acc1: 63.5417 (75.6719)  acc5: 88.5417 (93.5741)  time: 3.4247  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:03  loss: 1.3526 (1.0820)  acc1: 64.5833 (75.2377)  acc5: 88.5417 (93.3091)  time: 3.4224  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:29  loss: 1.4937 (1.1045)  acc1: 63.5417 (74.8340)  acc5: 86.4583 (92.9573)  time: 3.4229  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:54  loss: 1.5798 (1.1257)  acc1: 61.4583 (74.3056)  acc5: 85.4167 (92.6924)  time: 3.4217  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:20  loss: 1.4966 (1.1470)  acc1: 60.4167 (73.7623)  acc5: 86.4583 (92.4508)  time: 3.4214  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:46  loss: 1.4804 (1.1580)  acc1: 62.5000 (73.5135)  acc5: 87.5000 (92.3080)  time: 3.4215  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:11  loss: 1.4784 (1.1678)  acc1: 68.7500 (73.3247)  acc5: 87.5000 (92.1320)  time: 3.4215  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:37  loss: 1.0776 (1.1645)  acc1: 72.9167 (73.5084)  acc5: 89.5833 (92.1373)  time: 3.4252  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:12:03  loss: 1.1317 (1.1817)  acc1: 71.8750 (73.1210)  acc5: 90.6250 (91.9078)  time: 3.4246  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:28  loss: 1.3503 (1.1858)  acc1: 68.7500 (73.1373)  acc5: 89.5833 (91.8354)  time: 3.4221  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:54  loss: 1.3503 (1.2058)  acc1: 68.7500 (72.7058)  acc5: 89.5833 (91.5502)  time: 3.4228  data: 0.0003  max mem: 16226
Test:  [340/521]  eta: 0:10:20  loss: 1.5858 (1.2162)  acc1: 61.4583 (72.4615)  acc5: 86.4583 (91.3979)  time: 3.4223  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:45  loss: 1.5781 (1.2251)  acc1: 61.4583 (72.2074)  acc5: 88.5417 (91.3343)  time: 3.4242  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:11  loss: 1.5316 (1.2378)  acc1: 64.5833 (71.9443)  acc5: 86.4583 (91.1386)  time: 3.4244  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:37  loss: 1.5264 (1.2447)  acc1: 64.5833 (71.7964)  acc5: 86.4583 (91.0686)  time: 3.4225  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:03  loss: 1.3902 (1.2492)  acc1: 68.7500 (71.7820)  acc5: 89.5833 (90.9750)  time: 3.4236  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:28  loss: 1.4028 (1.2599)  acc1: 65.6250 (71.5074)  acc5: 88.5417 (90.8062)  time: 3.4234  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:54  loss: 1.6029 (1.2669)  acc1: 64.5833 (71.4074)  acc5: 85.4167 (90.7081)  time: 3.4233  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:20  loss: 1.5042 (1.2730)  acc1: 66.6667 (71.3149)  acc5: 85.4167 (90.6047)  time: 3.4236  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:46  loss: 1.5042 (1.2774)  acc1: 67.7083 (71.2935)  acc5: 86.4583 (90.5335)  time: 3.4237  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:11  loss: 1.6009 (1.2853)  acc1: 67.7083 (71.0992)  acc5: 86.4583 (90.4582)  time: 3.4248  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:37  loss: 1.8252 (1.2979)  acc1: 62.5000 (70.8239)  acc5: 84.3750 (90.2849)  time: 3.4243  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:03  loss: 1.6295 (1.3022)  acc1: 63.5417 (70.7456)  acc5: 86.4583 (90.2393)  time: 3.4240  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:28  loss: 1.4832 (1.3049)  acc1: 66.6667 (70.6480)  acc5: 87.5000 (90.2138)  time: 3.4252  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:54  loss: 1.3334 (1.3103)  acc1: 67.7083 (70.5679)  acc5: 88.5417 (90.1362)  time: 3.4240  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.3334 (1.3150)  acc1: 70.8333 (70.4067)  acc5: 88.5417 (90.1161)  time: 3.4222  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:46  loss: 1.1530 (1.3099)  acc1: 71.8750 (70.5130)  acc5: 92.7083 (90.2049)  time: 3.4221  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0050 (1.3033)  acc1: 75.0000 (70.6649)  acc5: 94.7917 (90.2819)  time: 3.4214  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1705 (1.3090)  acc1: 71.8750 (70.4949)  acc5: 92.7083 (90.2153)  time: 3.4207  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2453 (1.2998)  acc1: 70.8333 (70.7240)  acc5: 90.6250 (90.2980)  time: 3.3993  data: 0.0001  max mem: 16226
Test: Total time: 0:29:44 (3.4248 s / it)
* Acc@1 70.724 Acc@5 90.298 loss 1.300
Accuracy of the network on the 50000 test images: 70.7%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.08, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.08
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:17  loss: 0.5187 (0.5187)  acc1: 89.5833 (89.5833)  acc5: 97.9167 (97.9167)  time: 5.1003  data: 1.0849  max mem: 16225
Test:  [ 10/521]  eta: 0:30:24  loss: 0.5649 (0.6844)  acc1: 90.6250 (86.7424)  acc5: 97.9167 (96.8750)  time: 3.5714  data: 0.0989  max mem: 16226
Test:  [ 20/521]  eta: 0:29:12  loss: 0.8699 (0.8818)  acc1: 80.2083 (81.4484)  acc5: 94.7917 (94.9405)  time: 3.4186  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:25  loss: 1.0918 (0.9931)  acc1: 72.9167 (78.4946)  acc5: 92.7083 (93.8508)  time: 3.4184  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:43  loss: 1.2094 (1.0618)  acc1: 68.7500 (75.9400)  acc5: 92.7083 (93.3689)  time: 3.4173  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:05  loss: 0.8094 (0.9889)  acc1: 83.3333 (78.2271)  acc5: 94.7917 (93.8317)  time: 3.4163  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:28  loss: 0.7600 (0.9785)  acc1: 83.3333 (78.7568)  acc5: 95.8333 (93.9208)  time: 3.4169  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:51  loss: 0.8725 (0.9815)  acc1: 81.2500 (78.6092)  acc5: 93.7500 (93.9261)  time: 3.4161  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:16  loss: 0.7402 (0.9560)  acc1: 84.3750 (79.3982)  acc5: 95.8333 (94.1358)  time: 3.4168  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:41  loss: 1.0185 (0.9975)  acc1: 76.0417 (78.0334)  acc5: 93.7500 (93.8645)  time: 3.4208  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:05  loss: 1.2722 (1.0323)  acc1: 65.6250 (76.8668)  acc5: 92.7083 (93.6881)  time: 3.4195  data: 0.0001  max mem: 16226
Test:  [110/521]  eta: 0:23:30  loss: 1.1968 (1.0406)  acc1: 70.8333 (76.6141)  acc5: 93.7500 (93.7500)  time: 3.4162  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:22:56  loss: 1.1082 (1.0475)  acc1: 75.0000 (76.5410)  acc5: 93.7500 (93.6467)  time: 3.4178  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:21  loss: 1.1969 (1.0548)  acc1: 71.8750 (75.9701)  acc5: 93.7500 (93.7182)  time: 3.4189  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:46  loss: 1.0205 (1.0447)  acc1: 76.0417 (76.1820)  acc5: 95.8333 (93.8682)  time: 3.4173  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:12  loss: 0.9094 (1.0534)  acc1: 78.1250 (75.8899)  acc5: 95.8333 (93.8535)  time: 3.4194  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:37  loss: 1.0406 (1.0447)  acc1: 78.1250 (76.1840)  acc5: 95.8333 (93.9441)  time: 3.4205  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:03  loss: 0.8540 (1.0361)  acc1: 83.3333 (76.3889)  acc5: 94.7917 (93.9998)  time: 3.4174  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:28  loss: 0.8557 (1.0285)  acc1: 82.2917 (76.6977)  acc5: 93.7500 (94.0608)  time: 3.4188  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:54  loss: 0.9508 (1.0256)  acc1: 79.1667 (76.7288)  acc5: 94.7917 (94.1045)  time: 3.4191  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:19  loss: 0.9934 (1.0327)  acc1: 76.0417 (76.5910)  acc5: 93.7500 (93.9988)  time: 3.4165  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:45  loss: 1.0143 (1.0319)  acc1: 76.0417 (76.5650)  acc5: 91.6667 (93.9129)  time: 3.4174  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:11  loss: 1.1635 (1.0550)  acc1: 68.7500 (76.0417)  acc5: 91.6667 (93.6416)  time: 3.4196  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:36  loss: 1.3479 (1.0705)  acc1: 65.6250 (75.5952)  acc5: 90.6250 (93.4118)  time: 3.4214  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:16:02  loss: 1.3479 (1.0905)  acc1: 63.5417 (75.1037)  acc5: 88.5417 (93.1579)  time: 3.4190  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:28  loss: 1.5475 (1.1128)  acc1: 63.5417 (74.7344)  acc5: 87.5000 (92.8079)  time: 3.4173  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:53  loss: 1.5892 (1.1339)  acc1: 62.5000 (74.2816)  acc5: 87.5000 (92.5407)  time: 3.4194  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:19  loss: 1.5450 (1.1560)  acc1: 60.4167 (73.7162)  acc5: 83.3333 (92.2625)  time: 3.4215  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:45  loss: 1.5259 (1.1676)  acc1: 63.5417 (73.4690)  acc5: 86.4583 (92.1226)  time: 3.4195  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:10  loss: 1.4579 (1.1773)  acc1: 67.7083 (73.3212)  acc5: 86.4583 (91.9495)  time: 3.4158  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:36  loss: 1.1033 (1.1740)  acc1: 73.9583 (73.4877)  acc5: 90.6250 (91.9504)  time: 3.4165  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:12:02  loss: 1.1725 (1.1912)  acc1: 70.8333 (73.0841)  acc5: 91.6667 (91.7504)  time: 3.4169  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:28  loss: 1.3815 (1.1958)  acc1: 67.7083 (73.0627)  acc5: 87.5000 (91.6440)  time: 3.4197  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:53  loss: 1.3815 (1.2155)  acc1: 66.6667 (72.5548)  acc5: 87.5000 (91.4023)  time: 3.4213  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:19  loss: 1.6706 (1.2258)  acc1: 62.5000 (72.3393)  acc5: 87.5000 (91.2696)  time: 3.4198  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:45  loss: 1.6706 (1.2346)  acc1: 64.5833 (72.0976)  acc5: 87.5000 (91.1859)  time: 3.4228  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:11  loss: 1.5283 (1.2468)  acc1: 64.5833 (71.8548)  acc5: 85.4167 (90.9741)  time: 3.4240  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:36  loss: 1.5283 (1.2544)  acc1: 65.6250 (71.6925)  acc5: 87.5000 (90.9002)  time: 3.4202  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:02  loss: 1.4399 (1.2591)  acc1: 68.7500 (71.6699)  acc5: 89.5833 (90.8082)  time: 3.4190  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:28  loss: 1.4399 (1.2698)  acc1: 65.6250 (71.4061)  acc5: 86.4583 (90.6383)  time: 3.4195  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:54  loss: 1.5545 (1.2766)  acc1: 65.6250 (71.3087)  acc5: 84.3750 (90.5419)  time: 3.4173  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:19  loss: 1.5132 (1.2833)  acc1: 66.6667 (71.1882)  acc5: 85.4167 (90.4298)  time: 3.4171  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:45  loss: 1.5102 (1.2875)  acc1: 67.7083 (71.1649)  acc5: 86.4583 (90.3702)  time: 3.4179  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:11  loss: 1.5574 (1.2955)  acc1: 67.7083 (70.9711)  acc5: 87.5000 (90.2746)  time: 3.4170  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:37  loss: 1.7933 (1.3080)  acc1: 57.2917 (70.6869)  acc5: 84.3750 (90.1030)  time: 3.4169  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:02  loss: 1.6323 (1.3125)  acc1: 63.5417 (70.6070)  acc5: 85.4167 (90.0383)  time: 3.4178  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:28  loss: 1.5404 (1.3155)  acc1: 67.7083 (70.5306)  acc5: 87.5000 (90.0149)  time: 3.4186  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:54  loss: 1.4088 (1.3208)  acc1: 67.7083 (70.4485)  acc5: 88.5417 (89.9350)  time: 3.4191  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.4087 (1.3256)  acc1: 67.7083 (70.2876)  acc5: 89.5833 (89.8974)  time: 3.4172  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:46  loss: 1.1956 (1.3207)  acc1: 71.8750 (70.3815)  acc5: 92.7083 (89.9779)  time: 3.4194  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0200 (1.3139)  acc1: 73.9583 (70.5319)  acc5: 94.7917 (90.0574)  time: 3.4205  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.2054 (1.3193)  acc1: 72.9167 (70.3686)  acc5: 92.7083 (90.0033)  time: 3.4174  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1703 (1.3098)  acc1: 71.8750 (70.6100)  acc5: 90.6250 (90.0960)  time: 3.3971  data: 0.0001  max mem: 16226
Test: Total time: 0:29:42 (3.4212 s / it)
* Acc@1 70.610 Acc@5 90.096 loss 1.310
Accuracy of the network on the 50000 test images: 70.6%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:13  loss: 0.5551 (0.5551)  acc1: 89.5833 (89.5833)  acc5: 96.8750 (96.8750)  time: 4.9772  data: 0.9663  max mem: 16225
Test:  [ 10/521]  eta: 0:30:16  loss: 0.6131 (0.6938)  acc1: 89.5833 (86.6477)  acc5: 97.9167 (96.6856)  time: 3.5540  data: 0.0882  max mem: 16226
Test:  [ 20/521]  eta: 0:29:07  loss: 0.8386 (0.8655)  acc1: 83.3333 (81.9444)  acc5: 96.8750 (95.4365)  time: 3.4128  data: 0.0003  max mem: 16226
Test:  [ 30/521]  eta: 0:28:20  loss: 1.0412 (0.9725)  acc1: 72.9167 (79.2339)  acc5: 91.6667 (94.0860)  time: 3.4134  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:39  loss: 1.2520 (1.0426)  acc1: 71.8750 (76.7531)  acc5: 92.7083 (93.7246)  time: 3.4122  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:02  loss: 0.7782 (0.9714)  acc1: 86.4583 (79.1667)  acc5: 94.7917 (94.1381)  time: 3.4158  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:25  loss: 0.7358 (0.9587)  acc1: 86.4583 (79.7985)  acc5: 94.7917 (94.1598)  time: 3.4173  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:25:49  loss: 0.8338 (0.9575)  acc1: 83.3333 (79.7975)  acc5: 93.7500 (94.0728)  time: 3.4142  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:14  loss: 0.7051 (0.9320)  acc1: 83.3333 (80.4141)  acc5: 95.8333 (94.3416)  time: 3.4152  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:24:38  loss: 0.9948 (0.9712)  acc1: 75.0000 (79.0751)  acc5: 93.7500 (94.0934)  time: 3.4137  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:24:03  loss: 1.2373 (1.0045)  acc1: 66.6667 (78.0528)  acc5: 92.7083 (93.9563)  time: 3.4112  data: 0.0001  max mem: 16226
Test:  [110/521]  eta: 0:23:28  loss: 1.1539 (1.0117)  acc1: 69.7917 (77.6652)  acc5: 94.7917 (94.0597)  time: 3.4120  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:22:54  loss: 1.0496 (1.0185)  acc1: 76.0417 (77.5826)  acc5: 94.7917 (93.9566)  time: 3.4124  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:19  loss: 1.0989 (1.0262)  acc1: 73.9583 (77.0038)  acc5: 93.7500 (94.0681)  time: 3.4128  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:21:44  loss: 1.0083 (1.0191)  acc1: 75.0000 (77.1129)  acc5: 95.8333 (94.1637)  time: 3.4139  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:10  loss: 0.9873 (1.0296)  acc1: 76.0417 (76.7591)  acc5: 95.8333 (94.1294)  time: 3.4132  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:35  loss: 1.0917 (1.0210)  acc1: 76.0417 (77.0445)  acc5: 95.8333 (94.2094)  time: 3.4126  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:01  loss: 0.8222 (1.0131)  acc1: 80.2083 (77.1869)  acc5: 95.8333 (94.2617)  time: 3.4134  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:26  loss: 0.8834 (1.0054)  acc1: 80.2083 (77.4459)  acc5: 94.7917 (94.3198)  time: 3.4134  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:18:52  loss: 0.8834 (1.0024)  acc1: 80.2083 (77.4596)  acc5: 94.7917 (94.3663)  time: 3.4142  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:18  loss: 0.9729 (1.0099)  acc1: 78.1250 (77.2803)  acc5: 93.7500 (94.3045)  time: 3.4153  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:43  loss: 1.0047 (1.0084)  acc1: 78.1250 (77.2512)  acc5: 92.7083 (94.2684)  time: 3.4143  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:09  loss: 1.1611 (1.0316)  acc1: 69.7917 (76.7110)  acc5: 91.6667 (94.0092)  time: 3.4142  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:35  loss: 1.3097 (1.0483)  acc1: 63.5417 (76.2762)  acc5: 89.5833 (93.7365)  time: 3.4141  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:01  loss: 1.3763 (1.0666)  acc1: 64.5833 (75.7910)  acc5: 89.5833 (93.4863)  time: 3.4145  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:26  loss: 1.5307 (1.0899)  acc1: 64.5833 (75.3445)  acc5: 86.4583 (93.1109)  time: 3.4165  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:14:52  loss: 1.5843 (1.1114)  acc1: 61.4583 (74.8044)  acc5: 84.3750 (92.8520)  time: 3.4152  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:18  loss: 1.4527 (1.1322)  acc1: 61.4583 (74.2389)  acc5: 86.4583 (92.6276)  time: 3.4131  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:44  loss: 1.4279 (1.1428)  acc1: 61.4583 (73.9954)  acc5: 88.5417 (92.4859)  time: 3.4144  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:09  loss: 1.4583 (1.1525)  acc1: 69.7917 (73.8044)  acc5: 88.5417 (92.3289)  time: 3.4143  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:35  loss: 1.1056 (1.1497)  acc1: 75.0000 (73.9756)  acc5: 90.6250 (92.3069)  time: 3.4126  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:01  loss: 1.1834 (1.1650)  acc1: 75.0000 (73.6837)  acc5: 90.6250 (92.1423)  time: 3.4135  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:27  loss: 1.3789 (1.1698)  acc1: 67.7083 (73.6598)  acc5: 87.5000 (92.0236)  time: 3.4135  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:52  loss: 1.3789 (1.1884)  acc1: 68.7500 (73.2125)  acc5: 89.5833 (91.8083)  time: 3.4135  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:18  loss: 1.5934 (1.1989)  acc1: 63.5417 (72.9717)  acc5: 87.5000 (91.6545)  time: 3.4148  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:44  loss: 1.5934 (1.2080)  acc1: 63.5417 (72.7089)  acc5: 87.5000 (91.5865)  time: 3.4151  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:10  loss: 1.5582 (1.2197)  acc1: 63.5417 (72.4550)  acc5: 86.4583 (91.4329)  time: 3.4135  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:36  loss: 1.5582 (1.2272)  acc1: 65.6250 (72.2709)  acc5: 89.5833 (91.3634)  time: 3.4125  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:08:01  loss: 1.3073 (1.2322)  acc1: 67.7083 (72.2496)  acc5: 90.6250 (91.2648)  time: 3.4135  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:27  loss: 1.4436 (1.2428)  acc1: 63.5417 (71.9656)  acc5: 88.5417 (91.0939)  time: 3.4134  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:53  loss: 1.5147 (1.2487)  acc1: 64.5833 (71.8880)  acc5: 86.4583 (90.9861)  time: 3.4131  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:19  loss: 1.5139 (1.2554)  acc1: 67.7083 (71.7787)  acc5: 86.4583 (90.8506)  time: 3.4148  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:45  loss: 1.5139 (1.2597)  acc1: 67.7083 (71.7364)  acc5: 84.3750 (90.7809)  time: 3.4148  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:10  loss: 1.5650 (1.2672)  acc1: 66.6667 (71.5729)  acc5: 88.5417 (90.7120)  time: 3.4150  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:36  loss: 1.7251 (1.2799)  acc1: 60.4167 (71.2845)  acc5: 84.3750 (90.5234)  time: 3.4144  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:02  loss: 1.5889 (1.2844)  acc1: 63.5417 (71.1729)  acc5: 85.4167 (90.4564)  time: 3.4124  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:28  loss: 1.5007 (1.2878)  acc1: 66.6667 (71.0548)  acc5: 87.5000 (90.4442)  time: 3.4150  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:54  loss: 1.4275 (1.2933)  acc1: 69.7917 (70.9638)  acc5: 88.5417 (90.3662)  time: 3.4153  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.4275 (1.2982)  acc1: 69.7917 (70.8312)  acc5: 89.5833 (90.3456)  time: 3.4143  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1406 (1.2938)  acc1: 71.8750 (70.9203)  acc5: 91.6667 (90.4044)  time: 3.4161  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0056 (1.2870)  acc1: 76.0417 (71.0683)  acc5: 95.8333 (90.4815)  time: 3.4169  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1317 (1.2916)  acc1: 75.0000 (70.8965)  acc5: 92.7083 (90.4415)  time: 3.4140  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2291 (1.2827)  acc1: 72.9167 (71.1000)  acc5: 91.6667 (90.5340)  time: 3.3905  data: 0.0001  max mem: 16226
Test: Total time: 0:29:39 (3.4163 s / it)
* Acc@1 71.100 Acc@5 90.534 loss 1.283
Accuracy of the network on the 50000 test images: 71.1%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.04, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.04
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:44  loss: 0.5639 (0.5639)  acc1: 90.6250 (90.6250)  acc5: 96.8750 (96.8750)  time: 5.0365  data: 1.0723  max mem: 16225
Test:  [ 10/521]  eta: 0:30:20  loss: 0.6125 (0.6948)  acc1: 89.5833 (86.0795)  acc5: 96.8750 (96.4015)  time: 3.5623  data: 0.0979  max mem: 16226
Test:  [ 20/521]  eta: 0:29:10  loss: 0.8830 (0.8754)  acc1: 81.2500 (81.1508)  acc5: 96.8750 (95.2381)  time: 3.4175  data: 0.0003  max mem: 16226
Test:  [ 30/521]  eta: 0:28:23  loss: 1.0295 (0.9842)  acc1: 73.9583 (78.5618)  acc5: 91.6667 (93.9180)  time: 3.4176  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:42  loss: 1.2367 (1.0497)  acc1: 69.7917 (76.0671)  acc5: 91.6667 (93.5468)  time: 3.4149  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:27:04  loss: 0.7435 (0.9754)  acc1: 84.3750 (78.5335)  acc5: 94.7917 (93.9951)  time: 3.4158  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:27  loss: 0.7314 (0.9621)  acc1: 84.3750 (79.1325)  acc5: 94.7917 (94.0061)  time: 3.4175  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:25:51  loss: 0.8138 (0.9627)  acc1: 81.2500 (79.1520)  acc5: 93.7500 (93.9261)  time: 3.4173  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:15  loss: 0.7570 (0.9377)  acc1: 82.2917 (79.9126)  acc5: 95.8333 (94.2515)  time: 3.4153  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:24:39  loss: 0.9606 (0.9767)  acc1: 79.1667 (78.6630)  acc5: 94.7917 (94.0476)  time: 3.4138  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:04  loss: 1.2286 (1.0088)  acc1: 66.6667 (77.6609)  acc5: 92.7083 (93.9872)  time: 3.4131  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:29  loss: 1.1152 (1.0153)  acc1: 67.7083 (77.2335)  acc5: 93.7500 (94.0691)  time: 3.4161  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:55  loss: 1.0337 (1.0211)  acc1: 75.0000 (77.0919)  acc5: 93.7500 (93.9222)  time: 3.4193  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:20  loss: 1.1119 (1.0290)  acc1: 72.9167 (76.4949)  acc5: 94.7917 (94.0124)  time: 3.4173  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:45  loss: 0.9975 (1.0210)  acc1: 76.0417 (76.6031)  acc5: 95.8333 (94.1489)  time: 3.4152  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:11  loss: 0.9713 (1.0317)  acc1: 76.0417 (76.2279)  acc5: 95.8333 (94.1294)  time: 3.4173  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:36  loss: 1.0847 (1.0229)  acc1: 76.0417 (76.5334)  acc5: 95.8333 (94.1964)  time: 3.4189  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:02  loss: 0.8206 (1.0149)  acc1: 82.2917 (76.7178)  acc5: 95.8333 (94.2556)  time: 3.4176  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:28  loss: 0.8458 (1.0083)  acc1: 81.2500 (76.9740)  acc5: 95.8333 (94.3255)  time: 3.4153  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:18:53  loss: 0.9033 (1.0049)  acc1: 80.2083 (77.0233)  acc5: 95.8333 (94.3772)  time: 3.4136  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:19  loss: 0.9796 (1.0121)  acc1: 76.0417 (76.8294)  acc5: 93.7500 (94.3253)  time: 3.4151  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:44  loss: 1.0502 (1.0108)  acc1: 75.0000 (76.8464)  acc5: 93.7500 (94.2338)  time: 3.4180  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:10  loss: 1.1665 (1.0339)  acc1: 70.8333 (76.3245)  acc5: 91.6667 (93.9574)  time: 3.4171  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:36  loss: 1.3226 (1.0510)  acc1: 66.6667 (75.8387)  acc5: 88.5417 (93.7184)  time: 3.4151  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:01  loss: 1.3226 (1.0697)  acc1: 65.6250 (75.4279)  acc5: 88.5417 (93.4474)  time: 3.4152  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:27  loss: 1.5019 (1.0927)  acc1: 65.6250 (75.0166)  acc5: 87.5000 (93.0943)  time: 3.4156  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:14:53  loss: 1.6168 (1.1138)  acc1: 65.6250 (74.5370)  acc5: 84.3750 (92.8201)  time: 3.4147  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:18  loss: 1.4948 (1.1354)  acc1: 62.5000 (73.9968)  acc5: 87.5000 (92.5853)  time: 3.4136  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:44  loss: 1.4864 (1.1464)  acc1: 63.5417 (73.7656)  acc5: 87.5000 (92.4377)  time: 3.4132  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:10  loss: 1.4719 (1.1556)  acc1: 69.7917 (73.6183)  acc5: 87.5000 (92.2752)  time: 3.4137  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:36  loss: 1.0842 (1.1520)  acc1: 72.9167 (73.7853)  acc5: 91.6667 (92.2758)  time: 3.4167  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:01  loss: 1.1292 (1.1683)  acc1: 71.8750 (73.4358)  acc5: 91.6667 (92.0619)  time: 3.4184  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:27  loss: 1.3709 (1.1728)  acc1: 68.7500 (73.4326)  acc5: 88.5417 (91.9457)  time: 3.4185  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:53  loss: 1.3709 (1.1916)  acc1: 68.7500 (72.9607)  acc5: 88.5417 (91.7170)  time: 3.4160  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:19  loss: 1.5730 (1.2016)  acc1: 61.4583 (72.7273)  acc5: 87.5000 (91.5842)  time: 3.4138  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:44  loss: 1.5606 (1.2106)  acc1: 61.4583 (72.4596)  acc5: 87.5000 (91.5242)  time: 3.4147  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:10  loss: 1.5134 (1.2231)  acc1: 64.5833 (72.2011)  acc5: 87.5000 (91.3262)  time: 3.4154  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:36  loss: 1.5134 (1.2303)  acc1: 65.6250 (72.0912)  acc5: 87.5000 (91.2567)  time: 3.4148  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:08:02  loss: 1.3683 (1.2358)  acc1: 71.8750 (72.0418)  acc5: 90.6250 (91.1554)  time: 3.4147  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:28  loss: 1.4619 (1.2459)  acc1: 62.5000 (71.7525)  acc5: 87.5000 (90.9900)  time: 3.4177  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:53  loss: 1.5095 (1.2518)  acc1: 64.5833 (71.6854)  acc5: 86.4583 (90.8952)  time: 3.4164  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:19  loss: 1.4854 (1.2585)  acc1: 68.7500 (71.5557)  acc5: 86.4583 (90.7821)  time: 3.4144  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:45  loss: 1.4854 (1.2629)  acc1: 66.6667 (71.5162)  acc5: 86.4583 (90.7166)  time: 3.4162  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:11  loss: 1.5532 (1.2702)  acc1: 66.6667 (71.3433)  acc5: 87.5000 (90.6395)  time: 3.4157  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:36  loss: 1.7325 (1.2827)  acc1: 60.4167 (71.0365)  acc5: 85.4167 (90.4738)  time: 3.4172  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:02  loss: 1.6066 (1.2872)  acc1: 63.5417 (70.9142)  acc5: 86.4583 (90.4171)  time: 3.4160  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:28  loss: 1.4834 (1.2904)  acc1: 64.5833 (70.7994)  acc5: 87.5000 (90.3832)  time: 3.4150  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:54  loss: 1.4158 (1.2959)  acc1: 67.7083 (70.6962)  acc5: 89.5833 (90.2999)  time: 3.4158  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.4237 (1.3004)  acc1: 67.7083 (70.5605)  acc5: 89.5833 (90.2850)  time: 3.4146  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.0928 (1.2954)  acc1: 69.7917 (70.6445)  acc5: 92.7083 (90.3683)  time: 3.4144  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0111 (1.2882)  acc1: 73.9583 (70.7980)  acc5: 94.7917 (90.4400)  time: 3.4143  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1169 (1.2934)  acc1: 72.9167 (70.6376)  acc5: 92.7083 (90.3967)  time: 3.4164  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2060 (1.2844)  acc1: 71.8750 (70.8600)  acc5: 90.6250 (90.4800)  time: 3.3936  data: 0.0001  max mem: 16226
Test: Total time: 0:29:40 (3.4182 s / it)
* Acc@1 70.860 Acc@5 90.480 loss 1.284
Accuracy of the network on the 50000 test images: 70.9%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.05, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.05
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:22  loss: 0.5282 (0.5282)  acc1: 90.6250 (90.6250)  acc5: 96.8750 (96.8750)  time: 5.1103  data: 1.1362  max mem: 16225
Test:  [ 10/521]  eta: 0:30:26  loss: 0.6421 (0.6846)  acc1: 89.5833 (87.2159)  acc5: 97.9167 (96.9697)  time: 3.5737  data: 0.1036  max mem: 16226
Test:  [ 20/521]  eta: 0:29:12  loss: 0.8239 (0.8627)  acc1: 82.2917 (82.0437)  acc5: 96.8750 (95.6845)  time: 3.4168  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:23  loss: 1.0788 (0.9745)  acc1: 73.9583 (78.8306)  acc5: 91.6667 (94.2876)  time: 3.4129  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:42  loss: 1.2003 (1.0409)  acc1: 69.7917 (76.4736)  acc5: 91.6667 (93.9024)  time: 3.4132  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:03  loss: 0.7635 (0.9681)  acc1: 84.3750 (78.8807)  acc5: 94.7917 (94.2810)  time: 3.4137  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:26  loss: 0.7442 (0.9542)  acc1: 85.4167 (79.5594)  acc5: 94.7917 (94.3135)  time: 3.4134  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:50  loss: 0.8110 (0.9558)  acc1: 82.2917 (79.5041)  acc5: 94.7917 (94.2928)  time: 3.4138  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:14  loss: 0.7261 (0.9308)  acc1: 83.3333 (80.1312)  acc5: 96.8750 (94.5602)  time: 3.4141  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:24:39  loss: 0.9823 (0.9731)  acc1: 75.0000 (78.8233)  acc5: 93.7500 (94.2193)  time: 3.4135  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:04  loss: 1.2452 (1.0048)  acc1: 67.7083 (77.7434)  acc5: 92.7083 (94.1316)  time: 3.4154  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:29  loss: 1.0973 (1.0112)  acc1: 70.8333 (77.5056)  acc5: 93.7500 (94.1723)  time: 3.4161  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:54  loss: 1.0609 (1.0185)  acc1: 76.0417 (77.2813)  acc5: 93.7500 (94.0083)  time: 3.4151  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:20  loss: 1.1085 (1.0265)  acc1: 73.9583 (76.7017)  acc5: 93.7500 (94.1237)  time: 3.4185  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:21:45  loss: 0.9814 (1.0188)  acc1: 76.0417 (76.8026)  acc5: 94.7917 (94.2007)  time: 3.4192  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:11  loss: 0.9200 (1.0301)  acc1: 76.0417 (76.3797)  acc5: 94.7917 (94.1432)  time: 3.4168  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:36  loss: 1.0335 (1.0209)  acc1: 75.0000 (76.6434)  acc5: 94.7917 (94.2223)  time: 3.4149  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:02  loss: 0.8223 (1.0136)  acc1: 81.2500 (76.7788)  acc5: 95.8333 (94.2678)  time: 3.4141  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:27  loss: 0.8773 (1.0062)  acc1: 81.2500 (77.0431)  acc5: 95.8333 (94.3428)  time: 3.4159  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:18:53  loss: 0.8813 (1.0037)  acc1: 80.2083 (77.0779)  acc5: 94.7917 (94.3772)  time: 3.4154  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:18  loss: 0.9850 (1.0114)  acc1: 76.0417 (76.8968)  acc5: 93.7500 (94.2838)  time: 3.4143  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:44  loss: 1.0230 (1.0102)  acc1: 76.0417 (76.9402)  acc5: 92.7083 (94.2190)  time: 3.4159  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:10  loss: 1.1201 (1.0331)  acc1: 70.8333 (76.4470)  acc5: 91.6667 (93.9574)  time: 3.4158  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:35  loss: 1.3421 (1.0499)  acc1: 66.6667 (76.0011)  acc5: 88.5417 (93.7049)  time: 3.4155  data: 0.0004  max mem: 16226
Test:  [240/521]  eta: 0:16:01  loss: 1.3421 (1.0686)  acc1: 63.5417 (75.5187)  acc5: 88.5417 (93.4647)  time: 3.4166  data: 0.0004  max mem: 16226
Test:  [250/521]  eta: 0:15:27  loss: 1.4724 (1.0914)  acc1: 64.5833 (75.1038)  acc5: 86.4583 (93.1067)  time: 3.4165  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:14:53  loss: 1.6175 (1.1130)  acc1: 62.5000 (74.5730)  acc5: 85.4167 (92.8440)  time: 3.4162  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:18  loss: 1.5083 (1.1345)  acc1: 61.4583 (74.0160)  acc5: 88.5417 (92.6007)  time: 3.4156  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:44  loss: 1.4692 (1.1457)  acc1: 64.5833 (73.7767)  acc5: 88.5417 (92.4563)  time: 3.4156  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:10  loss: 1.4692 (1.1557)  acc1: 68.7500 (73.5861)  acc5: 87.5000 (92.3003)  time: 3.4151  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:36  loss: 1.1094 (1.1528)  acc1: 76.0417 (73.7645)  acc5: 90.6250 (92.2965)  time: 3.4143  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:01  loss: 1.1094 (1.1690)  acc1: 70.8333 (73.3923)  acc5: 90.6250 (92.0920)  time: 3.4165  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:27  loss: 1.3587 (1.1736)  acc1: 68.7500 (73.3937)  acc5: 87.5000 (91.9620)  time: 3.4174  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:53  loss: 1.3587 (1.1927)  acc1: 68.7500 (72.9418)  acc5: 88.5417 (91.7139)  time: 3.4169  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:19  loss: 1.5927 (1.2032)  acc1: 61.4583 (72.6631)  acc5: 87.5000 (91.5750)  time: 3.4169  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:44  loss: 1.5927 (1.2123)  acc1: 61.4583 (72.3944)  acc5: 87.5000 (91.5005)  time: 3.4159  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:10  loss: 1.5253 (1.2251)  acc1: 63.5417 (72.1289)  acc5: 85.4167 (91.3118)  time: 3.4170  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:36  loss: 1.5135 (1.2330)  acc1: 65.6250 (71.9648)  acc5: 86.4583 (91.2539)  time: 3.4170  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:02  loss: 1.3642 (1.2383)  acc1: 69.7917 (71.9269)  acc5: 89.5833 (91.1390)  time: 3.4151  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:28  loss: 1.4087 (1.2492)  acc1: 63.5417 (71.6379)  acc5: 88.5417 (90.9500)  time: 3.4178  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:53  loss: 1.5523 (1.2555)  acc1: 64.5833 (71.5607)  acc5: 85.4167 (90.8588)  time: 3.4188  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:19  loss: 1.4226 (1.2620)  acc1: 67.7083 (71.4340)  acc5: 87.5000 (90.7517)  time: 3.4162  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:45  loss: 1.4877 (1.2663)  acc1: 66.6667 (71.3925)  acc5: 87.5000 (90.6770)  time: 3.4174  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:11  loss: 1.5717 (1.2736)  acc1: 66.6667 (71.2224)  acc5: 87.5000 (90.6008)  time: 3.4191  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:37  loss: 1.7213 (1.2863)  acc1: 59.3750 (70.9325)  acc5: 84.3750 (90.4124)  time: 3.4202  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:02  loss: 1.5967 (1.2906)  acc1: 63.5417 (70.8056)  acc5: 85.4167 (90.3571)  time: 3.4210  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:28  loss: 1.4981 (1.2936)  acc1: 65.6250 (70.7000)  acc5: 88.5417 (90.3380)  time: 3.4178  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:54  loss: 1.3929 (1.2990)  acc1: 69.7917 (70.6011)  acc5: 88.5417 (90.2623)  time: 3.4177  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.3929 (1.3034)  acc1: 69.7917 (70.4652)  acc5: 89.5833 (90.2395)  time: 3.4201  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:46  loss: 1.1280 (1.2986)  acc1: 70.8333 (70.5363)  acc5: 92.7083 (90.3195)  time: 3.4190  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 0.9924 (1.2917)  acc1: 79.1667 (70.6961)  acc5: 94.7917 (90.3921)  time: 3.4174  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1002 (1.2964)  acc1: 71.8750 (70.5439)  acc5: 91.6667 (90.3641)  time: 3.4154  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2116 (1.2874)  acc1: 71.8750 (70.7620)  acc5: 90.6250 (90.4500)  time: 3.3948  data: 0.0001  max mem: 16226
Test: Total time: 0:29:41 (3.4189 s / it)
* Acc@1 70.762 Acc@5 90.450 loss 1.287
Accuracy of the network on the 50000 test images: 70.8%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.06, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.06
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:09  loss: 0.5492 (0.5492)  acc1: 91.6667 (91.6667)  acc5: 96.8750 (96.8750)  time: 5.0845  data: 1.0635  max mem: 16225
Test:  [ 10/521]  eta: 0:30:23  loss: 0.5878 (0.6795)  acc1: 90.6250 (87.3106)  acc5: 97.9167 (96.9697)  time: 3.5685  data: 0.0970  max mem: 16226
Test:  [ 20/521]  eta: 0:29:11  loss: 0.7661 (0.8557)  acc1: 81.2500 (81.9444)  acc5: 96.8750 (95.5357)  time: 3.4174  data: 0.0006  max mem: 16226
Test:  [ 30/521]  eta: 0:28:24  loss: 1.0364 (0.9745)  acc1: 73.9583 (78.7970)  acc5: 92.7083 (94.1532)  time: 3.4181  data: 0.0005  max mem: 16226
Test:  [ 40/521]  eta: 0:27:43  loss: 1.2245 (1.0441)  acc1: 70.8333 (76.2449)  acc5: 91.6667 (93.8262)  time: 3.4177  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:05  loss: 0.7426 (0.9713)  acc1: 85.4167 (78.7582)  acc5: 94.7917 (94.2810)  time: 3.4189  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:28  loss: 0.7426 (0.9588)  acc1: 85.4167 (79.4911)  acc5: 94.7917 (94.2452)  time: 3.4202  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:25:52  loss: 0.8341 (0.9591)  acc1: 83.3333 (79.4161)  acc5: 94.7917 (94.2342)  time: 3.4185  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:16  loss: 0.7114 (0.9336)  acc1: 83.3333 (80.0669)  acc5: 96.8750 (94.4830)  time: 3.4185  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:24:41  loss: 0.9828 (0.9752)  acc1: 78.1250 (78.7775)  acc5: 94.7917 (94.1735)  time: 3.4189  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:24:06  loss: 1.2247 (1.0079)  acc1: 66.6667 (77.6609)  acc5: 92.7083 (94.0491)  time: 3.4205  data: 0.0001  max mem: 16226
Test:  [110/521]  eta: 0:23:31  loss: 1.1165 (1.0132)  acc1: 71.8750 (77.4118)  acc5: 92.7083 (94.1160)  time: 3.4212  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:22:56  loss: 1.0527 (1.0211)  acc1: 75.0000 (77.1522)  acc5: 93.7500 (93.9910)  time: 3.4188  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:21  loss: 1.1209 (1.0284)  acc1: 71.8750 (76.6380)  acc5: 94.7917 (94.0919)  time: 3.4182  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:47  loss: 1.0151 (1.0213)  acc1: 75.0000 (76.6992)  acc5: 94.7917 (94.1563)  time: 3.4179  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:12  loss: 0.9990 (1.0323)  acc1: 75.0000 (76.3176)  acc5: 94.7917 (94.1156)  time: 3.4191  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:37  loss: 1.0476 (1.0238)  acc1: 73.9583 (76.6110)  acc5: 94.7917 (94.1835)  time: 3.4207  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:03  loss: 0.8415 (1.0155)  acc1: 82.2917 (76.7666)  acc5: 95.8333 (94.2556)  time: 3.4195  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:29  loss: 0.8604 (1.0089)  acc1: 80.2083 (77.0028)  acc5: 95.8333 (94.2967)  time: 3.4189  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:18:54  loss: 0.8774 (1.0056)  acc1: 78.1250 (77.0288)  acc5: 95.8333 (94.3717)  time: 3.4203  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:20  loss: 0.9688 (1.0129)  acc1: 76.0417 (76.8294)  acc5: 93.7500 (94.2734)  time: 3.4203  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:45  loss: 0.9885 (1.0118)  acc1: 73.9583 (76.8266)  acc5: 92.7083 (94.1943)  time: 3.4199  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:11  loss: 1.1747 (1.0353)  acc1: 68.7500 (76.2726)  acc5: 91.6667 (93.9291)  time: 3.4215  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:37  loss: 1.3035 (1.0518)  acc1: 66.6667 (75.8252)  acc5: 88.5417 (93.6869)  time: 3.4213  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:02  loss: 1.3035 (1.0713)  acc1: 65.6250 (75.3631)  acc5: 88.5417 (93.4345)  time: 3.4221  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:28  loss: 1.5052 (1.0944)  acc1: 63.5417 (74.9336)  acc5: 86.4583 (93.0694)  time: 3.4222  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:14:54  loss: 1.5918 (1.1166)  acc1: 61.4583 (74.4333)  acc5: 83.3333 (92.8001)  time: 3.4205  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:19  loss: 1.5002 (1.1382)  acc1: 60.4167 (73.8853)  acc5: 86.4583 (92.5200)  time: 3.4205  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:45  loss: 1.4873 (1.1496)  acc1: 65.6250 (73.6729)  acc5: 86.4583 (92.3599)  time: 3.4198  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:11  loss: 1.4873 (1.1594)  acc1: 69.7917 (73.4715)  acc5: 86.4583 (92.2143)  time: 3.4198  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:36  loss: 1.1175 (1.1570)  acc1: 71.8750 (73.6226)  acc5: 91.6667 (92.1962)  time: 3.4214  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:12:02  loss: 1.1724 (1.1735)  acc1: 70.8333 (73.2550)  acc5: 90.6250 (92.0016)  time: 3.4238  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:28  loss: 1.3851 (1.1778)  acc1: 67.7083 (73.2606)  acc5: 88.5417 (91.8938)  time: 3.4223  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:54  loss: 1.3851 (1.1974)  acc1: 66.6667 (72.8097)  acc5: 89.5833 (91.6289)  time: 3.4222  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:19  loss: 1.5845 (1.2075)  acc1: 61.4583 (72.5745)  acc5: 85.4167 (91.4712)  time: 3.4242  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:45  loss: 1.5845 (1.2170)  acc1: 61.4583 (72.3231)  acc5: 86.4583 (91.3996)  time: 3.4246  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:11  loss: 1.5463 (1.2297)  acc1: 62.5000 (72.0337)  acc5: 86.4583 (91.2050)  time: 3.4226  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:37  loss: 1.5291 (1.2373)  acc1: 64.5833 (71.8750)  acc5: 87.5000 (91.1388)  time: 3.4199  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:08:02  loss: 1.3549 (1.2425)  acc1: 69.7917 (71.8449)  acc5: 89.5833 (91.0324)  time: 3.4205  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:28  loss: 1.4709 (1.2533)  acc1: 63.5417 (71.5660)  acc5: 87.5000 (90.8328)  time: 3.4211  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:54  loss: 1.6011 (1.2598)  acc1: 64.5833 (71.4854)  acc5: 85.4167 (90.7341)  time: 3.4216  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:20  loss: 1.4766 (1.2662)  acc1: 65.6250 (71.3605)  acc5: 85.4167 (90.6123)  time: 3.4216  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:45  loss: 1.4725 (1.2705)  acc1: 64.5833 (71.3331)  acc5: 84.3750 (90.5285)  time: 3.4220  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:11  loss: 1.5541 (1.2782)  acc1: 64.5833 (71.1572)  acc5: 87.5000 (90.4486)  time: 3.4239  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:37  loss: 1.7467 (1.2911)  acc1: 60.4167 (70.8853)  acc5: 84.3750 (90.2683)  time: 3.4246  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:03  loss: 1.6132 (1.2955)  acc1: 63.5417 (70.7594)  acc5: 86.4583 (90.2046)  time: 3.4236  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:28  loss: 1.5012 (1.2987)  acc1: 65.6250 (70.6390)  acc5: 88.5417 (90.1821)  time: 3.4227  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:54  loss: 1.3563 (1.3041)  acc1: 70.8333 (70.5392)  acc5: 88.5417 (90.0986)  time: 3.4214  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.3563 (1.3090)  acc1: 69.7917 (70.3915)  acc5: 88.5417 (90.0749)  time: 3.4214  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:46  loss: 1.1454 (1.3044)  acc1: 69.7917 (70.4557)  acc5: 92.7083 (90.1540)  time: 3.4210  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0117 (1.2975)  acc1: 77.0833 (70.6192)  acc5: 94.7917 (90.2341)  time: 3.4208  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1107 (1.3028)  acc1: 73.9583 (70.4481)  acc5: 92.7083 (90.1786)  time: 3.4217  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2253 (1.2937)  acc1: 71.8750 (70.6740)  acc5: 91.6667 (90.2740)  time: 3.3987  data: 0.0001  max mem: 16226
Test: Total time: 0:29:43 (3.4233 s / it)
* Acc@1 70.674 Acc@5 90.274 loss 1.294
Accuracy of the network on the 50000 test images: 70.7%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.07, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.07
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:36  loss: 0.5085 (0.5085)  acc1: 90.6250 (90.6250)  acc5: 95.8333 (95.8333)  time: 5.1375  data: 1.1400  max mem: 16225
Test:  [ 10/521]  eta: 0:30:30  loss: 0.6401 (0.6900)  acc1: 89.5833 (86.8371)  acc5: 97.9167 (96.8750)  time: 3.5812  data: 0.1040  max mem: 16226
Test:  [ 20/521]  eta: 0:29:16  loss: 0.8368 (0.8712)  acc1: 81.2500 (81.6964)  acc5: 95.8333 (95.3869)  time: 3.4243  data: 0.0003  max mem: 16226
Test:  [ 30/521]  eta: 0:28:27  loss: 1.0451 (0.9841)  acc1: 71.8750 (78.6626)  acc5: 91.6667 (93.9852)  time: 3.4218  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:46  loss: 1.2079 (1.0553)  acc1: 70.8333 (75.8130)  acc5: 91.6667 (93.4959)  time: 3.4208  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:27:07  loss: 0.7670 (0.9828)  acc1: 85.4167 (78.2680)  acc5: 95.8333 (94.0768)  time: 3.4208  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:30  loss: 0.7655 (0.9687)  acc1: 84.3750 (78.9447)  acc5: 95.8333 (94.0745)  time: 3.4238  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:54  loss: 0.8320 (0.9681)  acc1: 83.3333 (79.0053)  acc5: 93.7500 (94.0141)  time: 3.4267  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:19  loss: 0.7253 (0.9422)  acc1: 85.4167 (79.6811)  acc5: 95.8333 (94.3158)  time: 3.4260  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:24:43  loss: 1.0131 (0.9836)  acc1: 78.1250 (78.4684)  acc5: 93.7500 (94.0362)  time: 3.4244  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:24:08  loss: 1.2827 (1.0167)  acc1: 66.6667 (77.3721)  acc5: 92.7083 (93.9047)  time: 3.4218  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:33  loss: 1.1094 (1.0234)  acc1: 70.8333 (77.1021)  acc5: 93.7500 (93.9846)  time: 3.4217  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:58  loss: 1.1033 (1.0317)  acc1: 72.9167 (76.8767)  acc5: 93.7500 (93.8533)  time: 3.4230  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:23  loss: 1.1398 (1.0402)  acc1: 71.8750 (76.1689)  acc5: 94.7917 (93.9647)  time: 3.4232  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:48  loss: 1.0138 (1.0320)  acc1: 73.9583 (76.3224)  acc5: 95.8333 (94.0751)  time: 3.4217  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:14  loss: 0.9717 (1.0427)  acc1: 76.0417 (75.9727)  acc5: 95.8333 (94.0397)  time: 3.4228  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:39  loss: 1.0483 (1.0336)  acc1: 76.0417 (76.2811)  acc5: 94.7917 (94.0994)  time: 3.4232  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:04  loss: 0.8171 (1.0258)  acc1: 81.2500 (76.3828)  acc5: 94.7917 (94.1642)  time: 3.4211  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:30  loss: 0.8974 (1.0187)  acc1: 81.2500 (76.6229)  acc5: 94.7917 (94.2392)  time: 3.4229  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:55  loss: 0.8745 (1.0151)  acc1: 79.1667 (76.6798)  acc5: 94.7917 (94.2954)  time: 3.4231  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:21  loss: 0.9663 (1.0218)  acc1: 77.0833 (76.5392)  acc5: 93.7500 (94.2320)  time: 3.4240  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:47  loss: 1.0367 (1.0208)  acc1: 77.0833 (76.5600)  acc5: 92.7083 (94.1548)  time: 3.4267  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:12  loss: 1.1423 (1.0442)  acc1: 68.7500 (76.0511)  acc5: 91.6667 (93.8914)  time: 3.4233  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:38  loss: 1.3252 (1.0611)  acc1: 64.5833 (75.5727)  acc5: 89.5833 (93.6733)  time: 3.4233  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:03  loss: 1.3588 (1.0791)  acc1: 65.6250 (75.1599)  acc5: 89.5833 (93.4215)  time: 3.4242  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:29  loss: 1.4545 (1.1029)  acc1: 65.6250 (74.7178)  acc5: 85.4167 (92.9947)  time: 3.4216  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:14:55  loss: 1.6331 (1.1243)  acc1: 59.3750 (74.1978)  acc5: 83.3333 (92.7004)  time: 3.4213  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:20  loss: 1.5072 (1.1461)  acc1: 59.3750 (73.6547)  acc5: 86.4583 (92.4585)  time: 3.4214  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:46  loss: 1.5067 (1.1578)  acc1: 63.5417 (73.4468)  acc5: 87.5000 (92.3117)  time: 3.4237  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:12  loss: 1.4749 (1.1672)  acc1: 67.7083 (73.2746)  acc5: 87.5000 (92.1428)  time: 3.4231  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:37  loss: 1.1037 (1.1643)  acc1: 70.8333 (73.4289)  acc5: 90.6250 (92.1339)  time: 3.4212  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:03  loss: 1.1819 (1.1813)  acc1: 69.7917 (73.0573)  acc5: 90.6250 (91.9447)  time: 3.4210  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:29  loss: 1.3730 (1.1856)  acc1: 67.7083 (73.0757)  acc5: 87.5000 (91.8322)  time: 3.4202  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:54  loss: 1.3730 (1.2057)  acc1: 69.7917 (72.6586)  acc5: 88.5417 (91.5565)  time: 3.4207  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:20  loss: 1.6062 (1.2156)  acc1: 63.5417 (72.4310)  acc5: 84.3750 (91.4223)  time: 3.4199  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:46  loss: 1.6062 (1.2246)  acc1: 63.5417 (72.1836)  acc5: 86.4583 (91.3551)  time: 3.4205  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:11  loss: 1.5428 (1.2371)  acc1: 64.5833 (71.9269)  acc5: 86.4583 (91.1646)  time: 3.4220  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:37  loss: 1.5258 (1.2442)  acc1: 64.5833 (71.7936)  acc5: 88.5417 (91.0911)  time: 3.4209  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:08:03  loss: 1.3382 (1.2499)  acc1: 69.7917 (71.7356)  acc5: 89.5833 (90.9695)  time: 3.4226  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:28  loss: 1.4079 (1.2606)  acc1: 65.6250 (71.4567)  acc5: 87.5000 (90.7955)  time: 3.4236  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:54  loss: 1.5410 (1.2669)  acc1: 65.6250 (71.3970)  acc5: 86.4583 (90.6899)  time: 3.4209  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:20  loss: 1.4735 (1.2731)  acc1: 66.6667 (71.2819)  acc5: 86.4583 (90.5768)  time: 3.4220  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:46  loss: 1.4735 (1.2775)  acc1: 64.5833 (71.2366)  acc5: 84.3750 (90.4815)  time: 3.4242  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:11  loss: 1.5888 (1.2851)  acc1: 64.5833 (71.0460)  acc5: 87.5000 (90.3882)  time: 3.4239  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:37  loss: 1.7249 (1.2982)  acc1: 59.3750 (70.7270)  acc5: 87.5000 (90.2258)  time: 3.4213  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:03  loss: 1.6500 (1.3029)  acc1: 62.5000 (70.6001)  acc5: 87.5000 (90.1654)  time: 3.4231  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:29  loss: 1.4654 (1.3061)  acc1: 64.5833 (70.4560)  acc5: 89.5833 (90.1415)  time: 3.4248  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:54  loss: 1.3988 (1.3115)  acc1: 67.7083 (70.3468)  acc5: 88.5417 (90.0544)  time: 3.4206  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.3774 (1.3160)  acc1: 68.7500 (70.2226)  acc5: 88.5417 (90.0295)  time: 3.4202  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:46  loss: 1.1211 (1.3111)  acc1: 69.7917 (70.3114)  acc5: 91.6667 (90.1052)  time: 3.4220  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0253 (1.3040)  acc1: 75.0000 (70.4778)  acc5: 94.7917 (90.1821)  time: 3.4219  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1428 (1.3091)  acc1: 71.8750 (70.3054)  acc5: 92.7083 (90.1317)  time: 3.4229  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1626 (1.2998)  acc1: 71.8750 (70.5320)  acc5: 91.6667 (90.2220)  time: 3.3987  data: 0.0001  max mem: 16226
Test: Total time: 0:29:44 (3.4251 s / it)
* Acc@1 70.532 Acc@5 90.222 loss 1.300
Accuracy of the network on the 50000 test images: 70.5%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.08, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.08
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:45:09  loss: 0.5832 (0.5832)  acc1: 88.5417 (88.5417)  acc5: 96.8750 (96.8750)  time: 5.2013  data: 1.2246  max mem: 16225
Test:  [ 10/521]  eta: 0:30:29  loss: 0.6597 (0.6972)  acc1: 88.5417 (86.0795)  acc5: 96.8750 (96.4962)  time: 3.5797  data: 0.1116  max mem: 16226
Test:  [ 20/521]  eta: 0:29:14  loss: 0.8321 (0.8796)  acc1: 81.2500 (80.7044)  acc5: 95.8333 (95.0397)  time: 3.4181  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:26  loss: 1.0206 (0.9906)  acc1: 72.9167 (78.0578)  acc5: 91.6667 (93.6156)  time: 3.4178  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:45  loss: 1.1883 (1.0538)  acc1: 71.8750 (75.8130)  acc5: 91.6667 (93.3181)  time: 3.4187  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:06  loss: 0.7723 (0.9821)  acc1: 85.4167 (78.3701)  acc5: 94.7917 (93.7909)  time: 3.4184  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:29  loss: 0.7682 (0.9687)  acc1: 85.4167 (79.1325)  acc5: 94.7917 (93.8525)  time: 3.4202  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:53  loss: 0.8207 (0.9687)  acc1: 83.3333 (79.1373)  acc5: 93.7500 (93.9114)  time: 3.4198  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:17  loss: 0.7301 (0.9437)  acc1: 84.3750 (79.8483)  acc5: 96.8750 (94.1615)  time: 3.4185  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:42  loss: 1.0194 (0.9866)  acc1: 77.0833 (78.4684)  acc5: 94.7917 (93.8988)  time: 3.4211  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:06  loss: 1.2585 (1.0194)  acc1: 66.6667 (77.4443)  acc5: 92.7083 (93.8016)  time: 3.4198  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:31  loss: 1.1363 (1.0267)  acc1: 68.7500 (77.0646)  acc5: 93.7500 (93.8814)  time: 3.4188  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:57  loss: 1.1001 (1.0346)  acc1: 73.9583 (76.8251)  acc5: 93.7500 (93.7672)  time: 3.4202  data: 0.0003  max mem: 16226
Test:  [130/521]  eta: 0:22:22  loss: 1.1124 (1.0425)  acc1: 73.9583 (76.2564)  acc5: 93.7500 (93.8852)  time: 3.4209  data: 0.0003  max mem: 16226
Test:  [140/521]  eta: 0:21:47  loss: 1.0065 (1.0349)  acc1: 77.0833 (76.4037)  acc5: 94.7917 (93.9938)  time: 3.4191  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:12  loss: 0.9479 (1.0467)  acc1: 77.0833 (75.9796)  acc5: 94.7917 (93.9845)  time: 3.4175  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:38  loss: 1.0195 (1.0380)  acc1: 76.0417 (76.2940)  acc5: 94.7917 (94.0606)  time: 3.4186  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:20:03  loss: 0.8086 (1.0304)  acc1: 81.2500 (76.4681)  acc5: 94.7917 (94.1155)  time: 3.4193  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:29  loss: 0.8242 (1.0228)  acc1: 81.2500 (76.7265)  acc5: 94.7917 (94.1816)  time: 3.4183  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:54  loss: 0.8684 (1.0188)  acc1: 79.1667 (76.7888)  acc5: 94.7917 (94.2408)  time: 3.4174  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:20  loss: 1.0310 (1.0268)  acc1: 76.0417 (76.5703)  acc5: 93.7500 (94.1490)  time: 3.4178  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:45  loss: 1.0391 (1.0257)  acc1: 73.9583 (76.6045)  acc5: 91.6667 (94.0758)  time: 3.4178  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:11  loss: 1.1998 (1.0492)  acc1: 69.7917 (76.0605)  acc5: 89.5833 (93.7689)  time: 3.4170  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:37  loss: 1.3791 (1.0673)  acc1: 65.6250 (75.5682)  acc5: 89.5833 (93.5516)  time: 3.4184  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:16:02  loss: 1.4000 (1.0867)  acc1: 63.5417 (75.0778)  acc5: 88.5417 (93.2918)  time: 3.4212  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:28  loss: 1.5098 (1.1096)  acc1: 64.5833 (74.6680)  acc5: 86.4583 (92.9366)  time: 3.4215  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:54  loss: 1.6630 (1.1311)  acc1: 61.4583 (74.1459)  acc5: 83.3333 (92.6445)  time: 3.4232  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:19  loss: 1.5575 (1.1527)  acc1: 59.3750 (73.5893)  acc5: 86.4583 (92.3816)  time: 3.4231  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:45  loss: 1.5575 (1.1647)  acc1: 63.5417 (73.3430)  acc5: 87.5000 (92.2116)  time: 3.4195  data: 0.0003  max mem: 16226
Test:  [290/521]  eta: 0:13:11  loss: 1.5269 (1.1738)  acc1: 67.7083 (73.1565)  acc5: 87.5000 (92.0497)  time: 3.4206  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:36  loss: 1.1270 (1.1712)  acc1: 72.9167 (73.3216)  acc5: 89.5833 (92.0439)  time: 3.4195  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:12:02  loss: 1.1655 (1.1886)  acc1: 71.8750 (72.9301)  acc5: 88.5417 (91.8241)  time: 3.4185  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:28  loss: 1.3626 (1.1928)  acc1: 66.6667 (72.9524)  acc5: 87.5000 (91.7056)  time: 3.4196  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:54  loss: 1.3427 (1.2131)  acc1: 66.6667 (72.4855)  acc5: 87.5000 (91.4275)  time: 3.4208  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:19  loss: 1.6223 (1.2233)  acc1: 59.3750 (72.2324)  acc5: 85.4167 (91.2818)  time: 3.4232  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:45  loss: 1.6037 (1.2322)  acc1: 60.4167 (71.9759)  acc5: 85.4167 (91.2096)  time: 3.4222  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:11  loss: 1.5738 (1.2445)  acc1: 62.5000 (71.7307)  acc5: 85.4167 (91.0117)  time: 3.4203  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:37  loss: 1.5515 (1.2518)  acc1: 64.5833 (71.6139)  acc5: 87.5000 (90.9507)  time: 3.4182  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:02  loss: 1.3994 (1.2575)  acc1: 68.7500 (71.5715)  acc5: 89.5833 (90.8410)  time: 3.4174  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:28  loss: 1.4358 (1.2684)  acc1: 67.7083 (71.2969)  acc5: 88.5417 (90.6676)  time: 3.4182  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:54  loss: 1.5357 (1.2751)  acc1: 65.6250 (71.2308)  acc5: 86.4583 (90.5730)  time: 3.4184  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:20  loss: 1.4497 (1.2812)  acc1: 65.6250 (71.1197)  acc5: 86.4583 (90.4831)  time: 3.4194  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:45  loss: 1.4736 (1.2854)  acc1: 65.6250 (71.0758)  acc5: 86.4583 (90.4271)  time: 3.4194  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:11  loss: 1.6329 (1.2929)  acc1: 64.5833 (70.8865)  acc5: 87.5000 (90.3591)  time: 3.4193  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:37  loss: 1.7390 (1.3058)  acc1: 59.3750 (70.5688)  acc5: 85.4167 (90.1880)  time: 3.4198  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:03  loss: 1.6260 (1.3099)  acc1: 62.5000 (70.4430)  acc5: 86.4583 (90.1330)  time: 3.4183  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:28  loss: 1.4847 (1.3132)  acc1: 65.6250 (70.2865)  acc5: 87.5000 (90.1076)  time: 3.4186  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:54  loss: 1.4066 (1.3186)  acc1: 67.7083 (70.1787)  acc5: 88.5417 (90.0212)  time: 3.4184  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.4036 (1.3234)  acc1: 68.7500 (70.0299)  acc5: 88.5417 (89.9926)  time: 3.4165  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:46  loss: 1.1187 (1.3186)  acc1: 70.8333 (70.1057)  acc5: 91.6667 (90.0734)  time: 3.4175  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0048 (1.3114)  acc1: 76.0417 (70.2761)  acc5: 94.7917 (90.1489)  time: 3.4184  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1401 (1.3166)  acc1: 72.9167 (70.1158)  acc5: 92.7083 (90.1093)  time: 3.4185  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2430 (1.3074)  acc1: 69.7917 (70.3340)  acc5: 92.7083 (90.1960)  time: 3.3993  data: 0.0001  max mem: 16226
Test: Total time: 0:29:42 (3.4220 s / it)
* Acc@1 70.334 Acc@5 90.196 loss 1.307
Accuracy of the network on the 50000 test images: 70.3%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:32  loss: 0.5646 (0.5646)  acc1: 88.5417 (88.5417)  acc5: 96.8750 (96.8750)  time: 5.1303  data: 1.1086  max mem: 16225
Test:  [ 10/521]  eta: 0:30:24  loss: 0.5872 (0.6746)  acc1: 88.5417 (86.7424)  acc5: 97.9167 (96.7803)  time: 3.5704  data: 0.1009  max mem: 16226
Test:  [ 20/521]  eta: 0:29:12  loss: 0.8329 (0.8635)  acc1: 83.3333 (81.9940)  acc5: 95.8333 (95.3373)  time: 3.4161  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:24  loss: 1.0776 (0.9754)  acc1: 75.0000 (79.0995)  acc5: 91.6667 (94.1868)  time: 3.4168  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:27:43  loss: 1.2185 (1.0424)  acc1: 72.9167 (76.5498)  acc5: 91.6667 (93.8008)  time: 3.4177  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:27:05  loss: 0.7353 (0.9680)  acc1: 86.4583 (78.8807)  acc5: 94.7917 (94.1381)  time: 3.4190  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:28  loss: 0.7282 (0.9581)  acc1: 86.4583 (79.4399)  acc5: 94.7917 (94.0915)  time: 3.4185  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:52  loss: 0.8509 (0.9610)  acc1: 83.3333 (79.2840)  acc5: 93.7500 (93.9554)  time: 3.4180  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:16  loss: 0.7410 (0.9352)  acc1: 83.3333 (79.9254)  acc5: 95.8333 (94.2387)  time: 3.4166  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:40  loss: 0.9969 (0.9731)  acc1: 77.0833 (78.6859)  acc5: 94.7917 (94.0820)  time: 3.4164  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:05  loss: 1.2318 (1.0080)  acc1: 66.6667 (77.4649)  acc5: 92.7083 (93.8944)  time: 3.4179  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:31  loss: 1.1523 (1.0145)  acc1: 68.7500 (77.2241)  acc5: 93.7500 (93.9471)  time: 3.4201  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:22:56  loss: 1.0369 (1.0216)  acc1: 76.0417 (77.1006)  acc5: 93.7500 (93.8533)  time: 3.4176  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:21  loss: 1.0986 (1.0289)  acc1: 71.8750 (76.5108)  acc5: 94.7917 (93.9647)  time: 3.4160  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:21:46  loss: 0.9730 (1.0212)  acc1: 78.1250 (76.6770)  acc5: 95.8333 (94.0677)  time: 3.4173  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:12  loss: 0.9689 (1.0320)  acc1: 78.1250 (76.3314)  acc5: 94.7917 (94.0673)  time: 3.4179  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:37  loss: 1.0583 (1.0237)  acc1: 77.0833 (76.6046)  acc5: 94.7917 (94.1706)  time: 3.4176  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:03  loss: 0.8458 (1.0158)  acc1: 82.2917 (76.7361)  acc5: 95.8333 (94.2434)  time: 3.4166  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:28  loss: 0.8741 (1.0105)  acc1: 80.2083 (76.9625)  acc5: 94.7917 (94.2795)  time: 3.4163  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:18:54  loss: 0.9071 (1.0084)  acc1: 79.1667 (76.9906)  acc5: 94.7917 (94.3172)  time: 3.4151  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:19  loss: 0.9667 (1.0159)  acc1: 77.0833 (76.8398)  acc5: 93.7500 (94.2527)  time: 3.4165  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:45  loss: 0.9990 (1.0146)  acc1: 77.0833 (76.8908)  acc5: 93.7500 (94.2091)  time: 3.4165  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:10  loss: 1.1358 (1.0366)  acc1: 70.8333 (76.3810)  acc5: 92.7083 (93.9385)  time: 3.4170  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:36  loss: 1.3308 (1.0533)  acc1: 68.7500 (75.9560)  acc5: 89.5833 (93.7410)  time: 3.4174  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:16:02  loss: 1.4315 (1.0717)  acc1: 66.6667 (75.5100)  acc5: 89.5833 (93.4691)  time: 3.4165  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:27  loss: 1.4724 (1.0937)  acc1: 66.6667 (75.1038)  acc5: 87.5000 (93.1316)  time: 3.4151  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:53  loss: 1.6191 (1.1144)  acc1: 62.5000 (74.6408)  acc5: 85.4167 (92.8560)  time: 3.4144  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:19  loss: 1.5067 (1.1350)  acc1: 62.5000 (74.1275)  acc5: 86.4583 (92.6046)  time: 3.4163  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:44  loss: 1.4457 (1.1462)  acc1: 64.5833 (73.9139)  acc5: 86.4583 (92.4414)  time: 3.4162  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:10  loss: 1.4457 (1.1557)  acc1: 67.7083 (73.7507)  acc5: 86.4583 (92.2967)  time: 3.4159  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:36  loss: 1.0828 (1.1528)  acc1: 73.9583 (73.8960)  acc5: 90.6250 (92.2861)  time: 3.4175  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:02  loss: 1.1345 (1.1682)  acc1: 69.7917 (73.5330)  acc5: 90.6250 (92.0987)  time: 3.4162  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:27  loss: 1.3890 (1.1727)  acc1: 69.7917 (73.5235)  acc5: 88.5417 (91.9879)  time: 3.4151  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:53  loss: 1.3995 (1.1919)  acc1: 69.7917 (73.0457)  acc5: 89.5833 (91.7548)  time: 3.4164  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:19  loss: 1.5673 (1.2018)  acc1: 58.3333 (72.7975)  acc5: 86.4583 (91.6239)  time: 3.4159  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:45  loss: 1.5559 (1.2110)  acc1: 58.3333 (72.5368)  acc5: 88.5417 (91.5539)  time: 3.4170  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:10  loss: 1.5283 (1.2229)  acc1: 62.5000 (72.2790)  acc5: 88.5417 (91.3752)  time: 3.4174  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:36  loss: 1.5463 (1.2304)  acc1: 64.5833 (72.1502)  acc5: 88.5417 (91.3157)  time: 3.4156  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:08:02  loss: 1.3630 (1.2352)  acc1: 71.8750 (72.1429)  acc5: 89.5833 (91.2074)  time: 3.4156  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:28  loss: 1.4508 (1.2461)  acc1: 63.5417 (71.8244)  acc5: 88.5417 (91.0246)  time: 3.4177  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:53  loss: 1.5426 (1.2520)  acc1: 65.6250 (71.7477)  acc5: 85.4167 (90.9315)  time: 3.4179  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:19  loss: 1.5272 (1.2586)  acc1: 67.7083 (71.6368)  acc5: 85.4167 (90.8075)  time: 3.4159  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:45  loss: 1.5002 (1.2628)  acc1: 68.7500 (71.6127)  acc5: 86.4583 (90.7314)  time: 3.4179  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:11  loss: 1.5002 (1.2700)  acc1: 67.7083 (71.4376)  acc5: 87.5000 (90.6588)  time: 3.4181  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:37  loss: 1.7491 (1.2822)  acc1: 59.3750 (71.1475)  acc5: 85.4167 (90.4762)  time: 3.4161  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:02  loss: 1.5756 (1.2868)  acc1: 64.5833 (71.0504)  acc5: 86.4583 (90.4218)  time: 3.4161  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:28  loss: 1.4601 (1.2904)  acc1: 64.5833 (70.9079)  acc5: 87.5000 (90.3945)  time: 3.4159  data: 0.0003  max mem: 16226
Test:  [470/521]  eta: 0:02:54  loss: 1.3953 (1.2962)  acc1: 66.6667 (70.8179)  acc5: 89.5833 (90.3132)  time: 3.4185  data: 0.0003  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.3953 (1.3006)  acc1: 70.8333 (70.6947)  acc5: 89.5833 (90.2763)  time: 3.4198  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:46  loss: 1.1078 (1.2961)  acc1: 71.8750 (70.7633)  acc5: 90.6250 (90.3428)  time: 3.4175  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 0.9970 (1.2898)  acc1: 76.0417 (70.9123)  acc5: 94.7917 (90.4171)  time: 3.4186  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1306 (1.2948)  acc1: 71.8750 (70.7579)  acc5: 92.7083 (90.3641)  time: 3.4183  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2418 (1.2860)  acc1: 70.8333 (70.9720)  acc5: 91.6667 (90.4560)  time: 3.3934  data: 0.0001  max mem: 16226
Test: Total time: 0:29:41 (3.4195 s / it)
* Acc@1 70.972 Acc@5 90.456 loss 1.286
Accuracy of the network on the 50000 test images: 71.0%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.04, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.04
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:20  loss: 0.5270 (0.5270)  acc1: 89.5833 (89.5833)  acc5: 97.9167 (97.9167)  time: 5.1068  data: 1.1034  max mem: 16225
Test:  [ 10/521]  eta: 0:30:27  loss: 0.5743 (0.6785)  acc1: 89.5833 (87.1212)  acc5: 97.9167 (96.9697)  time: 3.5768  data: 0.1006  max mem: 16226
Test:  [ 20/521]  eta: 0:29:14  loss: 0.8783 (0.8660)  acc1: 83.3333 (81.9940)  acc5: 95.8333 (95.2381)  time: 3.4217  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:26  loss: 1.0801 (0.9763)  acc1: 76.0417 (79.2675)  acc5: 91.6667 (93.8508)  time: 3.4204  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:27:45  loss: 1.1977 (1.0460)  acc1: 71.8750 (76.4990)  acc5: 91.6667 (93.4705)  time: 3.4199  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:27:06  loss: 0.7271 (0.9707)  acc1: 86.4583 (78.9011)  acc5: 94.7917 (93.9747)  time: 3.4185  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:29  loss: 0.7127 (0.9596)  acc1: 84.3750 (79.4740)  acc5: 94.7917 (94.0062)  time: 3.4184  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:25:53  loss: 0.8618 (0.9622)  acc1: 83.3333 (79.4161)  acc5: 94.7917 (93.9701)  time: 3.4181  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:17  loss: 0.7338 (0.9353)  acc1: 85.4167 (80.1055)  acc5: 95.8333 (94.2773)  time: 3.4204  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:24:41  loss: 1.0152 (0.9744)  acc1: 76.0417 (78.7317)  acc5: 94.7917 (94.0591)  time: 3.4201  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:24:06  loss: 1.2213 (1.0065)  acc1: 67.7083 (77.6712)  acc5: 92.7083 (93.9356)  time: 3.4197  data: 0.0001  max mem: 16226
Test:  [110/521]  eta: 0:23:31  loss: 1.1124 (1.0140)  acc1: 70.8333 (77.3555)  acc5: 94.7917 (94.0034)  time: 3.4211  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:22:56  loss: 1.0400 (1.0216)  acc1: 76.0417 (77.2211)  acc5: 94.7917 (93.8791)  time: 3.4193  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:22  loss: 1.1523 (1.0296)  acc1: 73.9583 (76.5744)  acc5: 93.7500 (93.9488)  time: 3.4180  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:47  loss: 0.9886 (1.0222)  acc1: 75.0000 (76.6992)  acc5: 95.8333 (94.0307)  time: 3.4172  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:12  loss: 0.9886 (1.0327)  acc1: 77.0833 (76.3452)  acc5: 94.7917 (94.0259)  time: 3.4191  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:38  loss: 1.0867 (1.0244)  acc1: 78.1250 (76.5722)  acc5: 94.7917 (94.1253)  time: 3.4201  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:03  loss: 0.8437 (1.0166)  acc1: 80.2083 (76.7605)  acc5: 95.8333 (94.1581)  time: 3.4185  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:29  loss: 0.8914 (1.0115)  acc1: 80.2083 (76.9567)  acc5: 94.7917 (94.2047)  time: 3.4188  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:18:54  loss: 0.9300 (1.0085)  acc1: 79.1667 (77.0397)  acc5: 94.7917 (94.2408)  time: 3.4190  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:20  loss: 0.9551 (1.0151)  acc1: 77.0833 (76.9330)  acc5: 93.7500 (94.1750)  time: 3.4172  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:45  loss: 1.0074 (1.0142)  acc1: 77.0833 (76.9402)  acc5: 92.7083 (94.0857)  time: 3.4160  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:11  loss: 1.1672 (1.0364)  acc1: 70.8333 (76.4659)  acc5: 90.6250 (93.8490)  time: 3.4164  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:37  loss: 1.3277 (1.0527)  acc1: 66.6667 (76.0372)  acc5: 89.5833 (93.6192)  time: 3.4177  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:02  loss: 1.3777 (1.0707)  acc1: 66.6667 (75.6094)  acc5: 87.5000 (93.3783)  time: 3.4176  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:28  loss: 1.4832 (1.0926)  acc1: 64.5833 (75.2034)  acc5: 87.5000 (93.0445)  time: 3.4197  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:14:54  loss: 1.5682 (1.1130)  acc1: 62.5000 (74.7446)  acc5: 84.3750 (92.8121)  time: 3.4191  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:19  loss: 1.5114 (1.1340)  acc1: 62.5000 (74.2236)  acc5: 85.4167 (92.5354)  time: 3.4197  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:45  loss: 1.4839 (1.1451)  acc1: 65.6250 (74.0176)  acc5: 87.5000 (92.3636)  time: 3.4206  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:11  loss: 1.4839 (1.1547)  acc1: 70.8333 (73.8509)  acc5: 87.5000 (92.2287)  time: 3.4183  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:36  loss: 1.0764 (1.1516)  acc1: 73.9583 (74.0275)  acc5: 91.6667 (92.2169)  time: 3.4182  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:12:02  loss: 1.1581 (1.1668)  acc1: 70.8333 (73.6971)  acc5: 90.6250 (92.0351)  time: 3.4171  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:28  loss: 1.3604 (1.1712)  acc1: 67.7083 (73.6728)  acc5: 89.5833 (91.9198)  time: 3.4179  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:53  loss: 1.3604 (1.1899)  acc1: 69.7917 (73.2219)  acc5: 89.5833 (91.6635)  time: 3.4178  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:19  loss: 1.5843 (1.2006)  acc1: 58.3333 (72.9686)  acc5: 87.5000 (91.5109)  time: 3.4171  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:45  loss: 1.5843 (1.2101)  acc1: 60.4167 (72.7178)  acc5: 87.5000 (91.4144)  time: 3.4192  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:11  loss: 1.4974 (1.2225)  acc1: 65.6250 (72.4463)  acc5: 86.4583 (91.2454)  time: 3.4209  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:36  loss: 1.5267 (1.2298)  acc1: 65.6250 (72.3158)  acc5: 88.5417 (91.1753)  time: 3.4207  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:02  loss: 1.3409 (1.2347)  acc1: 71.8750 (72.2878)  acc5: 89.5833 (91.0789)  time: 3.4192  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:28  loss: 1.4107 (1.2457)  acc1: 64.5833 (71.9709)  acc5: 87.5000 (90.9234)  time: 3.4194  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:54  loss: 1.5580 (1.2523)  acc1: 64.5833 (71.8698)  acc5: 85.4167 (90.8198)  time: 3.4188  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:19  loss: 1.4645 (1.2590)  acc1: 68.7500 (71.7762)  acc5: 85.4167 (90.6934)  time: 3.4177  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:45  loss: 1.4735 (1.2631)  acc1: 66.6667 (71.7364)  acc5: 85.4167 (90.6275)  time: 3.4181  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:11  loss: 1.5451 (1.2706)  acc1: 65.6250 (71.5366)  acc5: 87.5000 (90.5670)  time: 3.4170  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:37  loss: 1.6977 (1.2829)  acc1: 59.3750 (71.2420)  acc5: 84.3750 (90.3652)  time: 3.4178  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:03  loss: 1.5777 (1.2870)  acc1: 63.5417 (71.1498)  acc5: 85.4167 (90.3109)  time: 3.4189  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:28  loss: 1.4921 (1.2905)  acc1: 68.7500 (71.0367)  acc5: 87.5000 (90.2657)  time: 3.4184  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:54  loss: 1.3866 (1.2964)  acc1: 68.7500 (70.9152)  acc5: 87.5000 (90.1738)  time: 3.4185  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.3547 (1.3009)  acc1: 69.7917 (70.7770)  acc5: 88.5417 (90.1486)  time: 3.4193  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:46  loss: 1.1185 (1.2961)  acc1: 70.8333 (70.8821)  acc5: 92.7083 (90.2240)  time: 3.4194  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0204 (1.2896)  acc1: 76.0417 (71.0309)  acc5: 94.7917 (90.2923)  time: 3.4183  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1111 (1.2952)  acc1: 71.8750 (70.8252)  acc5: 91.6667 (90.2397)  time: 3.4191  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2241 (1.2862)  acc1: 69.7917 (71.0480)  acc5: 91.6667 (90.3320)  time: 3.3987  data: 0.0001  max mem: 16226
Test: Total time: 0:29:42 (3.4214 s / it)
* Acc@1 71.048 Acc@5 90.332 loss 1.286
Accuracy of the network on the 50000 test images: 71.0%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.05, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.05
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:02  loss: 0.4939 (0.4939)  acc1: 90.6250 (90.6250)  acc5: 97.9167 (97.9167)  time: 5.0712  data: 1.0756  max mem: 16225
Test:  [ 10/521]  eta: 0:30:21  loss: 0.5781 (0.6682)  acc1: 90.6250 (86.4583)  acc5: 97.9167 (97.2538)  time: 3.5654  data: 0.0979  max mem: 16226
Test:  [ 20/521]  eta: 0:29:10  loss: 0.8705 (0.8558)  acc1: 81.2500 (81.1508)  acc5: 95.8333 (95.6349)  time: 3.4150  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:23  loss: 1.0400 (0.9704)  acc1: 75.0000 (78.4274)  acc5: 92.7083 (94.3884)  time: 3.4163  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:27:42  loss: 1.1911 (1.0390)  acc1: 72.9167 (76.0163)  acc5: 91.6667 (94.0295)  time: 3.4162  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:27:04  loss: 0.7462 (0.9642)  acc1: 87.5000 (78.3497)  acc5: 94.7917 (94.3832)  time: 3.4153  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:27  loss: 0.7123 (0.9536)  acc1: 85.4167 (79.0984)  acc5: 94.7917 (94.4331)  time: 3.4166  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:51  loss: 0.8355 (0.9581)  acc1: 83.3333 (79.1960)  acc5: 94.7917 (94.2928)  time: 3.4176  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:15  loss: 0.7522 (0.9330)  acc1: 83.3333 (79.9640)  acc5: 95.8333 (94.5216)  time: 3.4179  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:40  loss: 0.9821 (0.9722)  acc1: 77.0833 (78.5600)  acc5: 94.7917 (94.3338)  time: 3.4166  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:05  loss: 1.2296 (1.0061)  acc1: 67.7083 (77.4546)  acc5: 92.7083 (94.1522)  time: 3.4201  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:30  loss: 1.1464 (1.0138)  acc1: 70.8333 (77.1490)  acc5: 93.7500 (94.2286)  time: 3.4218  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:55  loss: 1.0361 (1.0211)  acc1: 76.0417 (77.0231)  acc5: 93.7500 (94.1030)  time: 3.4175  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:21  loss: 1.1531 (1.0296)  acc1: 75.0000 (76.4313)  acc5: 93.7500 (94.1873)  time: 3.4159  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:46  loss: 0.9961 (1.0218)  acc1: 76.0417 (76.5366)  acc5: 95.8333 (94.2745)  time: 3.4170  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:11  loss: 0.9434 (1.0321)  acc1: 78.1250 (76.2003)  acc5: 94.7917 (94.2536)  time: 3.4170  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:37  loss: 1.0480 (1.0233)  acc1: 77.0833 (76.5204)  acc5: 94.7917 (94.3194)  time: 3.4164  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:20:02  loss: 0.8190 (1.0152)  acc1: 83.3333 (76.6996)  acc5: 94.7917 (94.3592)  time: 3.4182  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:28  loss: 0.8190 (1.0090)  acc1: 81.2500 (76.9452)  acc5: 94.7917 (94.4003)  time: 3.4197  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:53  loss: 0.8739 (1.0064)  acc1: 80.2083 (76.9688)  acc5: 94.7917 (94.4372)  time: 3.4170  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:19  loss: 0.9637 (1.0137)  acc1: 78.1250 (76.8294)  acc5: 92.7083 (94.3304)  time: 3.4153  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:45  loss: 1.0061 (1.0124)  acc1: 78.1250 (76.8513)  acc5: 91.6667 (94.2536)  time: 3.4169  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:10  loss: 1.1666 (1.0363)  acc1: 68.7500 (76.2868)  acc5: 90.6250 (93.9762)  time: 3.4168  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:36  loss: 1.3417 (1.0527)  acc1: 65.6250 (75.8929)  acc5: 89.5833 (93.7545)  time: 3.4179  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:02  loss: 1.3623 (1.0713)  acc1: 65.6250 (75.4582)  acc5: 89.5833 (93.5123)  time: 3.4184  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:27  loss: 1.5199 (1.0929)  acc1: 66.6667 (75.0789)  acc5: 87.5000 (93.1897)  time: 3.4166  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:14:53  loss: 1.5515 (1.1129)  acc1: 62.5000 (74.5889)  acc5: 87.5000 (92.9278)  time: 3.4174  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:19  loss: 1.4821 (1.1342)  acc1: 60.4167 (74.0314)  acc5: 87.5000 (92.6622)  time: 3.4174  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:44  loss: 1.4821 (1.1459)  acc1: 63.5417 (73.7989)  acc5: 87.5000 (92.5007)  time: 3.4174  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:10  loss: 1.4913 (1.1556)  acc1: 67.7083 (73.6147)  acc5: 87.5000 (92.3540)  time: 3.4178  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:36  loss: 1.0739 (1.1528)  acc1: 76.0417 (73.7749)  acc5: 91.6667 (92.3380)  time: 3.4180  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:12:02  loss: 1.1900 (1.1689)  acc1: 70.8333 (73.4425)  acc5: 89.5833 (92.1490)  time: 3.4176  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:27  loss: 1.4082 (1.1737)  acc1: 70.8333 (73.4359)  acc5: 87.5000 (92.0204)  time: 3.4183  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:53  loss: 1.3965 (1.1917)  acc1: 70.8333 (73.0016)  acc5: 89.5833 (91.7894)  time: 3.4190  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:19  loss: 1.5934 (1.2026)  acc1: 61.4583 (72.7395)  acc5: 87.5000 (91.6208)  time: 3.4179  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:45  loss: 1.5934 (1.2121)  acc1: 62.5000 (72.5042)  acc5: 87.5000 (91.5391)  time: 3.4182  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:10  loss: 1.5310 (1.2241)  acc1: 62.5000 (72.2732)  acc5: 86.4583 (91.3579)  time: 3.4182  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:36  loss: 1.5113 (1.2315)  acc1: 66.6667 (72.1474)  acc5: 88.5417 (91.2820)  time: 3.4173  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:02  loss: 1.3561 (1.2362)  acc1: 71.8750 (72.1402)  acc5: 89.5833 (91.1773)  time: 3.4186  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:28  loss: 1.4101 (1.2471)  acc1: 64.5833 (71.8457)  acc5: 87.5000 (90.9820)  time: 3.4200  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:54  loss: 1.5559 (1.2535)  acc1: 64.5833 (71.7633)  acc5: 85.4167 (90.8926)  time: 3.4184  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:19  loss: 1.5052 (1.2599)  acc1: 67.7083 (71.6722)  acc5: 86.4583 (90.7619)  time: 3.4183  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:45  loss: 1.5052 (1.2642)  acc1: 65.6250 (71.6300)  acc5: 86.4583 (90.7017)  time: 3.4186  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:11  loss: 1.5073 (1.2716)  acc1: 65.6250 (71.4279)  acc5: 87.5000 (90.6202)  time: 3.4178  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:37  loss: 1.7117 (1.2841)  acc1: 58.3333 (71.1404)  acc5: 84.3750 (90.4290)  time: 3.4174  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:02  loss: 1.6219 (1.2889)  acc1: 64.5833 (71.0089)  acc5: 85.4167 (90.3640)  time: 3.4166  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:28  loss: 1.5406 (1.2924)  acc1: 66.6667 (70.9011)  acc5: 87.5000 (90.3313)  time: 3.4166  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:54  loss: 1.4305 (1.2985)  acc1: 67.7083 (70.7758)  acc5: 88.5417 (90.2512)  time: 3.4170  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.3828 (1.3029)  acc1: 67.7083 (70.6558)  acc5: 89.5833 (90.2244)  time: 3.4174  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:46  loss: 1.0995 (1.2982)  acc1: 71.8750 (70.7336)  acc5: 92.7083 (90.3068)  time: 3.4191  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0608 (1.2920)  acc1: 76.0417 (70.8832)  acc5: 94.7917 (90.3776)  time: 3.4190  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1332 (1.2972)  acc1: 73.9583 (70.7131)  acc5: 92.7083 (90.3294)  time: 3.4172  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2098 (1.2885)  acc1: 70.8333 (70.9380)  acc5: 91.6667 (90.4100)  time: 3.3946  data: 0.0001  max mem: 16226
Test: Total time: 0:29:41 (3.4200 s / it)
* Acc@1 70.938 Acc@5 90.410 loss 1.288
Accuracy of the network on the 50000 test images: 70.9%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.06, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.06
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:16  loss: 0.5544 (0.5544)  acc1: 86.4583 (86.4583)  acc5: 97.9167 (97.9167)  time: 5.0982  data: 1.1094  max mem: 16225
Test:  [ 10/521]  eta: 0:30:19  loss: 0.5653 (0.6694)  acc1: 87.5000 (86.4583)  acc5: 97.9167 (96.9697)  time: 3.5600  data: 0.1012  max mem: 16226
Test:  [ 20/521]  eta: 0:29:06  loss: 0.8085 (0.8738)  acc1: 84.3750 (81.4980)  acc5: 95.8333 (95.0893)  time: 3.4054  data: 0.0003  max mem: 16226
Test:  [ 30/521]  eta: 0:28:17  loss: 1.0561 (0.9824)  acc1: 76.0417 (78.7634)  acc5: 91.6667 (93.9516)  time: 3.4020  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:36  loss: 1.2196 (1.0549)  acc1: 70.8333 (76.4228)  acc5: 91.6667 (93.5468)  time: 3.3977  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:26:57  loss: 0.7201 (0.9783)  acc1: 86.4583 (78.7990)  acc5: 95.8333 (94.0360)  time: 3.3967  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:20  loss: 0.7132 (0.9663)  acc1: 85.4167 (79.2179)  acc5: 95.8333 (94.1257)  time: 3.3975  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:44  loss: 0.8549 (0.9671)  acc1: 80.2083 (79.2107)  acc5: 93.7500 (94.0581)  time: 3.3998  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:08  loss: 0.7343 (0.9409)  acc1: 83.3333 (79.8483)  acc5: 95.8333 (94.3416)  time: 3.3978  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:24:33  loss: 1.0084 (0.9800)  acc1: 76.0417 (78.5027)  acc5: 95.8333 (94.1392)  time: 3.3958  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:23:58  loss: 1.2400 (1.0141)  acc1: 64.5833 (77.3927)  acc5: 91.6667 (93.9975)  time: 3.3978  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:23  loss: 1.1830 (1.0205)  acc1: 69.7917 (77.0833)  acc5: 93.7500 (94.0128)  time: 3.3968  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:22:48  loss: 1.0287 (1.0272)  acc1: 75.0000 (76.9370)  acc5: 93.7500 (93.8619)  time: 3.3971  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:13  loss: 1.1806 (1.0371)  acc1: 73.9583 (76.2484)  acc5: 93.7500 (93.9886)  time: 3.3973  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:21:39  loss: 0.9913 (1.0295)  acc1: 79.1667 (76.4111)  acc5: 94.7917 (94.0603)  time: 3.3962  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:04  loss: 0.9543 (1.0392)  acc1: 79.1667 (76.0279)  acc5: 94.7917 (94.0673)  time: 3.3963  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:30  loss: 1.0720 (1.0301)  acc1: 77.0833 (76.3005)  acc5: 94.7917 (94.1641)  time: 3.3965  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:19:56  loss: 0.8461 (1.0214)  acc1: 83.3333 (76.5046)  acc5: 94.7917 (94.2069)  time: 3.3985  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:22  loss: 0.8780 (1.0153)  acc1: 81.2500 (76.7783)  acc5: 94.7917 (94.2507)  time: 3.3989  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:47  loss: 0.9138 (1.0123)  acc1: 79.1667 (76.8161)  acc5: 94.7917 (94.2954)  time: 3.3979  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:13  loss: 0.9936 (1.0195)  acc1: 76.0417 (76.7154)  acc5: 93.7500 (94.2475)  time: 3.3970  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:39  loss: 1.0010 (1.0181)  acc1: 76.0417 (76.7081)  acc5: 93.7500 (94.1844)  time: 3.3966  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:05  loss: 1.1726 (1.0408)  acc1: 68.7500 (76.1831)  acc5: 91.6667 (93.9008)  time: 3.3956  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:30  loss: 1.3226 (1.0576)  acc1: 67.7083 (75.7395)  acc5: 89.5833 (93.7004)  time: 3.3949  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:15:56  loss: 1.3226 (1.0762)  acc1: 62.5000 (75.2637)  acc5: 88.5417 (93.4388)  time: 3.3959  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:22  loss: 1.4810 (1.0984)  acc1: 62.5000 (74.8589)  acc5: 86.4583 (93.0694)  time: 3.3959  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:48  loss: 1.6047 (1.1188)  acc1: 61.4583 (74.3415)  acc5: 84.3750 (92.8001)  time: 3.3950  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:14  loss: 1.5035 (1.1403)  acc1: 61.4583 (73.8007)  acc5: 86.4583 (92.5277)  time: 3.3950  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:40  loss: 1.5035 (1.1522)  acc1: 64.5833 (73.5988)  acc5: 86.4583 (92.3562)  time: 3.3970  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:06  loss: 1.5284 (1.1614)  acc1: 68.7500 (73.3999)  acc5: 87.5000 (92.1893)  time: 3.3975  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:32  loss: 1.0932 (1.1585)  acc1: 71.8750 (73.5569)  acc5: 90.6250 (92.1927)  time: 3.3971  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:11:58  loss: 1.2140 (1.1748)  acc1: 68.7500 (73.2181)  acc5: 90.6250 (91.9681)  time: 3.3974  data: 0.0005  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.3999 (1.1796)  acc1: 69.7917 (73.2250)  acc5: 88.5417 (91.8419)  time: 3.3969  data: 0.0005  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3999 (1.1989)  acc1: 70.8333 (72.7719)  acc5: 88.5417 (91.5502)  time: 3.3965  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.6416 (1.2098)  acc1: 58.3333 (72.5104)  acc5: 87.5000 (91.4131)  time: 3.3957  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.6416 (1.2186)  acc1: 61.4583 (72.2875)  acc5: 87.5000 (91.3402)  time: 3.3974  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5158 (1.2312)  acc1: 61.4583 (72.0193)  acc5: 86.4583 (91.1415)  time: 3.3992  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.5035 (1.2386)  acc1: 65.6250 (71.8750)  acc5: 88.5417 (91.0742)  time: 3.3976  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.3967 (1.2435)  acc1: 69.7917 (71.8777)  acc5: 89.5833 (90.9750)  time: 3.3962  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.4154 (1.2547)  acc1: 62.5000 (71.5633)  acc5: 86.4583 (90.7955)  time: 3.3951  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.5554 (1.2609)  acc1: 62.5000 (71.4750)  acc5: 85.4167 (90.7029)  time: 3.3975  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.5085 (1.2679)  acc1: 66.6667 (71.3656)  acc5: 84.3750 (90.5794)  time: 3.3995  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.5085 (1.2722)  acc1: 66.6667 (71.3307)  acc5: 85.4167 (90.5062)  time: 3.3966  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.5534 (1.2791)  acc1: 65.6250 (71.1379)  acc5: 87.5000 (90.4389)  time: 3.3944  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.7480 (1.2917)  acc1: 55.2083 (70.8381)  acc5: 84.3750 (90.2471)  time: 3.3942  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.5986 (1.2962)  acc1: 61.4583 (70.7340)  acc5: 85.4167 (90.1862)  time: 3.3954  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.5110 (1.2993)  acc1: 66.6667 (70.6368)  acc5: 87.5000 (90.1595)  time: 3.3975  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.3982 (1.3055)  acc1: 67.7083 (70.5171)  acc5: 88.5417 (90.0787)  time: 3.3961  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.3656 (1.3101)  acc1: 68.7500 (70.3872)  acc5: 89.5833 (90.0511)  time: 3.3952  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1756 (1.3055)  acc1: 70.8333 (70.4706)  acc5: 91.6667 (90.1243)  time: 3.3980  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0212 (1.2990)  acc1: 75.0000 (70.6254)  acc5: 93.7500 (90.1967)  time: 3.3984  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1453 (1.3046)  acc1: 72.9167 (70.4481)  acc5: 92.7083 (90.1378)  time: 3.3959  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2226 (1.2957)  acc1: 71.8750 (70.6720)  acc5: 91.6667 (90.2320)  time: 3.3731  data: 0.0001  max mem: 16226
Test: Total time: 0:29:31 (3.3997 s / it)
* Acc@1 70.672 Acc@5 90.232 loss 1.296
Accuracy of the network on the 50000 test images: 70.7%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.07, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.07
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:12  loss: 0.5398 (0.5398)  acc1: 90.6250 (90.6250)  acc5: 97.9167 (97.9167)  time: 4.9756  data: 1.0205  max mem: 16225
Test:  [ 10/521]  eta: 0:30:10  loss: 0.6150 (0.6748)  acc1: 89.5833 (87.5947)  acc5: 97.9167 (96.8750)  time: 3.5423  data: 0.0929  max mem: 16226
Test:  [ 20/521]  eta: 0:29:00  loss: 0.8282 (0.8613)  acc1: 82.2917 (82.1925)  acc5: 96.8750 (95.3373)  time: 3.4000  data: 0.0001  max mem: 16226
Test:  [ 30/521]  eta: 0:28:13  loss: 1.0754 (0.9754)  acc1: 73.9583 (79.2003)  acc5: 91.6667 (93.8844)  time: 3.3987  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:33  loss: 1.2144 (1.0425)  acc1: 71.8750 (76.8039)  acc5: 91.6667 (93.6992)  time: 3.3984  data: 0.0003  max mem: 16226
Test:  [ 50/521]  eta: 0:26:55  loss: 0.7414 (0.9660)  acc1: 87.5000 (79.1054)  acc5: 94.7917 (94.2198)  time: 3.3981  data: 0.0003  max mem: 16226
Test:  [ 60/521]  eta: 0:26:18  loss: 0.7414 (0.9576)  acc1: 84.3750 (79.5424)  acc5: 95.8333 (94.2281)  time: 3.3974  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:42  loss: 0.8301 (0.9598)  acc1: 82.2917 (79.4014)  acc5: 94.7917 (94.2048)  time: 3.3966  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:06  loss: 0.7412 (0.9333)  acc1: 84.3750 (80.0283)  acc5: 96.8750 (94.4959)  time: 3.3946  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:31  loss: 1.0050 (0.9763)  acc1: 75.0000 (78.5829)  acc5: 94.7917 (94.2651)  time: 3.3953  data: 0.0003  max mem: 16226
Test:  [100/521]  eta: 0:23:56  loss: 1.2343 (1.0099)  acc1: 68.7500 (77.5474)  acc5: 92.7083 (94.1729)  time: 3.3957  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:22  loss: 1.1910 (1.0172)  acc1: 72.9167 (77.2335)  acc5: 93.7500 (94.2192)  time: 3.3956  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:47  loss: 1.0440 (1.0248)  acc1: 76.0417 (77.0919)  acc5: 93.7500 (94.0427)  time: 3.3963  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:12  loss: 1.2036 (1.0349)  acc1: 71.8750 (76.3359)  acc5: 92.7083 (94.1078)  time: 3.3975  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:38  loss: 1.0109 (1.0280)  acc1: 73.9583 (76.3593)  acc5: 94.7917 (94.1563)  time: 3.3963  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:04  loss: 0.9704 (1.0386)  acc1: 77.0833 (75.9451)  acc5: 94.7917 (94.1363)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:29  loss: 1.0548 (1.0303)  acc1: 73.9583 (76.2616)  acc5: 94.7917 (94.2158)  time: 3.3949  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:19:55  loss: 0.8322 (1.0232)  acc1: 82.2917 (76.3889)  acc5: 94.7917 (94.2617)  time: 3.3957  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:21  loss: 0.8673 (1.0170)  acc1: 80.2083 (76.6344)  acc5: 94.7917 (94.3255)  time: 3.3973  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:47  loss: 0.8945 (1.0145)  acc1: 79.1667 (76.6634)  acc5: 94.7917 (94.3608)  time: 3.3957  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 0.9619 (1.0216)  acc1: 75.0000 (76.5236)  acc5: 92.7083 (94.2786)  time: 3.3939  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 1.0306 (1.0206)  acc1: 75.0000 (76.5205)  acc5: 92.7083 (94.1993)  time: 3.3945  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:04  loss: 1.1640 (1.0434)  acc1: 67.7083 (75.9898)  acc5: 91.6667 (93.9197)  time: 3.3945  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:30  loss: 1.3156 (1.0594)  acc1: 66.6667 (75.6043)  acc5: 90.6250 (93.7184)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:15:56  loss: 1.3832 (1.0786)  acc1: 67.7083 (75.1253)  acc5: 88.5417 (93.4388)  time: 3.3965  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:22  loss: 1.5510 (1.1001)  acc1: 65.6250 (74.7344)  acc5: 86.4583 (93.0735)  time: 3.3976  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:48  loss: 1.5899 (1.1210)  acc1: 61.4583 (74.2337)  acc5: 84.3750 (92.7762)  time: 3.3963  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.4888 (1.1424)  acc1: 61.4583 (73.6931)  acc5: 85.4167 (92.4892)  time: 3.3938  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.4888 (1.1543)  acc1: 63.5417 (73.4727)  acc5: 86.4583 (92.3191)  time: 3.3936  data: 0.0003  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.5302 (1.1634)  acc1: 68.7500 (73.3104)  acc5: 86.4583 (92.1857)  time: 3.3944  data: 0.0003  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.0984 (1.1603)  acc1: 73.9583 (73.4773)  acc5: 91.6667 (92.1927)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.2233 (1.1764)  acc1: 71.8750 (73.1478)  acc5: 91.6667 (91.9916)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.4036 (1.1813)  acc1: 69.7917 (73.1081)  acc5: 88.5417 (91.8711)  time: 3.3946  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.4036 (1.2006)  acc1: 69.7917 (72.6743)  acc5: 89.5833 (91.6069)  time: 3.3966  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.5978 (1.2114)  acc1: 59.3750 (72.4035)  acc5: 88.5417 (91.4956)  time: 3.3971  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.5978 (1.2209)  acc1: 59.3750 (72.1302)  acc5: 88.5417 (91.4203)  time: 3.3963  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5803 (1.2340)  acc1: 61.4583 (71.8779)  acc5: 87.5000 (91.2223)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.5311 (1.2413)  acc1: 64.5833 (71.7458)  acc5: 88.5417 (91.1472)  time: 3.3945  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.3712 (1.2457)  acc1: 68.7500 (71.7356)  acc5: 89.5833 (91.0515)  time: 3.3952  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.4548 (1.2562)  acc1: 65.6250 (71.4328)  acc5: 86.4583 (90.8808)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.5809 (1.2631)  acc1: 63.5417 (71.3477)  acc5: 85.4167 (90.7939)  time: 3.3950  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.5173 (1.2697)  acc1: 66.6667 (71.2515)  acc5: 86.4583 (90.6858)  time: 3.3945  data: 0.0003  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.5137 (1.2742)  acc1: 66.6667 (71.2218)  acc5: 85.4167 (90.5978)  time: 3.3950  data: 0.0003  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.5612 (1.2817)  acc1: 66.6667 (71.0339)  acc5: 85.4167 (90.5187)  time: 3.3955  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.7485 (1.2945)  acc1: 57.2917 (70.7389)  acc5: 82.2917 (90.3250)  time: 3.3953  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.5825 (1.2995)  acc1: 64.5833 (70.6278)  acc5: 85.4167 (90.2624)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.5446 (1.3027)  acc1: 67.7083 (70.5283)  acc5: 88.5417 (90.2364)  time: 3.3958  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.4261 (1.3094)  acc1: 67.7083 (70.3777)  acc5: 88.5417 (90.1384)  time: 3.3970  data: 0.0003  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.3558 (1.3140)  acc1: 68.7500 (70.2356)  acc5: 87.5000 (90.0988)  time: 3.3952  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1616 (1.3096)  acc1: 68.7500 (70.3114)  acc5: 90.6250 (90.1668)  time: 3.3955  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0407 (1.3031)  acc1: 76.0417 (70.4674)  acc5: 94.7917 (90.2362)  time: 3.3964  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1290 (1.3085)  acc1: 72.9167 (70.2829)  acc5: 92.7083 (90.1765)  time: 3.3969  data: 0.0002  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2257 (1.2995)  acc1: 69.7917 (70.5180)  acc5: 91.6667 (90.2660)  time: 3.3742  data: 0.0001  max mem: 16226
Test: Total time: 0:29:30 (3.3982 s / it)
* Acc@1 70.518 Acc@5 90.266 loss 1.299
Accuracy of the network on the 50000 test images: 70.5%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.08, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.08
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:42  loss: 0.5529 (0.5529)  acc1: 86.4583 (86.4583)  acc5: 98.9583 (98.9583)  time: 5.0338  data: 1.0696  max mem: 16225
Test:  [ 10/521]  eta: 0:30:10  loss: 0.5984 (0.6709)  acc1: 87.5000 (87.3106)  acc5: 97.9167 (96.9697)  time: 3.5433  data: 0.0974  max mem: 16226
Test:  [ 20/521]  eta: 0:29:00  loss: 0.8488 (0.8721)  acc1: 81.2500 (81.2500)  acc5: 96.8750 (95.5853)  time: 3.3956  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:13  loss: 1.0709 (0.9807)  acc1: 75.0000 (78.3266)  acc5: 92.7083 (94.0860)  time: 3.3965  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:32  loss: 1.2170 (1.0511)  acc1: 69.7917 (75.8384)  acc5: 90.6250 (93.6738)  time: 3.3976  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:26:54  loss: 0.7195 (0.9746)  acc1: 86.4583 (78.2271)  acc5: 95.8333 (94.1381)  time: 3.3964  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:17  loss: 0.7371 (0.9673)  acc1: 83.3333 (78.6373)  acc5: 95.8333 (94.1257)  time: 3.3941  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:42  loss: 0.8604 (0.9708)  acc1: 81.2500 (78.6238)  acc5: 94.7917 (94.0288)  time: 3.3964  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:06  loss: 0.7381 (0.9451)  acc1: 83.3333 (79.3982)  acc5: 95.8333 (94.2773)  time: 3.3965  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:31  loss: 1.0275 (0.9864)  acc1: 77.0833 (78.0105)  acc5: 94.7917 (94.0362)  time: 3.3949  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:23:56  loss: 1.2603 (1.0182)  acc1: 65.6250 (77.0008)  acc5: 92.7083 (93.9356)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:21  loss: 1.1571 (1.0251)  acc1: 71.8750 (76.5953)  acc5: 94.7917 (94.0315)  time: 3.3953  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:47  loss: 1.0387 (1.0326)  acc1: 73.9583 (76.3946)  acc5: 93.7500 (93.8705)  time: 3.3975  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:12  loss: 1.2108 (1.0427)  acc1: 71.8750 (75.6838)  acc5: 93.7500 (93.9567)  time: 3.4002  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:38  loss: 0.9980 (1.0355)  acc1: 73.9583 (75.8644)  acc5: 94.7917 (94.0603)  time: 3.3974  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:04  loss: 0.9626 (1.0447)  acc1: 78.1250 (75.5864)  acc5: 94.7917 (94.0811)  time: 3.3959  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:29  loss: 1.0856 (1.0360)  acc1: 77.0833 (75.9446)  acc5: 94.7917 (94.1511)  time: 3.3971  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:19:55  loss: 0.8362 (1.0283)  acc1: 82.2917 (76.1452)  acc5: 94.7917 (94.1886)  time: 3.3949  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:21  loss: 0.8662 (1.0227)  acc1: 79.1667 (76.3870)  acc5: 94.7917 (94.2564)  time: 3.3933  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:46  loss: 0.9126 (1.0199)  acc1: 78.1250 (76.4180)  acc5: 94.7917 (94.2790)  time: 3.3934  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 0.9746 (1.0269)  acc1: 76.0417 (76.3422)  acc5: 93.7500 (94.2164)  time: 3.3963  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 1.0083 (1.0258)  acc1: 76.0417 (76.3527)  acc5: 93.7500 (94.1696)  time: 3.3985  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:04  loss: 1.1739 (1.0489)  acc1: 70.8333 (75.8673)  acc5: 91.6667 (93.8867)  time: 3.3988  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:30  loss: 1.3586 (1.0668)  acc1: 67.7083 (75.4284)  acc5: 88.5417 (93.6147)  time: 3.3965  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:15:56  loss: 1.3864 (1.0861)  acc1: 65.6250 (74.9957)  acc5: 86.4583 (93.3264)  time: 3.3960  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:22  loss: 1.5481 (1.1075)  acc1: 65.6250 (74.6514)  acc5: 85.4167 (92.9532)  time: 3.3983  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:48  loss: 1.5637 (1.1279)  acc1: 61.4583 (74.2058)  acc5: 85.4167 (92.6964)  time: 3.3967  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.5604 (1.1494)  acc1: 61.4583 (73.6739)  acc5: 85.4167 (92.3816)  time: 3.3953  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.5319 (1.1616)  acc1: 63.5417 (73.4690)  acc5: 85.4167 (92.2116)  time: 3.3968  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.5319 (1.1708)  acc1: 67.7083 (73.3033)  acc5: 87.5000 (92.0783)  time: 3.3961  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.0508 (1.1675)  acc1: 71.8750 (73.4635)  acc5: 91.6667 (92.0750)  time: 3.3943  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.1674 (1.1837)  acc1: 71.8750 (73.1143)  acc5: 90.6250 (91.8676)  time: 3.3989  data: 0.0003  max mem: 16226
Test:  [320/521]  eta: 0:11:27  loss: 1.3995 (1.1891)  acc1: 68.7500 (73.0757)  acc5: 87.5000 (91.7608)  time: 3.7305  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:57  loss: 1.4004 (1.2082)  acc1: 68.7500 (72.6775)  acc5: 89.5833 (91.4841)  time: 4.1078  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:28  loss: 1.5794 (1.2191)  acc1: 59.3750 (72.4157)  acc5: 86.4583 (91.3368)  time: 4.2475  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:56  loss: 1.5794 (1.2288)  acc1: 60.4167 (72.1362)  acc5: 86.4583 (91.2453)  time: 4.2556  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:26  loss: 1.5492 (1.2419)  acc1: 62.5000 (71.8808)  acc5: 86.4583 (91.0232)  time: 4.2848  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:53  loss: 1.5298 (1.2496)  acc1: 63.5417 (71.7346)  acc5: 85.4167 (90.9423)  time: 4.2872  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:20  loss: 1.3552 (1.2544)  acc1: 68.7500 (71.7137)  acc5: 90.6250 (90.8465)  time: 4.1977  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:47  loss: 1.4245 (1.2654)  acc1: 65.6250 (71.4194)  acc5: 87.5000 (90.6596)  time: 4.2908  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:07:14  loss: 1.5876 (1.2720)  acc1: 65.6250 (71.3295)  acc5: 85.4167 (90.5575)  time: 4.2720  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:40  loss: 1.4953 (1.2784)  acc1: 67.7083 (71.2084)  acc5: 85.4167 (90.4451)  time: 4.2895  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:06:05  loss: 1.4861 (1.2827)  acc1: 66.6667 (71.1599)  acc5: 86.4583 (90.3800)  time: 4.2943  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:30  loss: 1.5062 (1.2900)  acc1: 64.5833 (70.9759)  acc5: 88.5417 (90.2939)  time: 4.1884  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:55  loss: 1.7520 (1.3031)  acc1: 56.2500 (70.6467)  acc5: 83.3333 (90.1054)  time: 4.2911  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:19  loss: 1.5928 (1.3079)  acc1: 60.4167 (70.5400)  acc5: 85.4167 (90.0453)  time: 3.9651  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:42  loss: 1.5422 (1.3114)  acc1: 63.5417 (70.4153)  acc5: 87.5000 (90.0217)  time: 3.4707  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:03:05  loss: 1.4140 (1.3176)  acc1: 64.5833 (70.2981)  acc5: 87.5000 (89.9283)  time: 3.4061  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:28  loss: 1.3744 (1.3225)  acc1: 69.7917 (70.1685)  acc5: 88.5417 (89.8952)  time: 3.4072  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:52  loss: 1.1399 (1.3180)  acc1: 70.8333 (70.2478)  acc5: 90.6250 (89.9652)  time: 3.4087  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:16  loss: 1.0566 (1.3118)  acc1: 75.0000 (70.3801)  acc5: 94.7917 (90.0366)  time: 3.4124  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:39  loss: 1.1719 (1.3171)  acc1: 71.8750 (70.1973)  acc5: 91.6667 (89.9971)  time: 3.4149  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2144 (1.3081)  acc1: 71.8750 (70.4340)  acc5: 92.7083 (90.0980)  time: 3.3970  data: 0.0001  max mem: 16226
Test: Total time: 0:31:24 (3.6163 s / it)
* Acc@1 70.434 Acc@5 90.098 loss 1.308
Accuracy of the network on the 50000 test images: 70.4%
