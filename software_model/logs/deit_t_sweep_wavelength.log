nohup: ignoring input
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=8, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 8
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:42:40  loss: 0.5645 (0.5645)  acc1: 87.5000 (87.5000)  acc5: 97.9167 (97.9167)  time: 4.9151  data: 0.9797  max mem: 16225
Test:  [ 10/521]  eta: 0:29:48  loss: 0.5894 (0.6943)  acc1: 87.5000 (86.1742)  acc5: 97.9167 (96.8750)  time: 3.5003  data: 0.0893  max mem: 16226
Test:  [ 20/521]  eta: 0:28:40  loss: 0.8632 (0.8756)  acc1: 81.2500 (81.8452)  acc5: 95.8333 (95.3373)  time: 3.3601  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:27:54  loss: 1.0399 (0.9808)  acc1: 75.0000 (79.0995)  acc5: 91.6667 (94.2204)  time: 3.3623  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:27:15  loss: 1.1475 (1.0476)  acc1: 70.8333 (76.6006)  acc5: 92.7083 (93.8516)  time: 3.3654  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:26:38  loss: 0.7263 (0.9750)  acc1: 85.4167 (78.8807)  acc5: 95.8333 (94.2198)  time: 3.3695  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:03  loss: 0.7347 (0.9657)  acc1: 85.4167 (79.5765)  acc5: 95.8333 (94.1428)  time: 3.3720  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:25:28  loss: 0.8713 (0.9691)  acc1: 83.3333 (79.5188)  acc5: 93.7500 (94.0581)  time: 3.3761  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:24:54  loss: 0.7339 (0.9434)  acc1: 84.3750 (80.2726)  acc5: 95.8333 (94.3030)  time: 3.3803  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:24:19  loss: 1.0058 (0.9827)  acc1: 78.1250 (79.0522)  acc5: 95.8333 (94.0820)  time: 3.3793  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:23:45  loss: 1.2329 (1.0161)  acc1: 67.7083 (77.9290)  acc5: 92.7083 (93.9150)  time: 3.3787  data: 0.0001  max mem: 16226
Test:  [110/521]  eta: 0:23:11  loss: 1.1766 (1.0229)  acc1: 68.7500 (77.6089)  acc5: 94.7917 (94.0409)  time: 3.3810  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:22:37  loss: 1.0336 (1.0293)  acc1: 76.0417 (77.4277)  acc5: 94.7917 (93.9480)  time: 3.3843  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:03  loss: 1.0939 (1.0375)  acc1: 70.8333 (76.7494)  acc5: 94.7917 (94.0283)  time: 3.3857  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:21:30  loss: 0.9941 (1.0270)  acc1: 76.0417 (76.9577)  acc5: 94.7917 (94.1563)  time: 3.3860  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:20:56  loss: 0.9473 (1.0366)  acc1: 78.1250 (76.6418)  acc5: 94.7917 (94.1363)  time: 3.3852  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:22  loss: 1.0537 (1.0284)  acc1: 77.0833 (76.8828)  acc5: 94.7917 (94.1964)  time: 3.3846  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:19:48  loss: 0.8089 (1.0211)  acc1: 79.1667 (77.0772)  acc5: 94.7917 (94.2434)  time: 3.3850  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:14  loss: 0.9309 (1.0140)  acc1: 80.2083 (77.2675)  acc5: 94.7917 (94.2910)  time: 3.3868  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:18:40  loss: 0.9031 (1.0115)  acc1: 80.2083 (77.2633)  acc5: 94.7917 (94.3336)  time: 3.3879  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:07  loss: 0.9869 (1.0182)  acc1: 77.0833 (77.0989)  acc5: 93.7500 (94.2527)  time: 3.3904  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:33  loss: 0.9869 (1.0169)  acc1: 77.0833 (77.1376)  acc5: 92.7083 (94.1993)  time: 3.3919  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:16:59  loss: 1.1728 (1.0395)  acc1: 70.8333 (76.5884)  acc5: 91.6667 (93.9103)  time: 3.3902  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:25  loss: 1.3167 (1.0544)  acc1: 65.6250 (76.1815)  acc5: 90.6250 (93.7229)  time: 3.3916  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:15:51  loss: 1.4051 (1.0732)  acc1: 65.6250 (75.7218)  acc5: 89.5833 (93.4950)  time: 3.3921  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:18  loss: 1.4927 (1.0960)  acc1: 65.6250 (75.2947)  acc5: 87.5000 (93.1275)  time: 3.3925  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:14:44  loss: 1.5832 (1.1170)  acc1: 61.4583 (74.7965)  acc5: 84.3750 (92.8440)  time: 3.3928  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:10  loss: 1.4885 (1.1379)  acc1: 60.4167 (74.2505)  acc5: 87.5000 (92.6315)  time: 3.3927  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:36  loss: 1.4820 (1.1489)  acc1: 63.5417 (74.0139)  acc5: 88.5417 (92.4822)  time: 3.3924  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:02  loss: 1.4370 (1.1589)  acc1: 68.7500 (73.8330)  acc5: 88.5417 (92.3146)  time: 3.3935  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:28  loss: 1.1359 (1.1558)  acc1: 72.9167 (73.9999)  acc5: 91.6667 (92.3000)  time: 3.3935  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:11:54  loss: 1.1359 (1.1711)  acc1: 71.8750 (73.6435)  acc5: 91.6667 (92.0987)  time: 3.3933  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:21  loss: 1.3215 (1.1760)  acc1: 68.7500 (73.6241)  acc5: 88.5417 (91.9977)  time: 3.3943  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:47  loss: 1.3818 (1.1949)  acc1: 68.7500 (73.1495)  acc5: 88.5417 (91.7485)  time: 3.3942  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:13  loss: 1.6154 (1.2047)  acc1: 64.5833 (72.9472)  acc5: 85.4167 (91.6178)  time: 3.3944  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:39  loss: 1.6069 (1.2135)  acc1: 64.5833 (72.6852)  acc5: 85.4167 (91.5391)  time: 3.3973  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:05  loss: 1.5283 (1.2259)  acc1: 64.5833 (72.4319)  acc5: 87.5000 (91.3320)  time: 3.3975  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:31  loss: 1.4824 (1.2326)  acc1: 64.5833 (72.2821)  acc5: 87.5000 (91.2708)  time: 3.3947  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:07:58  loss: 1.3877 (1.2368)  acc1: 69.7917 (72.2933)  acc5: 88.5417 (91.1663)  time: 3.3999  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:24  loss: 1.4232 (1.2476)  acc1: 67.7083 (71.9949)  acc5: 87.5000 (90.9926)  time: 3.3994  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:50  loss: 1.5326 (1.2531)  acc1: 65.6250 (71.9218)  acc5: 86.4583 (90.9030)  time: 3.3933  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:16  loss: 1.5326 (1.2598)  acc1: 68.7500 (71.8192)  acc5: 86.4583 (90.7720)  time: 3.3934  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:42  loss: 1.5396 (1.2636)  acc1: 68.7500 (71.8057)  acc5: 86.4583 (90.6992)  time: 3.3931  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:08  loss: 1.5636 (1.2711)  acc1: 68.7500 (71.6067)  acc5: 86.4583 (90.6419)  time: 3.3942  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:34  loss: 1.7956 (1.2832)  acc1: 57.2917 (71.3459)  acc5: 84.3750 (90.4738)  time: 3.3944  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:00  loss: 1.5376 (1.2874)  acc1: 62.5000 (71.2283)  acc5: 86.4583 (90.4218)  time: 3.3932  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:26  loss: 1.4786 (1.2898)  acc1: 66.6667 (71.1339)  acc5: 88.5417 (90.4126)  time: 3.3961  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:52  loss: 1.3702 (1.2952)  acc1: 69.7917 (71.0235)  acc5: 88.5417 (90.3353)  time: 3.4001  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.3834 (1.2996)  acc1: 69.7917 (70.8723)  acc5: 88.5417 (90.3088)  time: 3.3973  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1732 (1.2951)  acc1: 70.8333 (70.9458)  acc5: 91.6667 (90.3747)  time: 3.3954  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0258 (1.2887)  acc1: 76.0417 (71.1015)  acc5: 94.7917 (90.4379)  time: 3.3962  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1514 (1.2938)  acc1: 71.8750 (70.9495)  acc5: 91.6667 (90.3865)  time: 3.3954  data: 0.0002  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1921 (1.2846)  acc1: 71.8750 (71.1740)  acc5: 91.6667 (90.4740)  time: 3.3726  data: 0.0001  max mem: 16226
Test: Total time: 0:29:26 (3.3907 s / it)
* Acc@1 71.174 Acc@5 90.474 loss 1.285
Accuracy of the network on the 50000 test images: 71.2%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:26  loss: 0.5769 (0.5769)  acc1: 89.5833 (89.5833)  acc5: 97.9167 (97.9167)  time: 5.0029  data: 0.9888  max mem: 16225
Test:  [ 10/521]  eta: 0:30:08  loss: 0.5769 (0.6858)  acc1: 89.5833 (86.5530)  acc5: 97.9167 (96.9697)  time: 3.5394  data: 0.0901  max mem: 16226
Test:  [ 20/521]  eta: 0:28:58  loss: 0.8459 (0.8663)  acc1: 81.2500 (81.6468)  acc5: 95.8333 (95.3869)  time: 3.3927  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:11  loss: 1.0805 (0.9779)  acc1: 76.0417 (78.7634)  acc5: 92.7083 (94.2204)  time: 3.3924  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:31  loss: 1.1603 (1.0473)  acc1: 70.8333 (76.3211)  acc5: 91.6667 (93.8262)  time: 3.3959  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:26:53  loss: 0.7606 (0.9742)  acc1: 84.3750 (78.6152)  acc5: 95.8333 (94.2810)  time: 3.3968  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:16  loss: 0.7259 (0.9649)  acc1: 85.4167 (79.2691)  acc5: 95.8333 (94.2623)  time: 3.3941  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:41  loss: 0.8552 (0.9655)  acc1: 82.2917 (79.1960)  acc5: 94.7917 (94.1608)  time: 3.3943  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:05  loss: 0.7517 (0.9415)  acc1: 82.2917 (79.8225)  acc5: 95.8333 (94.3930)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:30  loss: 1.0319 (0.9798)  acc1: 76.0417 (78.5829)  acc5: 94.7917 (94.1735)  time: 3.3938  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:23:55  loss: 1.2399 (1.0124)  acc1: 68.7500 (77.4546)  acc5: 92.7083 (94.0903)  time: 3.3940  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:21  loss: 1.1789 (1.0176)  acc1: 69.7917 (77.1866)  acc5: 94.7917 (94.1817)  time: 3.3970  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:46  loss: 1.0264 (1.0239)  acc1: 73.9583 (77.0489)  acc5: 93.7500 (94.0685)  time: 3.3961  data: 0.0003  max mem: 16226
Test:  [130/521]  eta: 0:22:12  loss: 1.1433 (1.0319)  acc1: 72.9167 (76.4790)  acc5: 93.7500 (94.1794)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:37  loss: 1.0055 (1.0232)  acc1: 75.0000 (76.7287)  acc5: 95.8333 (94.2819)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:03  loss: 0.9547 (1.0332)  acc1: 79.1667 (76.4073)  acc5: 94.7917 (94.2743)  time: 3.3933  data: 0.0003  max mem: 16226
Test:  [160/521]  eta: 0:20:29  loss: 1.0760 (1.0246)  acc1: 77.0833 (76.6887)  acc5: 94.7917 (94.3129)  time: 3.3944  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:19:54  loss: 0.8463 (1.0168)  acc1: 81.2500 (76.8640)  acc5: 94.7917 (94.3348)  time: 3.3950  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:20  loss: 0.8907 (1.0098)  acc1: 81.2500 (77.1179)  acc5: 94.7917 (94.4003)  time: 3.3942  data: 0.0003  max mem: 16226
Test:  [190/521]  eta: 0:18:46  loss: 0.9013 (1.0074)  acc1: 79.1667 (77.1161)  acc5: 94.7917 (94.4372)  time: 3.3943  data: 0.0003  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 1.0076 (1.0148)  acc1: 76.0417 (76.9486)  acc5: 93.7500 (94.3253)  time: 3.3961  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 1.0140 (1.0141)  acc1: 75.0000 (76.9994)  acc5: 92.7083 (94.2684)  time: 3.3956  data: 0.0003  max mem: 16226
Test:  [220/521]  eta: 0:17:03  loss: 1.2063 (1.0371)  acc1: 69.7917 (76.4753)  acc5: 90.6250 (93.9480)  time: 3.3925  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:29  loss: 1.3119 (1.0532)  acc1: 68.7500 (76.0552)  acc5: 89.5833 (93.7094)  time: 3.3925  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:15:55  loss: 1.3527 (1.0720)  acc1: 64.5833 (75.6051)  acc5: 89.5833 (93.4820)  time: 3.3946  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:21  loss: 1.4808 (1.0950)  acc1: 64.5833 (75.2158)  acc5: 87.5000 (93.1316)  time: 3.3958  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:47  loss: 1.4920 (1.1153)  acc1: 62.5000 (74.7126)  acc5: 83.3333 (92.8560)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.4920 (1.1371)  acc1: 61.4583 (74.1236)  acc5: 87.5000 (92.6084)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.4679 (1.1479)  acc1: 63.5417 (73.9101)  acc5: 87.5000 (92.4488)  time: 3.3949  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.4526 (1.1579)  acc1: 67.7083 (73.7257)  acc5: 86.4583 (92.2752)  time: 3.3940  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.1088 (1.1548)  acc1: 73.9583 (73.8684)  acc5: 91.6667 (92.2792)  time: 3.3938  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.1105 (1.1704)  acc1: 70.8333 (73.5095)  acc5: 91.6667 (92.0954)  time: 3.3925  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.4197 (1.1748)  acc1: 69.7917 (73.5040)  acc5: 88.5417 (92.0009)  time: 3.3926  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3688 (1.1935)  acc1: 69.7917 (73.0488)  acc5: 90.6250 (91.7516)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.6057 (1.2041)  acc1: 63.5417 (72.8311)  acc5: 85.4167 (91.5903)  time: 3.3935  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.6057 (1.2133)  acc1: 63.5417 (72.5487)  acc5: 85.4167 (91.5005)  time: 3.3955  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5211 (1.2254)  acc1: 64.5833 (72.3194)  acc5: 85.4167 (91.3089)  time: 3.3955  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.5110 (1.2329)  acc1: 66.6667 (72.1474)  acc5: 86.4583 (91.2539)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.4058 (1.2377)  acc1: 69.7917 (72.1484)  acc5: 89.5833 (91.1445)  time: 3.3949  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.4058 (1.2480)  acc1: 66.6667 (71.8750)  acc5: 88.5417 (90.9580)  time: 3.3925  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.5398 (1.2543)  acc1: 65.6250 (71.7893)  acc5: 85.4167 (90.8588)  time: 3.3923  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.4818 (1.2608)  acc1: 68.7500 (71.6900)  acc5: 85.4167 (90.7619)  time: 3.3957  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.4818 (1.2650)  acc1: 68.7500 (71.6746)  acc5: 85.4167 (90.6992)  time: 3.3968  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.6131 (1.2725)  acc1: 67.7083 (71.4835)  acc5: 88.5417 (90.6298)  time: 3.3945  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.7444 (1.2845)  acc1: 59.3750 (71.2042)  acc5: 84.3750 (90.4644)  time: 3.3965  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.5783 (1.2893)  acc1: 63.5417 (71.0989)  acc5: 85.4167 (90.3987)  time: 3.3958  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.4870 (1.2922)  acc1: 66.6667 (70.9960)  acc5: 87.5000 (90.3674)  time: 3.3955  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.4062 (1.2978)  acc1: 69.7917 (70.8953)  acc5: 89.5833 (90.2999)  time: 3.3974  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.4062 (1.3024)  acc1: 69.7917 (70.7489)  acc5: 90.6250 (90.2612)  time: 3.3979  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1035 (1.2975)  acc1: 70.8333 (70.8333)  acc5: 92.7083 (90.3386)  time: 3.3971  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0038 (1.2909)  acc1: 77.0833 (70.9851)  acc5: 93.7500 (90.4005)  time: 3.3960  data: 0.0003  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1576 (1.2960)  acc1: 71.8750 (70.8252)  acc5: 92.7083 (90.3518)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1894 (1.2871)  acc1: 70.8333 (71.0520)  acc5: 91.6667 (90.4320)  time: 3.3705  data: 0.0001  max mem: 16226
Test: Total time: 0:29:29 (3.3971 s / it)
* Acc@1 71.052 Acc@5 90.432 loss 1.287
Accuracy of the network on the 50000 test images: 71.1%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=16, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 16
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:20  loss: 0.5447 (0.5447)  acc1: 89.5833 (89.5833)  acc5: 97.9167 (97.9167)  time: 4.9909  data: 1.0036  max mem: 16225
Test:  [ 10/521]  eta: 0:30:08  loss: 0.6033 (0.6784)  acc1: 89.5833 (86.5530)  acc5: 97.9167 (97.3485)  time: 3.5396  data: 0.0914  max mem: 16226
Test:  [ 20/521]  eta: 0:29:00  loss: 0.8309 (0.8675)  acc1: 81.2500 (81.4980)  acc5: 95.8333 (95.7837)  time: 3.3983  data: 0.0004  max mem: 16226
Test:  [ 30/521]  eta: 0:28:12  loss: 1.0534 (0.9732)  acc1: 76.0417 (78.3938)  acc5: 92.7083 (94.6237)  time: 3.3971  data: 0.0004  max mem: 16226
Test:  [ 40/521]  eta: 0:27:31  loss: 1.2106 (1.0478)  acc1: 69.7917 (75.9654)  acc5: 92.7083 (93.9787)  time: 3.3927  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:26:53  loss: 0.7726 (0.9759)  acc1: 84.3750 (78.3293)  acc5: 95.8333 (94.3015)  time: 3.3939  data: 0.0005  max mem: 16226
Test:  [ 60/521]  eta: 0:26:17  loss: 0.7452 (0.9675)  acc1: 84.3750 (78.9788)  acc5: 94.7917 (94.2452)  time: 3.3940  data: 0.0005  max mem: 16226
Test:  [ 70/521]  eta: 0:25:41  loss: 0.8778 (0.9696)  acc1: 82.2917 (78.9466)  acc5: 93.7500 (94.2342)  time: 3.3939  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:05  loss: 0.7340 (0.9459)  acc1: 83.3333 (79.6811)  acc5: 95.8333 (94.4187)  time: 3.3932  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:30  loss: 1.0489 (0.9852)  acc1: 76.0417 (78.4913)  acc5: 94.7917 (94.1964)  time: 3.3932  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:23:55  loss: 1.2327 (1.0187)  acc1: 67.7083 (77.3412)  acc5: 92.7083 (94.0182)  time: 3.3958  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:21  loss: 1.2136 (1.0254)  acc1: 67.7083 (76.9426)  acc5: 93.7500 (94.1066)  time: 3.3952  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:46  loss: 1.0557 (1.0322)  acc1: 73.9583 (76.7992)  acc5: 94.7917 (93.9997)  time: 3.3934  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:11  loss: 1.1508 (1.0405)  acc1: 71.8750 (76.1768)  acc5: 94.7917 (94.0840)  time: 3.3930  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:21:37  loss: 1.0148 (1.0308)  acc1: 75.0000 (76.4554)  acc5: 94.7917 (94.1637)  time: 3.3939  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:03  loss: 0.9749 (1.0410)  acc1: 78.1250 (76.1176)  acc5: 94.7917 (94.1432)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:28  loss: 1.0461 (1.0319)  acc1: 77.0833 (76.3910)  acc5: 94.7917 (94.2158)  time: 3.3932  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:19:54  loss: 0.8240 (1.0243)  acc1: 82.2917 (76.6021)  acc5: 94.7917 (94.2495)  time: 3.3938  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:20  loss: 0.8950 (1.0181)  acc1: 81.2500 (76.8877)  acc5: 93.7500 (94.2967)  time: 3.3970  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:46  loss: 0.9613 (1.0159)  acc1: 81.2500 (76.9197)  acc5: 94.7917 (94.3336)  time: 3.3960  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 0.9709 (1.0230)  acc1: 76.0417 (76.8035)  acc5: 93.7500 (94.2527)  time: 3.3948  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 0.9709 (1.0217)  acc1: 76.0417 (76.8118)  acc5: 92.7083 (94.2091)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:03  loss: 1.1758 (1.0448)  acc1: 69.7917 (76.2962)  acc5: 92.7083 (93.9103)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:29  loss: 1.3788 (1.0601)  acc1: 65.6250 (75.9019)  acc5: 89.5833 (93.7365)  time: 3.3934  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:15:55  loss: 1.4537 (1.0801)  acc1: 65.6250 (75.3933)  acc5: 89.5833 (93.4863)  time: 3.3959  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:21  loss: 1.4808 (1.1037)  acc1: 64.5833 (74.9917)  acc5: 86.4583 (93.1067)  time: 3.3978  data: 0.0003  max mem: 16226
Test:  [260/521]  eta: 0:14:47  loss: 1.6093 (1.1250)  acc1: 62.5000 (74.4931)  acc5: 84.3750 (92.8201)  time: 3.3947  data: 0.0003  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.4889 (1.1464)  acc1: 62.5000 (73.9660)  acc5: 87.5000 (92.5546)  time: 3.3931  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.4694 (1.1579)  acc1: 62.5000 (73.6914)  acc5: 87.5000 (92.4007)  time: 3.3938  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.4791 (1.1673)  acc1: 68.7500 (73.5252)  acc5: 87.5000 (92.2537)  time: 3.3966  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.1072 (1.1639)  acc1: 71.8750 (73.7161)  acc5: 91.6667 (92.2342)  time: 3.3960  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.2244 (1.1792)  acc1: 69.7917 (73.3621)  acc5: 91.6667 (92.0318)  time: 3.3941  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.3341 (1.1834)  acc1: 67.7083 (73.3580)  acc5: 88.5417 (91.9360)  time: 3.3933  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3341 (1.2021)  acc1: 67.7083 (72.9135)  acc5: 89.5833 (91.6950)  time: 3.3935  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.5896 (1.2127)  acc1: 64.5833 (72.6876)  acc5: 86.4583 (91.5659)  time: 3.3958  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.5896 (1.2215)  acc1: 65.6250 (72.4270)  acc5: 88.5417 (91.4738)  time: 3.3972  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5366 (1.2342)  acc1: 65.6250 (72.1780)  acc5: 87.5000 (91.3002)  time: 3.3960  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.5551 (1.2414)  acc1: 65.6250 (72.0210)  acc5: 88.5417 (91.2399)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.3941 (1.2457)  acc1: 68.7500 (72.0199)  acc5: 88.5417 (91.1281)  time: 3.3948  data: 0.0003  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.4667 (1.2562)  acc1: 63.5417 (71.7311)  acc5: 87.5000 (90.9607)  time: 3.3973  data: 0.0003  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.5296 (1.2617)  acc1: 64.5833 (71.7010)  acc5: 87.5000 (90.8796)  time: 3.3989  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.5218 (1.2678)  acc1: 71.8750 (71.6317)  acc5: 87.5000 (90.7593)  time: 3.3963  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.5218 (1.2715)  acc1: 67.7083 (71.6152)  acc5: 86.4583 (90.6918)  time: 3.3937  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.5603 (1.2793)  acc1: 66.6667 (71.4303)  acc5: 87.5000 (90.6057)  time: 3.3940  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.8034 (1.2918)  acc1: 58.3333 (71.1499)  acc5: 82.2917 (90.4384)  time: 3.3940  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.5722 (1.2963)  acc1: 63.5417 (71.0574)  acc5: 86.4583 (90.3894)  time: 3.3953  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.5328 (1.2989)  acc1: 68.7500 (70.9667)  acc5: 88.5417 (90.3855)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.3483 (1.3043)  acc1: 68.7500 (70.8621)  acc5: 89.5833 (90.3087)  time: 3.3930  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.3483 (1.3090)  acc1: 68.7500 (70.7251)  acc5: 89.5833 (90.2850)  time: 3.3962  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1544 (1.3044)  acc1: 70.8333 (70.8079)  acc5: 91.6667 (90.3619)  time: 3.3971  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0115 (1.2975)  acc1: 78.1250 (70.9747)  acc5: 94.7917 (90.4316)  time: 3.3970  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1639 (1.3022)  acc1: 70.8333 (70.8028)  acc5: 91.6667 (90.3865)  time: 3.3962  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2153 (1.2931)  acc1: 70.8333 (71.0340)  acc5: 91.6667 (90.4720)  time: 3.3714  data: 0.0001  max mem: 16226
Test: Total time: 0:29:30 (3.3973 s / it)
* Acc@1 71.034 Acc@5 90.472 loss 1.293
Accuracy of the network on the 50000 test images: 71.0%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=20, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 20
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:57  loss: 0.5566 (0.5566)  acc1: 89.5833 (89.5833)  acc5: 97.9167 (97.9167)  time: 5.0615  data: 1.0318  max mem: 16225
Test:  [ 10/521]  eta: 0:30:12  loss: 0.5570 (0.6868)  acc1: 89.5833 (87.5000)  acc5: 96.8750 (96.5909)  time: 3.5479  data: 0.0939  max mem: 16226
Test:  [ 20/521]  eta: 0:29:00  loss: 0.8358 (0.8657)  acc1: 82.2917 (82.3413)  acc5: 95.8333 (95.1885)  time: 3.3955  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:13  loss: 1.0497 (0.9673)  acc1: 75.0000 (79.4019)  acc5: 91.6667 (94.2204)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:32  loss: 1.2034 (1.0421)  acc1: 70.8333 (76.6768)  acc5: 91.6667 (93.8262)  time: 3.3929  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:26:54  loss: 0.7688 (0.9740)  acc1: 84.3750 (78.8194)  acc5: 95.8333 (94.2402)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:17  loss: 0.7627 (0.9649)  acc1: 84.3750 (79.3204)  acc5: 95.8333 (94.1257)  time: 3.3960  data: 0.0005  max mem: 16226
Test:  [ 70/521]  eta: 0:25:41  loss: 0.8512 (0.9680)  acc1: 83.3333 (79.3134)  acc5: 94.7917 (94.1755)  time: 3.3950  data: 0.0005  max mem: 16226
Test:  [ 80/521]  eta: 0:25:06  loss: 0.7391 (0.9445)  acc1: 86.4583 (80.0154)  acc5: 95.8333 (94.4187)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:31  loss: 1.0552 (0.9852)  acc1: 75.0000 (78.7088)  acc5: 94.7917 (94.2079)  time: 3.3940  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:23:56  loss: 1.2300 (1.0173)  acc1: 69.7917 (77.6609)  acc5: 92.7083 (94.0594)  time: 3.3935  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:21  loss: 1.1645 (1.0243)  acc1: 70.8333 (77.2992)  acc5: 93.7500 (94.0972)  time: 3.3946  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:46  loss: 1.0685 (1.0314)  acc1: 73.9583 (77.1092)  acc5: 93.7500 (93.9222)  time: 3.3944  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:12  loss: 1.1128 (1.0393)  acc1: 71.8750 (76.5347)  acc5: 94.7917 (94.0363)  time: 3.3933  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:37  loss: 1.0373 (1.0308)  acc1: 76.0417 (76.7213)  acc5: 95.8333 (94.1563)  time: 3.3926  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:03  loss: 0.9570 (1.0397)  acc1: 79.1667 (76.3797)  acc5: 95.8333 (94.1708)  time: 3.3936  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:29  loss: 1.0485 (1.0297)  acc1: 79.1667 (76.6563)  acc5: 95.8333 (94.2611)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:19:54  loss: 0.8053 (1.0218)  acc1: 82.2917 (76.8336)  acc5: 94.7917 (94.3226)  time: 3.3927  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:20  loss: 0.8837 (1.0162)  acc1: 81.2500 (77.0661)  acc5: 94.7917 (94.3600)  time: 3.3928  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:18:46  loss: 0.9428 (1.0139)  acc1: 80.2083 (77.0779)  acc5: 95.8333 (94.4208)  time: 3.3939  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 1.0011 (1.0211)  acc1: 77.0833 (76.9486)  acc5: 93.7500 (94.3408)  time: 3.3958  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 1.0075 (1.0210)  acc1: 77.0833 (76.9451)  acc5: 92.7083 (94.2634)  time: 3.3967  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:03  loss: 1.1922 (1.0436)  acc1: 69.7917 (76.4046)  acc5: 91.6667 (93.9621)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:29  loss: 1.2782 (1.0591)  acc1: 68.7500 (75.9966)  acc5: 89.5833 (93.7455)  time: 3.3940  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:15:55  loss: 1.3525 (1.0781)  acc1: 64.5833 (75.5100)  acc5: 89.5833 (93.4734)  time: 3.3938  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:21  loss: 1.5086 (1.1014)  acc1: 64.5833 (75.1038)  acc5: 87.5000 (93.0860)  time: 3.3939  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:47  loss: 1.6263 (1.1224)  acc1: 63.5417 (74.6807)  acc5: 84.3750 (92.8001)  time: 3.3968  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.4841 (1.1442)  acc1: 62.5000 (74.1544)  acc5: 85.4167 (92.5277)  time: 3.3969  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.4748 (1.1549)  acc1: 64.5833 (73.9472)  acc5: 87.5000 (92.3858)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.4405 (1.1637)  acc1: 69.7917 (73.7901)  acc5: 88.5417 (92.2645)  time: 3.3940  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.1234 (1.1614)  acc1: 73.9583 (73.9341)  acc5: 91.6667 (92.2550)  time: 3.3951  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.1547 (1.1770)  acc1: 70.8333 (73.5430)  acc5: 90.6250 (92.0519)  time: 3.3953  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.3803 (1.1818)  acc1: 70.8333 (73.5332)  acc5: 88.5417 (91.9328)  time: 3.3931  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3803 (1.2013)  acc1: 69.7917 (73.0551)  acc5: 88.5417 (91.6667)  time: 3.3959  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.6352 (1.2112)  acc1: 61.4583 (72.7975)  acc5: 86.4583 (91.5384)  time: 3.3969  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.5874 (1.2206)  acc1: 64.5833 (72.5249)  acc5: 86.4583 (91.4560)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5205 (1.2339)  acc1: 64.5833 (72.2703)  acc5: 86.4583 (91.2540)  time: 3.3937  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.5126 (1.2408)  acc1: 66.6667 (72.1165)  acc5: 86.4583 (91.2034)  time: 3.3928  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.3300 (1.2453)  acc1: 67.7083 (72.1183)  acc5: 90.6250 (91.1171)  time: 3.3931  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.4626 (1.2567)  acc1: 64.5833 (71.8430)  acc5: 87.5000 (90.9234)  time: 3.3957  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.5198 (1.2627)  acc1: 66.6667 (71.7711)  acc5: 86.4583 (90.8406)  time: 3.3987  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.5182 (1.2696)  acc1: 67.7083 (71.6469)  acc5: 87.5000 (90.7391)  time: 3.3960  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.5064 (1.2733)  acc1: 65.6250 (71.6152)  acc5: 87.5000 (90.6745)  time: 3.3930  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.5064 (1.2807)  acc1: 65.6250 (71.4110)  acc5: 87.5000 (90.6081)  time: 3.3962  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.8062 (1.2935)  acc1: 56.2500 (71.1168)  acc5: 84.3750 (90.4313)  time: 3.3961  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.5862 (1.2981)  acc1: 62.5000 (70.9904)  acc5: 86.4583 (90.3732)  time: 3.3931  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.5178 (1.3012)  acc1: 64.5833 (70.8876)  acc5: 88.5417 (90.3335)  time: 3.3949  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.3973 (1.3065)  acc1: 68.7500 (70.7913)  acc5: 88.5417 (90.2601)  time: 3.3946  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.3867 (1.3110)  acc1: 67.7083 (70.6514)  acc5: 89.5833 (90.2287)  time: 3.3945  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1497 (1.3062)  acc1: 70.8333 (70.7527)  acc5: 91.6667 (90.3068)  time: 3.3978  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0236 (1.2993)  acc1: 77.0833 (70.9144)  acc5: 93.7500 (90.3859)  time: 3.3974  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1468 (1.3042)  acc1: 71.8750 (70.7559)  acc5: 93.7500 (90.3498)  time: 3.3974  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2204 (1.2948)  acc1: 71.8750 (70.9900)  acc5: 90.6250 (90.4420)  time: 3.3750  data: 0.0001  max mem: 16226
Test: Total time: 0:29:30 (3.3974 s / it)
* Acc@1 70.990 Acc@5 90.442 loss 1.295
Accuracy of the network on the 50000 test images: 71.0%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=24, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 24
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:44  loss: 0.5242 (0.5242)  acc1: 89.5833 (89.5833)  acc5: 96.8750 (96.8750)  time: 5.0383  data: 1.1077  max mem: 16225
Test:  [ 10/521]  eta: 0:30:10  loss: 0.5836 (0.6793)  acc1: 89.5833 (86.0795)  acc5: 97.9167 (97.0644)  time: 3.5433  data: 0.1010  max mem: 16226
Test:  [ 20/521]  eta: 0:28:59  loss: 0.8520 (0.8578)  acc1: 82.2917 (81.7956)  acc5: 96.8750 (95.4861)  time: 3.3936  data: 0.0003  max mem: 16226
Test:  [ 30/521]  eta: 0:28:12  loss: 1.0464 (0.9718)  acc1: 72.9167 (78.7970)  acc5: 92.7083 (94.2876)  time: 3.3926  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:31  loss: 1.1774 (1.0441)  acc1: 69.7917 (76.4228)  acc5: 92.7083 (93.7754)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:26:53  loss: 0.7246 (0.9714)  acc1: 84.3750 (78.6969)  acc5: 94.7917 (94.1176)  time: 3.3933  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:16  loss: 0.7246 (0.9625)  acc1: 84.3750 (79.3374)  acc5: 94.7917 (94.1257)  time: 3.3923  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:25:40  loss: 0.8510 (0.9638)  acc1: 84.3750 (79.4161)  acc5: 94.7917 (94.0728)  time: 3.3928  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:05  loss: 0.7231 (0.9380)  acc1: 85.4167 (80.1312)  acc5: 95.8333 (94.3416)  time: 3.3929  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:24:30  loss: 1.0046 (0.9768)  acc1: 75.0000 (78.8233)  acc5: 95.8333 (94.0820)  time: 3.3946  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:23:55  loss: 1.2501 (1.0131)  acc1: 67.7083 (77.6300)  acc5: 91.6667 (93.8944)  time: 3.3956  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:20  loss: 1.1529 (1.0190)  acc1: 68.7500 (77.3743)  acc5: 93.7500 (93.9846)  time: 3.3949  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:46  loss: 1.0250 (1.0251)  acc1: 76.0417 (77.2727)  acc5: 93.7500 (93.8189)  time: 3.3948  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:11  loss: 1.0527 (1.0309)  acc1: 73.9583 (76.7017)  acc5: 93.7500 (93.9329)  time: 3.3934  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:21:37  loss: 1.0030 (1.0213)  acc1: 75.0000 (76.9134)  acc5: 95.8333 (94.0677)  time: 3.3956  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:03  loss: 0.9189 (1.0310)  acc1: 80.2083 (76.6004)  acc5: 94.7917 (94.0466)  time: 3.3956  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:29  loss: 1.0924 (1.0215)  acc1: 78.1250 (76.9086)  acc5: 94.7917 (94.1447)  time: 3.3954  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:19:54  loss: 0.7918 (1.0124)  acc1: 81.2500 (77.1443)  acc5: 94.7917 (94.2069)  time: 3.3954  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:20  loss: 0.8805 (1.0063)  acc1: 80.2083 (77.3826)  acc5: 93.7500 (94.2449)  time: 3.3924  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:18:46  loss: 0.9080 (1.0043)  acc1: 80.2083 (77.4324)  acc5: 94.7917 (94.2736)  time: 3.3961  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 0.9821 (1.0112)  acc1: 77.0833 (77.2647)  acc5: 93.7500 (94.1801)  time: 3.3972  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 0.9830 (1.0101)  acc1: 76.0417 (77.2364)  acc5: 92.7083 (94.1647)  time: 3.3939  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:03  loss: 1.1757 (1.0323)  acc1: 67.7083 (76.6874)  acc5: 91.6667 (93.8726)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:29  loss: 1.4109 (1.0484)  acc1: 65.6250 (76.1995)  acc5: 90.6250 (93.7004)  time: 3.3968  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:15:55  loss: 1.4109 (1.0674)  acc1: 64.5833 (75.7434)  acc5: 89.5833 (93.4691)  time: 3.3960  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:21  loss: 1.5286 (1.0899)  acc1: 64.5833 (75.3777)  acc5: 88.5417 (93.1233)  time: 3.3923  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:14:47  loss: 1.6164 (1.1111)  acc1: 63.5417 (74.8643)  acc5: 83.3333 (92.8281)  time: 3.3924  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.4503 (1.1315)  acc1: 60.4167 (74.3619)  acc5: 86.4583 (92.6007)  time: 3.3961  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.4104 (1.1423)  acc1: 64.5833 (74.1252)  acc5: 87.5000 (92.4526)  time: 3.3968  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.4220 (1.1514)  acc1: 67.7083 (73.9440)  acc5: 88.5417 (92.3038)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.1130 (1.1478)  acc1: 73.9583 (74.1279)  acc5: 91.6667 (92.3173)  time: 3.3989  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.1130 (1.1637)  acc1: 72.9167 (73.7339)  acc5: 92.7083 (92.1188)  time: 3.3979  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.3786 (1.1683)  acc1: 67.7083 (73.7247)  acc5: 88.5417 (92.0399)  time: 3.3954  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3786 (1.1866)  acc1: 70.8333 (73.2597)  acc5: 90.6250 (91.8051)  time: 3.3959  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.6234 (1.1971)  acc1: 63.5417 (73.0694)  acc5: 86.4583 (91.6545)  time: 3.3938  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.5624 (1.2070)  acc1: 64.5833 (72.8009)  acc5: 86.4583 (91.5598)  time: 3.3924  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5060 (1.2192)  acc1: 65.6250 (72.5646)  acc5: 86.4583 (91.3954)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.5009 (1.2266)  acc1: 67.7083 (72.3916)  acc5: 88.5417 (91.3606)  time: 3.3987  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.3842 (1.2317)  acc1: 69.7917 (72.3425)  acc5: 91.6667 (91.2538)  time: 3.3975  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.4421 (1.2429)  acc1: 65.6250 (72.0455)  acc5: 87.5000 (91.0619)  time: 3.3976  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.4977 (1.2490)  acc1: 64.5833 (71.9841)  acc5: 86.4583 (90.9809)  time: 3.3989  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.4843 (1.2556)  acc1: 68.7500 (71.8750)  acc5: 86.4583 (90.8430)  time: 3.3975  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.4843 (1.2595)  acc1: 66.6667 (71.8577)  acc5: 85.4167 (90.7487)  time: 3.3962  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.5096 (1.2671)  acc1: 66.6667 (71.6527)  acc5: 86.4583 (90.6709)  time: 3.3934  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.7040 (1.2796)  acc1: 60.4167 (71.3742)  acc5: 86.4583 (90.5045)  time: 3.3955  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.5408 (1.2832)  acc1: 64.5833 (71.2699)  acc5: 87.5000 (90.4772)  time: 3.3965  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.4608 (1.2861)  acc1: 66.6667 (71.1655)  acc5: 89.5833 (90.4510)  time: 3.3945  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.3528 (1.2915)  acc1: 66.6667 (71.0722)  acc5: 89.5833 (90.3662)  time: 3.3945  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.3482 (1.2957)  acc1: 68.7500 (70.9156)  acc5: 89.5833 (90.3261)  time: 3.3957  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1111 (1.2908)  acc1: 68.7500 (71.0031)  acc5: 92.7083 (90.4107)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 0.9979 (1.2841)  acc1: 78.1250 (71.1618)  acc5: 94.7917 (90.4795)  time: 3.3936  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1356 (1.2894)  acc1: 73.9583 (70.9801)  acc5: 93.7500 (90.4354)  time: 3.3937  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1995 (1.2807)  acc1: 72.9167 (71.2060)  acc5: 91.6667 (90.5220)  time: 3.3722  data: 0.0001  max mem: 16226
Test: Total time: 0:29:30 (3.3975 s / it)
* Acc@1 71.206 Acc@5 90.522 loss 1.281
Accuracy of the network on the 50000 test images: 71.2%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=8, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 8
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:40  loss: 0.5316 (0.5316)  acc1: 89.5833 (89.5833)  acc5: 96.8750 (96.8750)  time: 5.0292  data: 0.9800  max mem: 16225
Test:  [ 10/521]  eta: 0:30:10  loss: 0.5981 (0.6864)  acc1: 89.5833 (86.3636)  acc5: 96.8750 (96.6856)  time: 3.5423  data: 0.0892  max mem: 16226
Test:  [ 20/521]  eta: 0:29:00  loss: 0.7883 (0.8606)  acc1: 81.2500 (81.4980)  acc5: 96.8750 (95.4861)  time: 3.3963  data: 0.0001  max mem: 16226
Test:  [ 30/521]  eta: 0:28:12  loss: 1.0988 (0.9674)  acc1: 75.0000 (79.1331)  acc5: 91.6667 (94.1868)  time: 3.3960  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:27:32  loss: 1.2121 (1.0406)  acc1: 71.8750 (76.5498)  acc5: 91.6667 (93.7754)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:26:54  loss: 0.7603 (0.9665)  acc1: 84.3750 (78.8194)  acc5: 94.7917 (94.1993)  time: 3.3958  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:17  loss: 0.7504 (0.9563)  acc1: 85.4167 (79.5424)  acc5: 94.7917 (94.2794)  time: 3.3958  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:25:41  loss: 0.8248 (0.9578)  acc1: 82.2917 (79.6948)  acc5: 94.7917 (94.2195)  time: 3.3965  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:06  loss: 0.7399 (0.9330)  acc1: 83.3333 (80.3755)  acc5: 95.8333 (94.4187)  time: 3.3943  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:31  loss: 1.0011 (0.9732)  acc1: 75.0000 (79.0179)  acc5: 93.7500 (94.2079)  time: 3.3935  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:23:56  loss: 1.1757 (1.0056)  acc1: 66.6667 (77.9909)  acc5: 92.7083 (94.0491)  time: 3.3963  data: 0.0001  max mem: 16226
Test:  [110/521]  eta: 0:23:21  loss: 1.1634 (1.0117)  acc1: 71.8750 (77.7309)  acc5: 93.7500 (94.1160)  time: 3.3977  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:22:47  loss: 1.0529 (1.0198)  acc1: 76.0417 (77.5052)  acc5: 93.7500 (93.9824)  time: 3.3960  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:12  loss: 1.1066 (1.0279)  acc1: 72.9167 (76.9243)  acc5: 93.7500 (94.0522)  time: 3.3965  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:38  loss: 1.0366 (1.0212)  acc1: 77.0833 (77.0464)  acc5: 94.7917 (94.0972)  time: 3.3952  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:03  loss: 0.9762 (1.0322)  acc1: 78.1250 (76.6487)  acc5: 95.8333 (94.1018)  time: 3.3940  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:29  loss: 1.0482 (1.0233)  acc1: 78.1250 (76.9345)  acc5: 94.7917 (94.1770)  time: 3.3954  data: 0.0004  max mem: 16226
Test:  [170/521]  eta: 0:19:55  loss: 0.8366 (1.0159)  acc1: 83.3333 (77.1321)  acc5: 94.7917 (94.2373)  time: 3.3968  data: 0.0004  max mem: 16226
Test:  [180/521]  eta: 0:19:21  loss: 0.8732 (1.0092)  acc1: 81.2500 (77.3884)  acc5: 95.8333 (94.2852)  time: 3.3985  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:46  loss: 0.9466 (1.0070)  acc1: 80.2083 (77.4051)  acc5: 95.8333 (94.3390)  time: 3.3960  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 0.9720 (1.0144)  acc1: 76.0417 (77.2025)  acc5: 93.7500 (94.2786)  time: 3.3938  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 1.0197 (1.0128)  acc1: 76.0417 (77.2068)  acc5: 92.7083 (94.2338)  time: 3.3941  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:04  loss: 1.1374 (1.0354)  acc1: 69.7917 (76.6733)  acc5: 92.7083 (93.9574)  time: 3.3953  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:30  loss: 1.3058 (1.0512)  acc1: 65.6250 (76.2581)  acc5: 90.6250 (93.7229)  time: 3.3979  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:15:56  loss: 1.4215 (1.0695)  acc1: 64.5833 (75.7650)  acc5: 89.5833 (93.5036)  time: 3.3960  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:21  loss: 1.5137 (1.0921)  acc1: 64.5833 (75.3237)  acc5: 87.5000 (93.1233)  time: 3.3933  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:47  loss: 1.6225 (1.1136)  acc1: 63.5417 (74.8044)  acc5: 83.3333 (92.8480)  time: 3.3931  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.4548 (1.1343)  acc1: 61.4583 (74.2582)  acc5: 87.5000 (92.6084)  time: 3.3928  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.4548 (1.1458)  acc1: 63.5417 (74.0139)  acc5: 87.5000 (92.4451)  time: 3.3950  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.4518 (1.1543)  acc1: 68.7500 (73.8223)  acc5: 87.5000 (92.3074)  time: 3.3967  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.1341 (1.1514)  acc1: 73.9583 (73.9756)  acc5: 90.6250 (92.2757)  time: 3.3950  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.1596 (1.1675)  acc1: 71.8750 (73.6535)  acc5: 90.6250 (92.0586)  time: 3.3947  data: 0.0003  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.3861 (1.1719)  acc1: 69.7917 (73.6533)  acc5: 87.5000 (91.9490)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3861 (1.1909)  acc1: 69.7917 (73.1905)  acc5: 90.6250 (91.7296)  time: 3.3934  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.5814 (1.2012)  acc1: 61.4583 (72.9442)  acc5: 88.5417 (91.5628)  time: 3.3937  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.5693 (1.2103)  acc1: 65.6250 (72.6793)  acc5: 88.5417 (91.5064)  time: 3.3955  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5398 (1.2228)  acc1: 65.6250 (72.4088)  acc5: 87.5000 (91.3464)  time: 3.3965  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.4877 (1.2301)  acc1: 66.6667 (72.2512)  acc5: 87.5000 (91.2904)  time: 3.3943  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.3980 (1.2346)  acc1: 69.7917 (72.2496)  acc5: 89.5833 (91.1882)  time: 3.3933  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.4423 (1.2457)  acc1: 64.5833 (71.9576)  acc5: 88.5417 (91.0140)  time: 3.3945  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.5378 (1.2516)  acc1: 64.5833 (71.9088)  acc5: 86.4583 (90.9315)  time: 3.3967  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.5225 (1.2584)  acc1: 67.7083 (71.7914)  acc5: 87.5000 (90.7999)  time: 3.3956  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.4719 (1.2616)  acc1: 67.7083 (71.7612)  acc5: 87.5000 (90.7413)  time: 3.3947  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.4719 (1.2690)  acc1: 66.6667 (71.5584)  acc5: 88.5417 (90.6758)  time: 3.3947  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.6965 (1.2816)  acc1: 57.2917 (71.2278)  acc5: 84.3750 (90.4927)  time: 3.3932  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.5821 (1.2862)  acc1: 64.5833 (71.1105)  acc5: 87.5000 (90.4287)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.4628 (1.2890)  acc1: 65.6250 (71.0005)  acc5: 88.5417 (90.4103)  time: 3.3953  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.3622 (1.2945)  acc1: 68.7500 (70.8930)  acc5: 87.5000 (90.3176)  time: 3.3945  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.4055 (1.2989)  acc1: 70.8333 (70.7510)  acc5: 88.5417 (90.2893)  time: 3.3971  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1478 (1.2945)  acc1: 70.8333 (70.8355)  acc5: 91.6667 (90.3598)  time: 3.3975  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0169 (1.2878)  acc1: 76.0417 (70.9810)  acc5: 93.7500 (90.4337)  time: 3.3948  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1496 (1.2924)  acc1: 70.8333 (70.8068)  acc5: 92.7083 (90.4008)  time: 3.3933  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2085 (1.2834)  acc1: 68.7500 (71.0140)  acc5: 92.7083 (90.4900)  time: 3.3715  data: 0.0001  max mem: 16226
Test: Total time: 0:29:30 (3.3976 s / it)
* Acc@1 71.014 Acc@5 90.490 loss 1.283
Accuracy of the network on the 50000 test images: 71.0%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:05  loss: 0.5551 (0.5551)  acc1: 89.5833 (89.5833)  acc5: 96.8750 (96.8750)  time: 5.0780  data: 1.0293  max mem: 16225
Test:  [ 10/521]  eta: 0:30:13  loss: 0.6131 (0.6938)  acc1: 89.5833 (86.6477)  acc5: 97.9167 (96.6856)  time: 3.5483  data: 0.0938  max mem: 16226
Test:  [ 20/521]  eta: 0:29:03  loss: 0.8386 (0.8655)  acc1: 83.3333 (81.9444)  acc5: 96.8750 (95.4365)  time: 3.4007  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:15  loss: 1.0412 (0.9725)  acc1: 72.9167 (79.2339)  acc5: 91.6667 (94.0860)  time: 3.4008  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:27:33  loss: 1.2520 (1.0426)  acc1: 71.8750 (76.7531)  acc5: 92.7083 (93.7246)  time: 3.3942  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:26:55  loss: 0.7782 (0.9714)  acc1: 86.4583 (79.1667)  acc5: 94.7917 (94.1381)  time: 3.3961  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:18  loss: 0.7358 (0.9587)  acc1: 86.4583 (79.7985)  acc5: 94.7917 (94.1598)  time: 3.3962  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:25:42  loss: 0.8338 (0.9575)  acc1: 83.3333 (79.7975)  acc5: 93.7500 (94.0728)  time: 3.3953  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:07  loss: 0.7051 (0.9320)  acc1: 83.3333 (80.4141)  acc5: 95.8333 (94.3416)  time: 3.3972  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:31  loss: 0.9948 (0.9712)  acc1: 75.0000 (79.0751)  acc5: 93.7500 (94.0934)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:23:56  loss: 1.2373 (1.0045)  acc1: 66.6667 (78.0528)  acc5: 92.7083 (93.9563)  time: 3.3927  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:21  loss: 1.1539 (1.0117)  acc1: 69.7917 (77.6652)  acc5: 94.7917 (94.0597)  time: 3.3932  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:22:47  loss: 1.0496 (1.0185)  acc1: 76.0417 (77.5826)  acc5: 94.7917 (93.9566)  time: 3.3934  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:12  loss: 1.0989 (1.0262)  acc1: 73.9583 (77.0038)  acc5: 93.7500 (94.0681)  time: 3.3943  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:21:38  loss: 1.0083 (1.0191)  acc1: 75.0000 (77.1129)  acc5: 95.8333 (94.1637)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:03  loss: 0.9873 (1.0296)  acc1: 76.0417 (76.7591)  acc5: 95.8333 (94.1294)  time: 3.3932  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:29  loss: 1.0917 (1.0210)  acc1: 76.0417 (77.0445)  acc5: 95.8333 (94.2094)  time: 3.3947  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:19:55  loss: 0.8222 (1.0131)  acc1: 80.2083 (77.1869)  acc5: 95.8333 (94.2617)  time: 3.3954  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:20  loss: 0.8834 (1.0054)  acc1: 80.2083 (77.4459)  acc5: 94.7917 (94.3198)  time: 3.3937  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:18:46  loss: 0.8834 (1.0024)  acc1: 80.2083 (77.4596)  acc5: 94.7917 (94.3663)  time: 3.3930  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 0.9729 (1.0099)  acc1: 78.1250 (77.2803)  acc5: 93.7500 (94.3045)  time: 3.3934  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 1.0047 (1.0084)  acc1: 78.1250 (77.2512)  acc5: 92.7083 (94.2684)  time: 3.3956  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:04  loss: 1.1611 (1.0316)  acc1: 69.7917 (76.7110)  acc5: 91.6667 (94.0092)  time: 3.3955  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:30  loss: 1.3097 (1.0483)  acc1: 63.5417 (76.2762)  acc5: 89.5833 (93.7365)  time: 3.3932  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:15:55  loss: 1.3763 (1.0666)  acc1: 64.5833 (75.7910)  acc5: 89.5833 (93.4863)  time: 3.3935  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:21  loss: 1.5307 (1.0899)  acc1: 64.5833 (75.3445)  acc5: 86.4583 (93.1109)  time: 3.3962  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:47  loss: 1.5843 (1.1114)  acc1: 61.4583 (74.8044)  acc5: 84.3750 (92.8520)  time: 3.3978  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.4527 (1.1322)  acc1: 61.4583 (74.2389)  acc5: 86.4583 (92.6276)  time: 3.3959  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.4279 (1.1428)  acc1: 61.4583 (73.9954)  acc5: 88.5417 (92.4859)  time: 3.3943  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.4583 (1.1525)  acc1: 69.7917 (73.8044)  acc5: 88.5417 (92.3289)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.1056 (1.1497)  acc1: 75.0000 (73.9756)  acc5: 90.6250 (92.3069)  time: 3.3975  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.1834 (1.1650)  acc1: 75.0000 (73.6837)  acc5: 90.6250 (92.1423)  time: 3.3962  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.3789 (1.1698)  acc1: 67.7083 (73.6598)  acc5: 87.5000 (92.0236)  time: 3.3950  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3789 (1.1884)  acc1: 68.7500 (73.2125)  acc5: 89.5833 (91.8083)  time: 3.3953  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.5934 (1.1989)  acc1: 63.5417 (72.9717)  acc5: 87.5000 (91.6545)  time: 3.3952  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.5934 (1.2080)  acc1: 63.5417 (72.7089)  acc5: 87.5000 (91.5865)  time: 3.3951  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5582 (1.2197)  acc1: 63.5417 (72.4550)  acc5: 86.4583 (91.4329)  time: 3.3943  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.5582 (1.2272)  acc1: 65.6250 (72.2709)  acc5: 89.5833 (91.3634)  time: 3.3954  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.3073 (1.2322)  acc1: 67.7083 (72.2496)  acc5: 90.6250 (91.2648)  time: 3.3938  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.4436 (1.2428)  acc1: 63.5417 (71.9656)  acc5: 88.5417 (91.0939)  time: 3.3957  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.5147 (1.2487)  acc1: 64.5833 (71.8880)  acc5: 86.4583 (90.9861)  time: 3.3970  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.5139 (1.2554)  acc1: 67.7083 (71.7787)  acc5: 86.4583 (90.8506)  time: 3.3959  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.5139 (1.2597)  acc1: 67.7083 (71.7364)  acc5: 84.3750 (90.7809)  time: 3.3970  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.5650 (1.2672)  acc1: 66.6667 (71.5729)  acc5: 88.5417 (90.7120)  time: 3.3952  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.7251 (1.2799)  acc1: 60.4167 (71.2845)  acc5: 84.3750 (90.5234)  time: 3.3933  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.5889 (1.2844)  acc1: 63.5417 (71.1729)  acc5: 85.4167 (90.4564)  time: 3.3944  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.5007 (1.2878)  acc1: 66.6667 (71.0548)  acc5: 87.5000 (90.4442)  time: 3.3974  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.4275 (1.2933)  acc1: 69.7917 (70.9638)  acc5: 88.5417 (90.3662)  time: 3.3964  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.4275 (1.2982)  acc1: 69.7917 (70.8312)  acc5: 89.5833 (90.3456)  time: 3.3933  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1406 (1.2938)  acc1: 71.8750 (70.9203)  acc5: 91.6667 (90.4044)  time: 3.3930  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0056 (1.2870)  acc1: 76.0417 (71.0683)  acc5: 95.8333 (90.4815)  time: 3.3938  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1317 (1.2916)  acc1: 75.0000 (70.8965)  acc5: 92.7083 (90.4415)  time: 3.3961  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2291 (1.2827)  acc1: 72.9167 (71.1000)  acc5: 91.6667 (90.5340)  time: 3.3737  data: 0.0001  max mem: 16226
Test: Total time: 0:29:30 (3.3977 s / it)
* Acc@1 71.100 Acc@5 90.534 loss 1.283
Accuracy of the network on the 50000 test images: 71.1%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=16, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 16
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:50  loss: 0.5469 (0.5469)  acc1: 89.5833 (89.5833)  acc5: 97.9167 (97.9167)  time: 5.0495  data: 1.0771  max mem: 16225
Test:  [ 10/521]  eta: 0:30:10  loss: 0.5893 (0.6962)  acc1: 89.5833 (86.7424)  acc5: 96.8750 (96.8750)  time: 3.5434  data: 0.0980  max mem: 16226
Test:  [ 20/521]  eta: 0:28:59  loss: 0.8632 (0.8732)  acc1: 81.2500 (81.4484)  acc5: 95.8333 (95.5853)  time: 3.3925  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:12  loss: 1.0265 (0.9799)  acc1: 76.0417 (78.9315)  acc5: 91.6667 (94.1532)  time: 3.3936  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:31  loss: 1.1898 (1.0502)  acc1: 68.7500 (76.2703)  acc5: 92.7083 (93.7246)  time: 3.3944  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:26:53  loss: 0.7505 (0.9759)  acc1: 86.4583 (78.7786)  acc5: 94.7917 (94.1789)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:17  loss: 0.7505 (0.9637)  acc1: 84.3750 (79.3033)  acc5: 95.8333 (94.1428)  time: 3.3952  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:41  loss: 0.8094 (0.9629)  acc1: 82.2917 (79.5481)  acc5: 94.7917 (94.0874)  time: 3.3973  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:06  loss: 0.7105 (0.9379)  acc1: 84.3750 (80.1183)  acc5: 95.8333 (94.3158)  time: 3.3962  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:30  loss: 1.0062 (0.9821)  acc1: 77.0833 (78.6401)  acc5: 94.7917 (94.0362)  time: 3.3930  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:23:55  loss: 1.2292 (1.0142)  acc1: 66.6667 (77.5990)  acc5: 92.7083 (93.9356)  time: 3.3932  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:21  loss: 1.1364 (1.0211)  acc1: 70.8333 (77.3086)  acc5: 93.7500 (93.9940)  time: 3.3933  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:46  loss: 1.0755 (1.0277)  acc1: 73.9583 (77.1436)  acc5: 93.7500 (93.9222)  time: 3.3955  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:12  loss: 1.1152 (1.0348)  acc1: 71.8750 (76.5903)  acc5: 94.7917 (94.0283)  time: 3.3985  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:21:37  loss: 1.0196 (1.0268)  acc1: 75.0000 (76.7287)  acc5: 95.8333 (94.1268)  time: 3.3963  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:03  loss: 0.9375 (1.0381)  acc1: 76.0417 (76.2969)  acc5: 94.7917 (94.0742)  time: 3.3933  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:29  loss: 1.0494 (1.0288)  acc1: 75.0000 (76.6240)  acc5: 94.7917 (94.1447)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:19:54  loss: 0.8351 (1.0213)  acc1: 83.3333 (76.7788)  acc5: 94.7917 (94.1825)  time: 3.3954  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:20  loss: 0.9073 (1.0147)  acc1: 81.2500 (77.0833)  acc5: 93.7500 (94.1816)  time: 3.3949  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:18:46  loss: 0.9471 (1.0115)  acc1: 81.2500 (77.1324)  acc5: 93.7500 (94.2136)  time: 3.3941  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 0.9698 (1.0186)  acc1: 78.1250 (76.9952)  acc5: 93.7500 (94.1750)  time: 3.3941  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 1.0222 (1.0179)  acc1: 76.0417 (76.9895)  acc5: 92.7083 (94.1203)  time: 3.3953  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:04  loss: 1.1416 (1.0405)  acc1: 70.8333 (76.4847)  acc5: 91.6667 (93.8584)  time: 3.3955  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:29  loss: 1.3227 (1.0562)  acc1: 65.6250 (76.0281)  acc5: 89.5833 (93.6643)  time: 3.3935  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:15:55  loss: 1.4407 (1.0753)  acc1: 65.6250 (75.5100)  acc5: 88.5417 (93.4042)  time: 3.3931  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:21  loss: 1.5364 (1.0984)  acc1: 64.5833 (75.0789)  acc5: 87.5000 (93.0777)  time: 3.3931  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:14:47  loss: 1.5775 (1.1199)  acc1: 63.5417 (74.5650)  acc5: 85.4167 (92.7882)  time: 3.3945  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.5208 (1.1414)  acc1: 63.5417 (74.0775)  acc5: 88.5417 (92.5584)  time: 3.3954  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.4739 (1.1530)  acc1: 64.5833 (73.8805)  acc5: 88.5417 (92.4081)  time: 3.3953  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.4377 (1.1622)  acc1: 68.7500 (73.7257)  acc5: 88.5417 (92.2573)  time: 3.3944  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.1342 (1.1594)  acc1: 73.9583 (73.8960)  acc5: 91.6667 (92.2654)  time: 3.3962  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.1804 (1.1754)  acc1: 71.8750 (73.5464)  acc5: 91.6667 (92.0619)  time: 3.3964  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.3370 (1.1802)  acc1: 66.6667 (73.5267)  acc5: 87.5000 (91.9295)  time: 3.3934  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3333 (1.1990)  acc1: 66.6667 (73.0709)  acc5: 89.5833 (91.6887)  time: 3.3950  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.5899 (1.2090)  acc1: 61.4583 (72.8281)  acc5: 87.5000 (91.5567)  time: 3.3944  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.5899 (1.2180)  acc1: 62.5000 (72.5635)  acc5: 87.5000 (91.4916)  time: 3.3925  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5307 (1.2314)  acc1: 63.5417 (72.2905)  acc5: 86.4583 (91.2973)  time: 3.3944  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.5050 (1.2381)  acc1: 65.6250 (72.1305)  acc5: 86.4583 (91.2455)  time: 3.3952  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.4204 (1.2432)  acc1: 68.7500 (72.0855)  acc5: 89.5833 (91.1390)  time: 3.3955  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.4369 (1.2535)  acc1: 62.5000 (71.8111)  acc5: 88.5417 (90.9847)  time: 3.3946  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.5631 (1.2586)  acc1: 66.6667 (71.7763)  acc5: 86.4583 (90.9056)  time: 3.3945  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.5319 (1.2653)  acc1: 66.6667 (71.6469)  acc5: 86.4583 (90.7796)  time: 3.3946  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.5319 (1.2688)  acc1: 65.6250 (71.6202)  acc5: 85.4167 (90.7215)  time: 3.3935  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.5379 (1.2764)  acc1: 65.6250 (71.4327)  acc5: 87.5000 (90.6492)  time: 3.3935  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.7116 (1.2892)  acc1: 59.3750 (71.1357)  acc5: 84.3750 (90.4667)  time: 3.3944  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.6097 (1.2939)  acc1: 62.5000 (71.0273)  acc5: 85.4167 (90.4079)  time: 3.3963  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.4651 (1.2967)  acc1: 65.6250 (70.9056)  acc5: 88.5417 (90.3923)  time: 3.3951  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.4196 (1.3026)  acc1: 69.7917 (70.7891)  acc5: 89.5833 (90.3198)  time: 3.3954  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.4196 (1.3071)  acc1: 69.7917 (70.6666)  acc5: 89.5833 (90.2980)  time: 3.3956  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1472 (1.3025)  acc1: 69.7917 (70.7273)  acc5: 92.7083 (90.3725)  time: 3.3935  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 0.9947 (1.2955)  acc1: 76.0417 (70.8770)  acc5: 94.7917 (90.4462)  time: 3.3939  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1514 (1.3006)  acc1: 72.9167 (70.7090)  acc5: 91.6667 (90.3947)  time: 3.3947  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2431 (1.2918)  acc1: 71.8750 (70.9240)  acc5: 91.6667 (90.4760)  time: 3.3724  data: 0.0001  max mem: 16226
Test: Total time: 0:29:29 (3.3971 s / it)
* Acc@1 70.924 Acc@5 90.476 loss 1.292
Accuracy of the network on the 50000 test images: 70.9%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=20, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 20
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:26  loss: 0.5422 (0.5422)  acc1: 89.5833 (89.5833)  acc5: 97.9167 (97.9167)  time: 5.1186  data: 1.1699  max mem: 16225
Test:  [ 10/521]  eta: 0:30:13  loss: 0.5670 (0.6923)  acc1: 89.5833 (86.3636)  acc5: 97.9167 (97.0644)  time: 3.5496  data: 0.1065  max mem: 16226
Test:  [ 20/521]  eta: 0:29:00  loss: 0.8527 (0.8706)  acc1: 81.2500 (81.2996)  acc5: 95.8333 (95.6845)  time: 3.3925  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:13  loss: 1.0429 (0.9749)  acc1: 73.9583 (78.6290)  acc5: 92.7083 (94.2876)  time: 3.3943  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:33  loss: 1.2035 (1.0504)  acc1: 71.8750 (76.1179)  acc5: 91.6667 (93.6738)  time: 3.3973  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:26:54  loss: 0.7756 (0.9791)  acc1: 84.3750 (78.4314)  acc5: 94.7917 (94.0360)  time: 3.3961  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:18  loss: 0.7620 (0.9650)  acc1: 84.3750 (79.2179)  acc5: 95.8333 (94.1598)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:42  loss: 0.8245 (0.9653)  acc1: 84.3750 (79.2987)  acc5: 95.8333 (94.1755)  time: 3.3976  data: 0.0003  max mem: 16226
Test:  [ 80/521]  eta: 0:25:06  loss: 0.7290 (0.9424)  acc1: 85.4167 (79.9254)  acc5: 95.8333 (94.3930)  time: 3.3955  data: 0.0003  max mem: 16226
Test:  [ 90/521]  eta: 0:24:31  loss: 0.9752 (0.9841)  acc1: 79.1667 (78.6745)  acc5: 93.7500 (94.0820)  time: 3.3945  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:23:56  loss: 1.2315 (1.0155)  acc1: 65.6250 (77.6815)  acc5: 92.7083 (93.9356)  time: 3.3954  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:21  loss: 1.1568 (1.0206)  acc1: 72.9167 (77.3930)  acc5: 93.7500 (94.0315)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:47  loss: 1.0531 (1.0272)  acc1: 75.0000 (77.2899)  acc5: 93.7500 (93.9308)  time: 3.3933  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:12  loss: 1.1284 (1.0356)  acc1: 71.8750 (76.5824)  acc5: 93.7500 (94.0442)  time: 3.3955  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:21:38  loss: 1.0217 (1.0274)  acc1: 76.0417 (76.7878)  acc5: 95.8333 (94.1120)  time: 3.3981  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:03  loss: 0.9502 (1.0374)  acc1: 78.1250 (76.4763)  acc5: 94.7917 (94.1087)  time: 3.3969  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:29  loss: 1.0396 (1.0290)  acc1: 77.0833 (76.7469)  acc5: 95.8333 (94.2029)  time: 3.3946  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:19:55  loss: 0.8214 (1.0218)  acc1: 81.2500 (76.8762)  acc5: 95.8333 (94.2191)  time: 3.3944  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:21  loss: 0.8873 (1.0150)  acc1: 81.2500 (77.1524)  acc5: 94.7917 (94.2622)  time: 3.3952  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:46  loss: 0.8772 (1.0115)  acc1: 81.2500 (77.2524)  acc5: 95.8333 (94.3172)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 0.9720 (1.0189)  acc1: 77.0833 (77.0937)  acc5: 93.7500 (94.2631)  time: 3.3943  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 1.0421 (1.0184)  acc1: 77.0833 (77.0537)  acc5: 93.7500 (94.2190)  time: 3.3955  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:04  loss: 1.1350 (1.0413)  acc1: 68.7500 (76.5272)  acc5: 90.6250 (93.9385)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:30  loss: 1.3437 (1.0581)  acc1: 66.6667 (76.0327)  acc5: 89.5833 (93.7275)  time: 3.3934  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:15:56  loss: 1.3923 (1.0768)  acc1: 64.5833 (75.5576)  acc5: 88.5417 (93.4950)  time: 3.3941  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:21  loss: 1.4957 (1.1002)  acc1: 64.5833 (75.1204)  acc5: 87.5000 (93.1233)  time: 3.3941  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:14:47  loss: 1.6037 (1.1212)  acc1: 61.4583 (74.6248)  acc5: 84.3750 (92.8640)  time: 3.3940  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.4713 (1.1423)  acc1: 61.4583 (74.1236)  acc5: 87.5000 (92.6315)  time: 3.3949  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.4616 (1.1528)  acc1: 64.5833 (73.9101)  acc5: 89.5833 (92.5119)  time: 3.3971  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.4840 (1.1636)  acc1: 67.7083 (73.7400)  acc5: 87.5000 (92.3325)  time: 3.3957  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.1950 (1.1608)  acc1: 75.0000 (73.9099)  acc5: 89.5833 (92.3207)  time: 3.3936  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.1950 (1.1763)  acc1: 73.9583 (73.5899)  acc5: 90.6250 (92.1356)  time: 3.3938  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.3962 (1.1810)  acc1: 69.7917 (73.6046)  acc5: 89.5833 (92.0269)  time: 3.3968  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3962 (1.2006)  acc1: 69.7917 (73.1244)  acc5: 89.5833 (91.7737)  time: 3.3979  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.5923 (1.2104)  acc1: 61.4583 (72.8922)  acc5: 88.5417 (91.6270)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.5749 (1.2195)  acc1: 63.5417 (72.6288)  acc5: 88.5417 (91.5687)  time: 3.3940  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5197 (1.2324)  acc1: 63.5417 (72.3367)  acc5: 88.5417 (91.3810)  time: 3.3941  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.5339 (1.2395)  acc1: 65.6250 (72.2063)  acc5: 88.5417 (91.3045)  time: 3.3941  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.3492 (1.2450)  acc1: 69.7917 (72.1648)  acc5: 89.5833 (91.1937)  time: 3.3940  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.4355 (1.2557)  acc1: 67.7083 (71.8777)  acc5: 88.5417 (91.0220)  time: 3.3969  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.5452 (1.2616)  acc1: 65.6250 (71.7893)  acc5: 85.4167 (90.9159)  time: 3.3970  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.4706 (1.2683)  acc1: 65.6250 (71.6672)  acc5: 85.4167 (90.7897)  time: 3.3941  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.4706 (1.2720)  acc1: 68.7500 (71.6424)  acc5: 86.4583 (90.7289)  time: 3.3942  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.5405 (1.2792)  acc1: 68.7500 (71.4690)  acc5: 88.5417 (90.6588)  time: 3.3949  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.7601 (1.2921)  acc1: 58.3333 (71.1758)  acc5: 85.4167 (90.4809)  time: 3.3958  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.6267 (1.2972)  acc1: 63.5417 (71.0389)  acc5: 85.4167 (90.4033)  time: 3.3979  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.5067 (1.3004)  acc1: 65.6250 (70.9192)  acc5: 87.5000 (90.3652)  time: 3.3958  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.4717 (1.3062)  acc1: 67.7083 (70.8090)  acc5: 88.5417 (90.2977)  time: 3.3926  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.4717 (1.3108)  acc1: 68.7500 (70.6709)  acc5: 89.5833 (90.2742)  time: 3.3938  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1448 (1.3063)  acc1: 69.7917 (70.7421)  acc5: 91.6667 (90.3450)  time: 3.3938  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0427 (1.2994)  acc1: 75.0000 (70.9019)  acc5: 93.7500 (90.4067)  time: 3.3956  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.0980 (1.3040)  acc1: 73.9583 (70.7233)  acc5: 91.6667 (90.3743)  time: 3.3959  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2071 (1.2948)  acc1: 71.8750 (70.9520)  acc5: 91.6667 (90.4640)  time: 3.3721  data: 0.0001  max mem: 16226
Test: Total time: 0:29:30 (3.3976 s / it)
* Acc@1 70.952 Acc@5 90.464 loss 1.295
Accuracy of the network on the 50000 test images: 71.0%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=24, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 24
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:27  loss: 0.5658 (0.5658)  acc1: 88.5417 (88.5417)  acc5: 97.9167 (97.9167)  time: 5.0055  data: 1.0277  max mem: 16225
Test:  [ 10/521]  eta: 0:30:09  loss: 0.6042 (0.6898)  acc1: 88.5417 (86.3636)  acc5: 96.8750 (96.4015)  time: 3.5402  data: 0.0936  max mem: 16226
Test:  [ 20/521]  eta: 0:28:58  loss: 0.8245 (0.8674)  acc1: 81.2500 (81.6468)  acc5: 95.8333 (95.1885)  time: 3.3936  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:11  loss: 1.0384 (0.9748)  acc1: 75.0000 (78.9651)  acc5: 91.6667 (93.9852)  time: 3.3930  data: 0.0003  max mem: 16226
Test:  [ 40/521]  eta: 0:27:31  loss: 1.2118 (1.0439)  acc1: 72.9167 (76.4736)  acc5: 91.6667 (93.5722)  time: 3.3960  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:26:53  loss: 0.7632 (0.9704)  acc1: 85.4167 (78.8399)  acc5: 94.7917 (93.9338)  time: 3.3959  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:16  loss: 0.7413 (0.9570)  acc1: 85.4167 (79.4399)  acc5: 94.7917 (94.0403)  time: 3.3931  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:40  loss: 0.7934 (0.9556)  acc1: 83.3333 (79.5041)  acc5: 94.7917 (94.1168)  time: 3.3930  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:05  loss: 0.7329 (0.9313)  acc1: 85.4167 (80.0797)  acc5: 96.8750 (94.3544)  time: 3.3939  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:30  loss: 1.0039 (0.9719)  acc1: 76.0417 (78.7317)  acc5: 94.7917 (94.1163)  time: 3.3950  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:23:55  loss: 1.2434 (1.0036)  acc1: 66.6667 (77.7228)  acc5: 92.7083 (94.0388)  time: 3.3943  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:20  loss: 1.1466 (1.0089)  acc1: 68.7500 (77.4587)  acc5: 93.7500 (94.1254)  time: 3.3941  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:46  loss: 1.0107 (1.0153)  acc1: 76.0417 (77.3502)  acc5: 94.7917 (94.0599)  time: 3.3972  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:12  loss: 1.0945 (1.0230)  acc1: 72.9167 (76.7653)  acc5: 94.7917 (94.1237)  time: 3.3989  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:37  loss: 0.9807 (1.0143)  acc1: 76.0417 (76.9356)  acc5: 95.8333 (94.2376)  time: 3.3984  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:03  loss: 0.9546 (1.0252)  acc1: 78.1250 (76.5660)  acc5: 94.7917 (94.1984)  time: 3.3966  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:29  loss: 1.0443 (1.0163)  acc1: 75.0000 (76.8828)  acc5: 95.8333 (94.2870)  time: 3.3939  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:19:54  loss: 0.7944 (1.0100)  acc1: 82.2917 (77.0163)  acc5: 95.8333 (94.3470)  time: 3.3928  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:20  loss: 0.9198 (1.0032)  acc1: 80.2083 (77.2042)  acc5: 94.7917 (94.3773)  time: 3.3929  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:18:46  loss: 0.9214 (1.0010)  acc1: 80.2083 (77.2251)  acc5: 94.7917 (94.4154)  time: 3.3939  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 0.9740 (1.0081)  acc1: 78.1250 (77.0211)  acc5: 92.7083 (94.3304)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 0.9815 (1.0076)  acc1: 75.0000 (77.0389)  acc5: 92.7083 (94.2289)  time: 3.3957  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:04  loss: 1.1450 (1.0308)  acc1: 66.6667 (76.4706)  acc5: 90.6250 (93.9480)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:29  loss: 1.3157 (1.0468)  acc1: 65.6250 (76.0146)  acc5: 90.6250 (93.7320)  time: 3.3953  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:15:55  loss: 1.3438 (1.0655)  acc1: 62.5000 (75.5446)  acc5: 89.5833 (93.5123)  time: 3.3951  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:21  loss: 1.5078 (1.0892)  acc1: 63.5417 (75.0789)  acc5: 87.5000 (93.1607)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:47  loss: 1.5928 (1.1099)  acc1: 62.5000 (74.5769)  acc5: 85.4167 (92.8839)  time: 3.3949  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.5021 (1.1314)  acc1: 61.4583 (74.0121)  acc5: 86.4583 (92.6161)  time: 3.3927  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.4615 (1.1427)  acc1: 62.5000 (73.7841)  acc5: 86.4583 (92.4674)  time: 3.3929  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.4568 (1.1521)  acc1: 69.7917 (73.6111)  acc5: 88.5417 (92.3110)  time: 3.3929  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.0970 (1.1495)  acc1: 75.0000 (73.7507)  acc5: 90.6250 (92.3000)  time: 3.3941  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.1484 (1.1652)  acc1: 73.9583 (73.4258)  acc5: 90.6250 (92.0954)  time: 3.3953  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.3327 (1.1695)  acc1: 67.7083 (73.4164)  acc5: 87.5000 (91.9879)  time: 3.3950  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3327 (1.1883)  acc1: 66.6667 (72.9733)  acc5: 89.5833 (91.7674)  time: 3.3984  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.5807 (1.1987)  acc1: 63.5417 (72.7181)  acc5: 86.4583 (91.6331)  time: 3.3982  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.5807 (1.2081)  acc1: 60.4167 (72.4626)  acc5: 86.4583 (91.5598)  time: 3.3949  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5186 (1.2201)  acc1: 64.5833 (72.1982)  acc5: 87.5000 (91.3897)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.4804 (1.2273)  acc1: 65.6250 (72.0603)  acc5: 88.5417 (91.3185)  time: 3.3938  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.3676 (1.2328)  acc1: 69.7917 (72.0445)  acc5: 89.5833 (91.2019)  time: 3.3955  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.4163 (1.2443)  acc1: 67.7083 (71.7311)  acc5: 88.5417 (91.0033)  time: 3.3979  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.5115 (1.2502)  acc1: 66.6667 (71.6776)  acc5: 86.4583 (90.9107)  time: 3.3959  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.5076 (1.2572)  acc1: 68.7500 (71.5379)  acc5: 86.4583 (90.7821)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.4390 (1.2612)  acc1: 68.7500 (71.5187)  acc5: 85.4167 (90.7042)  time: 3.3967  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.4390 (1.2690)  acc1: 67.7083 (71.3191)  acc5: 88.5417 (90.6323)  time: 3.3980  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.7565 (1.2815)  acc1: 58.3333 (71.0176)  acc5: 86.4583 (90.4455)  time: 3.3971  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.5690 (1.2855)  acc1: 64.5833 (70.8934)  acc5: 86.4583 (90.3848)  time: 3.3940  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.4757 (1.2887)  acc1: 65.6250 (70.7655)  acc5: 87.5000 (90.3674)  time: 3.3929  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.3482 (1.2937)  acc1: 67.7083 (70.6852)  acc5: 88.5417 (90.2933)  time: 3.3929  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.3482 (1.2982)  acc1: 69.7917 (70.5388)  acc5: 89.5833 (90.2677)  time: 3.3930  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1149 (1.2935)  acc1: 69.7917 (70.5894)  acc5: 92.7083 (90.3450)  time: 3.3928  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0240 (1.2866)  acc1: 77.0833 (70.7626)  acc5: 94.7917 (90.4150)  time: 3.3927  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.0980 (1.2915)  acc1: 73.9583 (70.5948)  acc5: 91.6667 (90.3845)  time: 3.3939  data: 0.0002  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2166 (1.2825)  acc1: 71.8750 (70.8200)  acc5: 90.6250 (90.4640)  time: 3.3748  data: 0.0001  max mem: 16226
Test: Total time: 0:29:29 (3.3973 s / it)
* Acc@1 70.820 Acc@5 90.464 loss 1.283
Accuracy of the network on the 50000 test images: 70.8%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=8, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 8
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:16  loss: 0.5307 (0.5307)  acc1: 88.5417 (88.5417)  acc5: 97.9167 (97.9167)  time: 4.9829  data: 0.9644  max mem: 16225
Test:  [ 10/521]  eta: 0:30:07  loss: 0.5810 (0.6713)  acc1: 88.5417 (87.9735)  acc5: 97.9167 (97.0644)  time: 3.5366  data: 0.0878  max mem: 16226
Test:  [ 20/521]  eta: 0:28:58  loss: 0.8047 (0.8613)  acc1: 83.3333 (82.5893)  acc5: 95.8333 (95.4365)  time: 3.3949  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:12  loss: 1.0543 (0.9668)  acc1: 76.0417 (79.9395)  acc5: 92.7083 (94.2204)  time: 3.3965  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:31  loss: 1.1778 (1.0413)  acc1: 72.9167 (77.0579)  acc5: 91.6667 (93.7754)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:26:53  loss: 0.7509 (0.9699)  acc1: 86.4583 (79.1871)  acc5: 94.7917 (94.1789)  time: 3.3965  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:17  loss: 0.7333 (0.9599)  acc1: 86.4583 (79.7131)  acc5: 95.8333 (94.2281)  time: 3.3969  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:25:41  loss: 0.8653 (0.9635)  acc1: 82.2917 (79.5335)  acc5: 95.8333 (94.1901)  time: 3.3957  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:06  loss: 0.7400 (0.9383)  acc1: 83.3333 (80.1312)  acc5: 96.8750 (94.4830)  time: 3.3962  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:31  loss: 0.9792 (0.9783)  acc1: 76.0417 (78.7202)  acc5: 93.7500 (94.1735)  time: 3.3960  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:23:56  loss: 1.2108 (1.0100)  acc1: 66.6667 (77.6712)  acc5: 92.7083 (94.0800)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:21  loss: 1.1722 (1.0166)  acc1: 68.7500 (77.3555)  acc5: 93.7500 (94.1817)  time: 3.3938  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:46  loss: 1.0614 (1.0222)  acc1: 75.0000 (77.1436)  acc5: 94.7917 (94.0685)  time: 3.3968  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:12  loss: 1.1265 (1.0305)  acc1: 73.9583 (76.5188)  acc5: 94.7917 (94.1555)  time: 3.3970  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:37  loss: 0.9948 (1.0226)  acc1: 75.0000 (76.6179)  acc5: 95.8333 (94.2598)  time: 3.3946  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:03  loss: 0.9365 (1.0331)  acc1: 78.1250 (76.2969)  acc5: 94.7917 (94.2536)  time: 3.3936  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:29  loss: 1.0490 (1.0238)  acc1: 78.1250 (76.6304)  acc5: 94.7917 (94.3258)  time: 3.3960  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:19:55  loss: 0.7808 (1.0164)  acc1: 82.2917 (76.7909)  acc5: 94.7917 (94.3470)  time: 3.3972  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:20  loss: 0.8883 (1.0117)  acc1: 80.2083 (77.0143)  acc5: 93.7500 (94.3485)  time: 3.3936  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:46  loss: 0.9191 (1.0103)  acc1: 80.2083 (76.9852)  acc5: 93.7500 (94.3772)  time: 3.3955  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 1.0051 (1.0172)  acc1: 77.0833 (76.8346)  acc5: 93.7500 (94.3149)  time: 3.3970  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 1.0051 (1.0151)  acc1: 77.0833 (76.9105)  acc5: 93.7500 (94.2684)  time: 3.3951  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:04  loss: 1.1869 (1.0377)  acc1: 71.8750 (76.3386)  acc5: 91.6667 (93.9527)  time: 3.3953  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:30  loss: 1.3264 (1.0532)  acc1: 66.6667 (75.9334)  acc5: 89.5833 (93.7545)  time: 3.3964  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:15:55  loss: 1.4299 (1.0716)  acc1: 65.6250 (75.4625)  acc5: 88.5417 (93.5123)  time: 3.3961  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:21  loss: 1.4578 (1.0934)  acc1: 64.5833 (75.0913)  acc5: 87.5000 (93.1441)  time: 3.3939  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:47  loss: 1.5201 (1.1137)  acc1: 63.5417 (74.6368)  acc5: 84.3750 (92.8839)  time: 3.3931  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.4978 (1.1349)  acc1: 61.4583 (74.1544)  acc5: 86.4583 (92.6161)  time: 3.3960  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.4879 (1.1459)  acc1: 62.5000 (73.9287)  acc5: 88.5417 (92.4859)  time: 3.3991  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.4857 (1.1551)  acc1: 66.6667 (73.7436)  acc5: 88.5417 (92.3396)  time: 3.3983  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.1160 (1.1519)  acc1: 76.0417 (73.9237)  acc5: 91.6667 (92.3104)  time: 3.3980  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.1171 (1.1677)  acc1: 70.8333 (73.5799)  acc5: 90.6250 (92.0920)  time: 3.3958  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.3606 (1.1722)  acc1: 68.7500 (73.5462)  acc5: 89.5833 (92.0009)  time: 3.3937  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3606 (1.1910)  acc1: 68.7500 (73.0614)  acc5: 89.5833 (91.7422)  time: 3.3951  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.6367 (1.2020)  acc1: 60.4167 (72.8128)  acc5: 87.5000 (91.5872)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.5714 (1.2115)  acc1: 61.4583 (72.5398)  acc5: 87.5000 (91.5183)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5205 (1.2237)  acc1: 61.4583 (72.2876)  acc5: 87.5000 (91.3348)  time: 3.3930  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.5284 (1.2308)  acc1: 66.6667 (72.1502)  acc5: 87.5000 (91.2567)  time: 3.3933  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.3695 (1.2355)  acc1: 70.8333 (72.1320)  acc5: 90.6250 (91.1527)  time: 3.3938  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.3722 (1.2470)  acc1: 63.5417 (71.8217)  acc5: 87.5000 (90.9793)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.5449 (1.2526)  acc1: 65.6250 (71.7451)  acc5: 86.4583 (90.9004)  time: 3.3942  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.5449 (1.2598)  acc1: 67.7083 (71.6292)  acc5: 86.4583 (90.7872)  time: 3.3945  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.5270 (1.2637)  acc1: 67.7083 (71.6053)  acc5: 86.4583 (90.7190)  time: 3.3931  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.5270 (1.2709)  acc1: 66.6667 (71.3844)  acc5: 87.5000 (90.6613)  time: 3.3931  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.7334 (1.2828)  acc1: 57.2917 (71.1121)  acc5: 85.4167 (90.5093)  time: 3.3933  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.5515 (1.2870)  acc1: 62.5000 (71.0250)  acc5: 86.4583 (90.4472)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.4722 (1.2893)  acc1: 67.7083 (70.9576)  acc5: 88.5417 (90.4239)  time: 3.3949  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.4084 (1.2950)  acc1: 67.7083 (70.8466)  acc5: 88.5417 (90.3331)  time: 3.3930  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.4403 (1.2997)  acc1: 68.7500 (70.7207)  acc5: 88.5417 (90.2785)  time: 3.3927  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1527 (1.2954)  acc1: 70.8333 (70.7930)  acc5: 91.6667 (90.3471)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0013 (1.2890)  acc1: 76.0417 (70.9539)  acc5: 94.7917 (90.4254)  time: 3.3941  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1495 (1.2939)  acc1: 72.9167 (70.7783)  acc5: 92.7083 (90.3845)  time: 3.3952  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1958 (1.2850)  acc1: 71.8750 (70.9900)  acc5: 90.6250 (90.4720)  time: 3.3733  data: 0.0001  max mem: 16226
Test: Total time: 0:29:30 (3.3973 s / it)
* Acc@1 70.990 Acc@5 90.472 loss 1.285
Accuracy of the network on the 50000 test images: 71.0%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:44  loss: 0.5646 (0.5646)  acc1: 88.5417 (88.5417)  acc5: 96.8750 (96.8750)  time: 5.0379  data: 1.0501  max mem: 16225
Test:  [ 10/521]  eta: 0:30:10  loss: 0.5872 (0.6746)  acc1: 88.5417 (86.7424)  acc5: 97.9167 (96.7803)  time: 3.5429  data: 0.0957  max mem: 16226
Test:  [ 20/521]  eta: 0:28:58  loss: 0.8329 (0.8635)  acc1: 83.3333 (81.9940)  acc5: 95.8333 (95.3373)  time: 3.3927  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:12  loss: 1.0776 (0.9754)  acc1: 75.0000 (79.0995)  acc5: 91.6667 (94.1868)  time: 3.3950  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:32  loss: 1.2185 (1.0424)  acc1: 72.9167 (76.5498)  acc5: 91.6667 (93.8008)  time: 3.3962  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:26:54  loss: 0.7353 (0.9680)  acc1: 86.4583 (78.8807)  acc5: 94.7917 (94.1381)  time: 3.3957  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:17  loss: 0.7282 (0.9581)  acc1: 86.4583 (79.4399)  acc5: 94.7917 (94.0915)  time: 3.3978  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:41  loss: 0.8509 (0.9610)  acc1: 83.3333 (79.2840)  acc5: 93.7500 (93.9554)  time: 3.3982  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:06  loss: 0.7410 (0.9352)  acc1: 83.3333 (79.9254)  acc5: 95.8333 (94.2387)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:31  loss: 0.9969 (0.9731)  acc1: 77.0833 (78.6859)  acc5: 94.7917 (94.0820)  time: 3.3925  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:23:56  loss: 1.2318 (1.0080)  acc1: 66.6667 (77.4649)  acc5: 92.7083 (93.8944)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:21  loss: 1.1523 (1.0145)  acc1: 68.7500 (77.2241)  acc5: 93.7500 (93.9471)  time: 3.3940  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:46  loss: 1.0369 (1.0216)  acc1: 76.0417 (77.1006)  acc5: 93.7500 (93.8533)  time: 3.3946  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:12  loss: 1.0986 (1.0289)  acc1: 71.8750 (76.5108)  acc5: 94.7917 (93.9647)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:37  loss: 0.9730 (1.0212)  acc1: 78.1250 (76.6770)  acc5: 95.8333 (94.0677)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:03  loss: 0.9689 (1.0320)  acc1: 78.1250 (76.3314)  acc5: 94.7917 (94.0673)  time: 3.3941  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:29  loss: 1.0583 (1.0237)  acc1: 77.0833 (76.6046)  acc5: 94.7917 (94.1706)  time: 3.3930  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:19:54  loss: 0.8458 (1.0158)  acc1: 82.2917 (76.7361)  acc5: 95.8333 (94.2434)  time: 3.3933  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:20  loss: 0.8741 (1.0105)  acc1: 80.2083 (76.9625)  acc5: 94.7917 (94.2795)  time: 3.3935  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:46  loss: 0.9071 (1.0084)  acc1: 79.1667 (76.9906)  acc5: 94.7917 (94.3172)  time: 3.3960  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 0.9667 (1.0159)  acc1: 77.0833 (76.8398)  acc5: 93.7500 (94.2527)  time: 3.3980  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 0.9990 (1.0146)  acc1: 77.0833 (76.8908)  acc5: 93.7500 (94.2091)  time: 3.3944  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:04  loss: 1.1358 (1.0366)  acc1: 70.8333 (76.3810)  acc5: 92.7083 (93.9385)  time: 3.3934  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:30  loss: 1.3308 (1.0533)  acc1: 68.7500 (75.9560)  acc5: 89.5833 (93.7410)  time: 3.3987  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:15:55  loss: 1.4315 (1.0717)  acc1: 66.6667 (75.5100)  acc5: 89.5833 (93.4691)  time: 3.4003  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:21  loss: 1.4724 (1.0937)  acc1: 66.6667 (75.1038)  acc5: 87.5000 (93.1316)  time: 3.3952  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:47  loss: 1.6191 (1.1144)  acc1: 62.5000 (74.6408)  acc5: 85.4167 (92.8560)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.5067 (1.1350)  acc1: 62.5000 (74.1275)  acc5: 86.4583 (92.6046)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.4457 (1.1462)  acc1: 64.5833 (73.9139)  acc5: 86.4583 (92.4414)  time: 3.3940  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.4457 (1.1557)  acc1: 67.7083 (73.7507)  acc5: 86.4583 (92.2967)  time: 3.3958  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.0828 (1.1528)  acc1: 73.9583 (73.8960)  acc5: 90.6250 (92.2861)  time: 3.3978  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.1345 (1.1682)  acc1: 69.7917 (73.5330)  acc5: 90.6250 (92.0987)  time: 3.3992  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.3890 (1.1727)  acc1: 69.7917 (73.5235)  acc5: 88.5417 (91.9879)  time: 3.3959  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3995 (1.1919)  acc1: 69.7917 (73.0457)  acc5: 89.5833 (91.7548)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.5673 (1.2018)  acc1: 58.3333 (72.7975)  acc5: 86.4583 (91.6239)  time: 3.3959  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.5559 (1.2110)  acc1: 58.3333 (72.5368)  acc5: 88.5417 (91.5539)  time: 3.3939  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5283 (1.2229)  acc1: 62.5000 (72.2790)  acc5: 88.5417 (91.3752)  time: 3.3929  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.5463 (1.2304)  acc1: 64.5833 (72.1502)  acc5: 88.5417 (91.3157)  time: 3.3925  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.3630 (1.2352)  acc1: 71.8750 (72.1429)  acc5: 89.5833 (91.2074)  time: 3.3929  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.4508 (1.2461)  acc1: 63.5417 (71.8244)  acc5: 88.5417 (91.0246)  time: 3.3952  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.5426 (1.2520)  acc1: 65.6250 (71.7477)  acc5: 85.4167 (90.9315)  time: 3.3978  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.5272 (1.2586)  acc1: 67.7083 (71.6368)  acc5: 85.4167 (90.8075)  time: 3.3977  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.5002 (1.2628)  acc1: 68.7500 (71.6127)  acc5: 86.4583 (90.7314)  time: 3.3965  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.5002 (1.2700)  acc1: 67.7083 (71.4376)  acc5: 87.5000 (90.6588)  time: 3.3968  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.7491 (1.2822)  acc1: 59.3750 (71.1475)  acc5: 85.4167 (90.4762)  time: 3.3949  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.5756 (1.2868)  acc1: 64.5833 (71.0504)  acc5: 86.4583 (90.4218)  time: 3.3926  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.4601 (1.2904)  acc1: 64.5833 (70.9079)  acc5: 87.5000 (90.3945)  time: 3.3936  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.3953 (1.2962)  acc1: 66.6667 (70.8179)  acc5: 89.5833 (90.3132)  time: 3.3935  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.3953 (1.3006)  acc1: 70.8333 (70.6947)  acc5: 89.5833 (90.2763)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1078 (1.2961)  acc1: 71.8750 (70.7633)  acc5: 90.6250 (90.3428)  time: 3.3941  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 0.9970 (1.2898)  acc1: 76.0417 (70.9123)  acc5: 94.7917 (90.4171)  time: 3.3952  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1306 (1.2948)  acc1: 71.8750 (70.7579)  acc5: 92.7083 (90.3641)  time: 3.3950  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2418 (1.2860)  acc1: 70.8333 (70.9720)  acc5: 91.6667 (90.4560)  time: 3.3706  data: 0.0001  max mem: 16226
Test: Total time: 0:29:30 (3.3975 s / it)
* Acc@1 70.972 Acc@5 90.456 loss 1.286
Accuracy of the network on the 50000 test images: 71.0%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=16, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 16
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:45:06  loss: 0.5529 (0.5529)  acc1: 87.5000 (87.5000)  acc5: 96.8750 (96.8750)  time: 5.1950  data: 1.2514  max mem: 16225
Test:  [ 10/521]  eta: 0:30:17  loss: 0.5904 (0.6810)  acc1: 87.5000 (86.9318)  acc5: 97.9167 (96.8750)  time: 3.5575  data: 0.1140  max mem: 16226
Test:  [ 20/521]  eta: 0:29:03  loss: 0.8396 (0.8631)  acc1: 83.3333 (81.1508)  acc5: 95.8333 (95.5357)  time: 3.3940  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:15  loss: 1.0619 (0.9713)  acc1: 76.0417 (78.8642)  acc5: 92.7083 (94.2876)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:33  loss: 1.1678 (1.0450)  acc1: 72.9167 (76.4736)  acc5: 91.6667 (93.9278)  time: 3.3952  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:26:56  loss: 0.7336 (0.9727)  acc1: 87.5000 (78.7377)  acc5: 94.7917 (94.4036)  time: 3.3990  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:19  loss: 0.7322 (0.9627)  acc1: 85.4167 (79.4057)  acc5: 95.8333 (94.4160)  time: 3.4008  data: 0.0003  max mem: 16226
Test:  [ 70/521]  eta: 0:25:43  loss: 0.8348 (0.9672)  acc1: 84.3750 (79.3427)  acc5: 94.7917 (94.3369)  time: 3.3963  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:07  loss: 0.7067 (0.9411)  acc1: 84.3750 (80.0283)  acc5: 95.8333 (94.6116)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:32  loss: 1.0071 (0.9837)  acc1: 76.0417 (78.6058)  acc5: 94.7917 (94.3452)  time: 3.3953  data: 0.0003  max mem: 16226
Test:  [100/521]  eta: 0:23:57  loss: 1.2041 (1.0153)  acc1: 66.6667 (77.6403)  acc5: 92.7083 (94.1935)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:22  loss: 1.1785 (1.0215)  acc1: 71.8750 (77.3930)  acc5: 93.7500 (94.2380)  time: 3.3926  data: 0.0003  max mem: 16226
Test:  [120/521]  eta: 0:22:47  loss: 1.0577 (1.0296)  acc1: 77.0833 (77.2297)  acc5: 93.7500 (94.0513)  time: 3.3929  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:12  loss: 1.1286 (1.0377)  acc1: 72.9167 (76.5188)  acc5: 94.7917 (94.1555)  time: 3.3932  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:38  loss: 1.0136 (1.0301)  acc1: 75.0000 (76.5662)  acc5: 95.8333 (94.2671)  time: 3.3934  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:03  loss: 0.9764 (1.0406)  acc1: 76.0417 (76.2072)  acc5: 95.8333 (94.2398)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:29  loss: 1.0717 (1.0319)  acc1: 79.1667 (76.5204)  acc5: 94.7917 (94.3000)  time: 3.3953  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:19:55  loss: 0.8787 (1.0244)  acc1: 81.2500 (76.7178)  acc5: 94.7917 (94.3470)  time: 3.3943  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:21  loss: 0.9152 (1.0182)  acc1: 80.2083 (76.9452)  acc5: 94.7917 (94.3715)  time: 3.3934  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:46  loss: 0.9138 (1.0156)  acc1: 80.2083 (76.9634)  acc5: 94.7917 (94.4099)  time: 3.3939  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 0.9701 (1.0229)  acc1: 77.0833 (76.8398)  acc5: 93.7500 (94.2942)  time: 3.3935  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 1.0173 (1.0220)  acc1: 77.0833 (76.8464)  acc5: 91.6667 (94.2141)  time: 3.3937  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:04  loss: 1.1634 (1.0444)  acc1: 68.7500 (76.3339)  acc5: 91.6667 (93.9527)  time: 3.3949  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:30  loss: 1.3220 (1.0603)  acc1: 65.6250 (75.9199)  acc5: 89.5833 (93.7320)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:15:56  loss: 1.4687 (1.0791)  acc1: 66.6667 (75.4841)  acc5: 89.5833 (93.4820)  time: 3.3960  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:21  loss: 1.5101 (1.1019)  acc1: 64.5833 (75.0540)  acc5: 86.4583 (93.1026)  time: 3.3959  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:47  loss: 1.5946 (1.1232)  acc1: 61.4583 (74.5690)  acc5: 83.3333 (92.8400)  time: 3.3959  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.5432 (1.1448)  acc1: 61.4583 (74.0237)  acc5: 85.4167 (92.5623)  time: 3.3962  data: 0.0003  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.5094 (1.1560)  acc1: 65.6250 (73.8212)  acc5: 86.4583 (92.4007)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.4942 (1.1654)  acc1: 68.7500 (73.6720)  acc5: 86.4583 (92.2358)  time: 3.3969  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.1314 (1.1624)  acc1: 72.9167 (73.8407)  acc5: 90.6250 (92.2238)  time: 3.3971  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.1834 (1.1781)  acc1: 72.9167 (73.5229)  acc5: 90.6250 (92.0251)  time: 3.3952  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.3345 (1.1825)  acc1: 69.7917 (73.4878)  acc5: 89.5833 (91.9328)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3345 (1.2009)  acc1: 69.7917 (73.0488)  acc5: 89.5833 (91.6887)  time: 3.3938  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.5814 (1.2113)  acc1: 61.4583 (72.8281)  acc5: 85.4167 (91.5598)  time: 3.3940  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.5620 (1.2203)  acc1: 62.5000 (72.5783)  acc5: 88.5417 (91.4945)  time: 3.3959  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.4797 (1.2335)  acc1: 62.5000 (72.3049)  acc5: 86.4583 (91.2944)  time: 3.3960  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.4852 (1.2403)  acc1: 65.6250 (72.1782)  acc5: 86.4583 (91.2315)  time: 3.3953  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.3231 (1.2449)  acc1: 69.7917 (72.1675)  acc5: 89.5833 (91.1199)  time: 3.3944  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.4373 (1.2555)  acc1: 65.6250 (71.8723)  acc5: 87.5000 (90.9713)  time: 3.3930  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.5367 (1.2612)  acc1: 65.6250 (71.8153)  acc5: 86.4583 (90.8822)  time: 3.3926  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.5123 (1.2677)  acc1: 65.6250 (71.6849)  acc5: 85.4167 (90.7568)  time: 3.3926  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.5058 (1.2715)  acc1: 65.6250 (71.6375)  acc5: 86.4583 (90.7067)  time: 3.3936  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.5058 (1.2790)  acc1: 65.6250 (71.4158)  acc5: 86.4583 (90.6443)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.7731 (1.2916)  acc1: 56.2500 (71.1310)  acc5: 84.3750 (90.4620)  time: 3.3954  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.6228 (1.2963)  acc1: 63.5417 (71.0366)  acc5: 86.4583 (90.4148)  time: 3.3964  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.4580 (1.2997)  acc1: 67.7083 (70.9124)  acc5: 88.5417 (90.3787)  time: 3.3958  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.4124 (1.3059)  acc1: 67.7083 (70.7913)  acc5: 88.5417 (90.2844)  time: 3.3949  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.4279 (1.3102)  acc1: 68.7500 (70.6536)  acc5: 89.5833 (90.2590)  time: 3.3936  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1729 (1.3057)  acc1: 69.7917 (70.7294)  acc5: 91.6667 (90.3280)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0354 (1.2993)  acc1: 77.0833 (70.8812)  acc5: 95.8333 (90.4046)  time: 3.3970  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1174 (1.3042)  acc1: 70.8333 (70.6927)  acc5: 92.7083 (90.3661)  time: 3.3990  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2297 (1.2952)  acc1: 69.7917 (70.9240)  acc5: 91.6667 (90.4440)  time: 3.3744  data: 0.0001  max mem: 16226
Test: Total time: 0:29:30 (3.3978 s / it)
* Acc@1 70.924 Acc@5 90.444 loss 1.295
Accuracy of the network on the 50000 test images: 70.9%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=20, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 20
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:38  loss: 0.5383 (0.5383)  acc1: 88.5417 (88.5417)  acc5: 97.9167 (97.9167)  time: 5.1420  data: 1.1590  max mem: 16225
Test:  [ 10/521]  eta: 0:30:19  loss: 0.5697 (0.6916)  acc1: 89.5833 (87.3106)  acc5: 97.9167 (96.8750)  time: 3.5611  data: 0.1058  max mem: 16226
Test:  [ 20/521]  eta: 0:29:05  loss: 0.7981 (0.8737)  acc1: 83.3333 (82.3909)  acc5: 96.8750 (95.2877)  time: 3.4007  data: 0.0003  max mem: 16226
Test:  [ 30/521]  eta: 0:28:16  loss: 0.9995 (0.9721)  acc1: 76.0417 (79.8387)  acc5: 92.7083 (94.1868)  time: 3.3963  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:34  loss: 1.1551 (1.0471)  acc1: 71.8750 (77.1341)  acc5: 92.7083 (93.8262)  time: 3.3943  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:26:56  loss: 0.7490 (0.9722)  acc1: 84.3750 (79.2688)  acc5: 95.8333 (94.3627)  time: 3.3953  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:18  loss: 0.7490 (0.9643)  acc1: 85.4167 (79.8668)  acc5: 95.8333 (94.3135)  time: 3.3946  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:42  loss: 0.8478 (0.9680)  acc1: 84.3750 (79.7829)  acc5: 94.7917 (94.1755)  time: 3.3943  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:07  loss: 0.7434 (0.9423)  acc1: 84.3750 (80.4784)  acc5: 95.8333 (94.4573)  time: 3.3944  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:31  loss: 0.9840 (0.9848)  acc1: 79.1667 (79.0064)  acc5: 94.7917 (94.2079)  time: 3.3945  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:23:56  loss: 1.2102 (1.0179)  acc1: 65.6250 (77.9909)  acc5: 91.6667 (94.0388)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:21  loss: 1.1408 (1.0240)  acc1: 72.9167 (77.7496)  acc5: 93.7500 (94.0972)  time: 3.3928  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:47  loss: 1.0243 (1.0302)  acc1: 78.1250 (77.6515)  acc5: 93.7500 (93.9394)  time: 3.3927  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:12  loss: 1.1639 (1.0386)  acc1: 72.9167 (76.9720)  acc5: 93.7500 (94.0283)  time: 3.3962  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:38  loss: 1.0095 (1.0304)  acc1: 78.1250 (77.0759)  acc5: 95.8333 (94.1416)  time: 3.3974  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:03  loss: 0.9880 (1.0408)  acc1: 79.1667 (76.7453)  acc5: 94.7917 (94.1363)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:29  loss: 1.0717 (1.0317)  acc1: 79.1667 (77.0898)  acc5: 94.7917 (94.2223)  time: 3.3949  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:19:55  loss: 0.8192 (1.0240)  acc1: 82.2917 (77.2113)  acc5: 95.8333 (94.2678)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:21  loss: 0.8738 (1.0191)  acc1: 82.2917 (77.4114)  acc5: 94.7917 (94.2795)  time: 3.3939  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:46  loss: 0.9432 (1.0169)  acc1: 79.1667 (77.3615)  acc5: 94.7917 (94.3281)  time: 3.3956  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 0.9851 (1.0237)  acc1: 76.0417 (77.2129)  acc5: 93.7500 (94.2527)  time: 3.3947  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 1.0257 (1.0224)  acc1: 76.0417 (77.2709)  acc5: 92.7083 (94.1449)  time: 3.3931  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:04  loss: 1.1542 (1.0457)  acc1: 68.7500 (76.7204)  acc5: 91.6667 (93.8631)  time: 3.3943  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:30  loss: 1.3629 (1.0618)  acc1: 67.7083 (76.3077)  acc5: 90.6250 (93.6869)  time: 3.3990  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:15:56  loss: 1.4393 (1.0805)  acc1: 68.7500 (75.8385)  acc5: 89.5833 (93.4691)  time: 3.3984  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:22  loss: 1.5038 (1.1022)  acc1: 66.6667 (75.4607)  acc5: 86.4583 (93.1109)  time: 3.3931  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:47  loss: 1.6195 (1.1231)  acc1: 61.4583 (74.9521)  acc5: 84.3750 (92.8440)  time: 3.3936  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.5062 (1.1448)  acc1: 61.4583 (74.4350)  acc5: 87.5000 (92.5969)  time: 3.3961  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.5045 (1.1553)  acc1: 63.5417 (74.1882)  acc5: 87.5000 (92.4711)  time: 3.3951  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.5045 (1.1648)  acc1: 68.7500 (73.9762)  acc5: 88.5417 (92.3110)  time: 3.3937  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.1309 (1.1620)  acc1: 76.0417 (74.1244)  acc5: 90.6250 (92.3069)  time: 3.3934  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.1309 (1.1775)  acc1: 72.9167 (73.7976)  acc5: 90.6250 (92.1054)  time: 3.3926  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.3447 (1.1818)  acc1: 68.7500 (73.7766)  acc5: 88.5417 (92.0074)  time: 3.3961  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3447 (1.2009)  acc1: 68.7500 (73.2943)  acc5: 88.5417 (91.7485)  time: 3.3968  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.6131 (1.2112)  acc1: 60.4167 (73.0511)  acc5: 86.4583 (91.6117)  time: 3.3948  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.6131 (1.2210)  acc1: 61.4583 (72.7861)  acc5: 85.4167 (91.5183)  time: 3.3940  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5441 (1.2337)  acc1: 64.5833 (72.5531)  acc5: 86.4583 (91.3348)  time: 3.3960  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.5039 (1.2408)  acc1: 65.6250 (72.4113)  acc5: 88.5417 (91.2764)  time: 3.3989  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.3148 (1.2456)  acc1: 69.7917 (72.3863)  acc5: 90.6250 (91.1855)  time: 3.3988  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.4199 (1.2563)  acc1: 66.6667 (72.0801)  acc5: 87.5000 (91.0140)  time: 3.3970  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.4916 (1.2617)  acc1: 65.6250 (71.9711)  acc5: 86.4583 (90.9315)  time: 3.3937  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.5351 (1.2693)  acc1: 65.6250 (71.8345)  acc5: 85.4167 (90.7897)  time: 3.3939  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.5275 (1.2734)  acc1: 66.6667 (71.7884)  acc5: 85.4167 (90.7289)  time: 3.3951  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.5275 (1.2810)  acc1: 66.6667 (71.5850)  acc5: 87.5000 (90.6588)  time: 3.3948  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.7987 (1.2943)  acc1: 54.1667 (71.2703)  acc5: 83.3333 (90.4620)  time: 3.3950  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.5659 (1.2992)  acc1: 63.5417 (71.1567)  acc5: 84.3750 (90.3987)  time: 3.3954  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.5195 (1.3024)  acc1: 65.6250 (71.0661)  acc5: 89.5833 (90.3674)  time: 3.3941  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.4281 (1.3084)  acc1: 68.7500 (70.9572)  acc5: 87.5000 (90.2800)  time: 3.3956  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.4281 (1.3130)  acc1: 69.7917 (70.8312)  acc5: 88.5417 (90.2547)  time: 3.3980  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1787 (1.3085)  acc1: 70.8333 (70.9118)  acc5: 92.7083 (90.3322)  time: 3.3960  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0184 (1.3019)  acc1: 78.1250 (71.0828)  acc5: 94.7917 (90.4067)  time: 3.3958  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1685 (1.3067)  acc1: 73.9583 (70.9251)  acc5: 92.7083 (90.3682)  time: 3.3957  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1685 (1.2975)  acc1: 72.9167 (71.1440)  acc5: 92.7083 (90.4460)  time: 3.3725  data: 0.0001  max mem: 16226
Test: Total time: 0:29:30 (3.3980 s / it)
* Acc@1 71.144 Acc@5 90.446 loss 1.298
Accuracy of the network on the 50000 test images: 71.1%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=24, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 24
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:58  loss: 0.5604 (0.5604)  acc1: 89.5833 (89.5833)  acc5: 97.9167 (97.9167)  time: 5.0645  data: 1.0247  max mem: 16225
Test:  [ 10/521]  eta: 0:30:12  loss: 0.6004 (0.6833)  acc1: 89.5833 (87.4053)  acc5: 97.9167 (97.2538)  time: 3.5470  data: 0.0933  max mem: 16226
Test:  [ 20/521]  eta: 0:29:01  loss: 0.8823 (0.8645)  acc1: 79.1667 (81.6964)  acc5: 96.8750 (95.4861)  time: 3.3976  data: 0.0001  max mem: 16226
Test:  [ 30/521]  eta: 0:28:13  loss: 1.0070 (0.9710)  acc1: 77.0833 (79.3011)  acc5: 91.6667 (94.2540)  time: 3.3967  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:27:33  loss: 1.1704 (1.0413)  acc1: 72.9167 (76.5498)  acc5: 91.6667 (93.7500)  time: 3.3951  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:26:55  loss: 0.7590 (0.9687)  acc1: 87.5000 (78.8399)  acc5: 95.8333 (94.1585)  time: 3.3971  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:18  loss: 0.7280 (0.9550)  acc1: 85.4167 (79.5594)  acc5: 95.8333 (94.2794)  time: 3.3981  data: 0.0003  max mem: 16226
Test:  [ 70/521]  eta: 0:25:42  loss: 0.8329 (0.9579)  acc1: 84.3750 (79.5775)  acc5: 94.7917 (94.1755)  time: 3.3982  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:06  loss: 0.7156 (0.9322)  acc1: 84.3750 (80.3369)  acc5: 96.8750 (94.4573)  time: 3.3953  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:31  loss: 0.9932 (0.9737)  acc1: 77.0833 (78.8118)  acc5: 94.7917 (94.2193)  time: 3.3940  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:23:56  loss: 1.2452 (1.0080)  acc1: 65.6250 (77.5887)  acc5: 92.7083 (94.0697)  time: 3.3941  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:21  loss: 1.1683 (1.0133)  acc1: 69.7917 (77.4775)  acc5: 93.7500 (94.1911)  time: 3.3952  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:22:47  loss: 1.0073 (1.0189)  acc1: 77.0833 (77.4105)  acc5: 93.7500 (94.0341)  time: 3.3971  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:12  loss: 1.1227 (1.0256)  acc1: 73.9583 (76.7971)  acc5: 93.7500 (94.1317)  time: 3.3981  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:38  loss: 0.9684 (1.0173)  acc1: 77.0833 (76.8913)  acc5: 95.8333 (94.2450)  time: 3.3973  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:04  loss: 0.9832 (1.0281)  acc1: 77.0833 (76.5315)  acc5: 94.7917 (94.1777)  time: 3.3967  data: 0.0003  max mem: 16226
Test:  [160/521]  eta: 0:20:29  loss: 1.0642 (1.0190)  acc1: 76.0417 (76.8763)  acc5: 94.7917 (94.2482)  time: 3.3965  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:19:55  loss: 0.8190 (1.0116)  acc1: 84.3750 (76.9981)  acc5: 94.7917 (94.2556)  time: 3.3946  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:21  loss: 0.8257 (1.0056)  acc1: 80.2083 (77.1927)  acc5: 95.8333 (94.3025)  time: 3.3939  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:18:46  loss: 0.9206 (1.0036)  acc1: 80.2083 (77.1597)  acc5: 95.8333 (94.3336)  time: 3.3926  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:12  loss: 0.9959 (1.0103)  acc1: 77.0833 (77.0367)  acc5: 93.7500 (94.2579)  time: 3.3932  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:38  loss: 0.9976 (1.0096)  acc1: 77.0833 (77.0636)  acc5: 92.7083 (94.1647)  time: 3.3933  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:04  loss: 1.1545 (1.0316)  acc1: 70.8333 (76.5272)  acc5: 89.5833 (93.8773)  time: 3.3932  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:30  loss: 1.3356 (1.0474)  acc1: 66.6667 (76.1183)  acc5: 88.5417 (93.6733)  time: 3.3953  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:15:56  loss: 1.3882 (1.0656)  acc1: 64.5833 (75.6700)  acc5: 88.5417 (93.4215)  time: 3.3954  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:22  loss: 1.4786 (1.0873)  acc1: 64.5833 (75.3071)  acc5: 87.5000 (93.0818)  time: 3.3954  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:47  loss: 1.6022 (1.1080)  acc1: 61.4583 (74.8244)  acc5: 84.3750 (92.8321)  time: 3.3953  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:13  loss: 1.4420 (1.1283)  acc1: 60.4167 (74.2428)  acc5: 87.5000 (92.6084)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:39  loss: 1.4420 (1.1396)  acc1: 63.5417 (74.0436)  acc5: 87.5000 (92.4451)  time: 3.3959  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:05  loss: 1.4511 (1.1490)  acc1: 68.7500 (73.8509)  acc5: 88.5417 (92.3253)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:31  loss: 1.1256 (1.1463)  acc1: 72.9167 (74.0241)  acc5: 90.6250 (92.3138)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:11:57  loss: 1.1662 (1.1611)  acc1: 69.7917 (73.7239)  acc5: 90.6250 (92.1255)  time: 3.3941  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:23  loss: 1.3611 (1.1654)  acc1: 69.7917 (73.7085)  acc5: 89.5833 (92.0334)  time: 3.3960  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:49  loss: 1.3768 (1.1837)  acc1: 70.8333 (73.2565)  acc5: 90.6250 (91.8051)  time: 3.3971  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:15  loss: 1.5972 (1.1949)  acc1: 58.3333 (73.0022)  acc5: 88.5417 (91.6697)  time: 3.3940  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:41  loss: 1.5792 (1.2047)  acc1: 62.5000 (72.6971)  acc5: 88.5417 (91.5925)  time: 3.3930  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:07  loss: 1.5564 (1.2166)  acc1: 64.5833 (72.4550)  acc5: 88.5417 (91.4329)  time: 3.3939  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:33  loss: 1.4651 (1.2239)  acc1: 67.7083 (72.3158)  acc5: 88.5417 (91.3606)  time: 3.3933  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:07:59  loss: 1.3451 (1.2287)  acc1: 70.8333 (72.2988)  acc5: 90.6250 (91.2702)  time: 3.3928  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:25  loss: 1.3837 (1.2401)  acc1: 64.5833 (71.9842)  acc5: 88.5417 (91.0992)  time: 3.3932  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:51  loss: 1.5409 (1.2458)  acc1: 65.6250 (71.9140)  acc5: 86.4583 (91.0121)  time: 3.3931  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:17  loss: 1.4728 (1.2526)  acc1: 67.7083 (71.7888)  acc5: 86.4583 (90.8632)  time: 3.3929  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:43  loss: 1.4580 (1.2564)  acc1: 67.7083 (71.7488)  acc5: 86.4583 (90.7982)  time: 3.3929  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:09  loss: 1.4580 (1.2637)  acc1: 67.7083 (71.5487)  acc5: 88.5417 (90.7120)  time: 3.3949  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:35  loss: 1.7312 (1.2759)  acc1: 59.3750 (71.2561)  acc5: 83.3333 (90.5447)  time: 3.3959  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:01  loss: 1.5346 (1.2796)  acc1: 65.6250 (71.1983)  acc5: 86.4583 (90.5026)  time: 3.3940  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:27  loss: 1.4811 (1.2836)  acc1: 65.6250 (71.0864)  acc5: 88.5417 (90.4691)  time: 3.3932  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:53  loss: 1.3693 (1.2891)  acc1: 66.6667 (70.9881)  acc5: 88.5417 (90.3839)  time: 3.3942  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:19  loss: 1.3298 (1.2934)  acc1: 69.7917 (70.8766)  acc5: 88.5417 (90.3586)  time: 3.3959  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:45  loss: 1.1122 (1.2887)  acc1: 70.8333 (70.9564)  acc5: 92.7083 (90.4489)  time: 3.3948  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:11  loss: 1.0491 (1.2825)  acc1: 77.0833 (71.1036)  acc5: 94.7917 (90.5190)  time: 3.3930  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1270 (1.2876)  acc1: 71.8750 (70.9475)  acc5: 92.7083 (90.4803)  time: 3.3930  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2634 (1.2789)  acc1: 69.7917 (71.1840)  acc5: 91.6667 (90.5680)  time: 3.3711  data: 0.0001  max mem: 16226
Test: Total time: 0:29:30 (3.3973 s / it)
* Acc@1 71.184 Acc@5 90.568 loss 1.279
Accuracy of the network on the 50000 test images: 71.2%
