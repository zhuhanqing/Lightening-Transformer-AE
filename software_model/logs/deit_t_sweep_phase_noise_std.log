nohup: ignoring input
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:43:22  loss: 0.5769 (0.5769)  acc1: 89.5833 (89.5833)  acc5: 97.9167 (97.9167)  time: 4.9947  data: 0.9950  max mem: 16225
Test:  [ 10/521]  eta: 0:30:09  loss: 0.5769 (0.6858)  acc1: 89.5833 (86.5530)  acc5: 97.9167 (96.9697)  time: 3.5418  data: 0.0906  max mem: 16226
Test:  [ 20/521]  eta: 0:29:00  loss: 0.8459 (0.8663)  acc1: 81.2500 (81.6468)  acc5: 95.8333 (95.3869)  time: 3.3977  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:13  loss: 1.0805 (0.9779)  acc1: 76.0417 (78.7634)  acc5: 92.7083 (94.2204)  time: 3.3983  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:27:34  loss: 1.1603 (1.0473)  acc1: 70.8333 (76.3211)  acc5: 91.6667 (93.8262)  time: 3.4052  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:26:58  loss: 0.7606 (0.9742)  acc1: 84.3750 (78.6152)  acc5: 95.8333 (94.2810)  time: 3.4174  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:23  loss: 0.7259 (0.9649)  acc1: 85.4167 (79.2691)  acc5: 95.8333 (94.2623)  time: 3.4215  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:48  loss: 0.8552 (0.9655)  acc1: 82.2917 (79.1960)  acc5: 94.7917 (94.1608)  time: 3.4242  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:13  loss: 0.7517 (0.9415)  acc1: 82.2917 (79.8225)  acc5: 95.8333 (94.3930)  time: 3.4290  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:24:39  loss: 1.0319 (0.9798)  acc1: 76.0417 (78.5829)  acc5: 94.7917 (94.1735)  time: 3.4325  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:24:05  loss: 1.2399 (1.0124)  acc1: 68.7500 (77.4546)  acc5: 92.7083 (94.0903)  time: 3.4339  data: 0.0001  max mem: 16226
Test:  [110/521]  eta: 0:23:31  loss: 1.1789 (1.0176)  acc1: 69.7917 (77.1866)  acc5: 94.7917 (94.1817)  time: 3.4354  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:22:56  loss: 1.0264 (1.0239)  acc1: 73.9583 (77.0489)  acc5: 93.7500 (94.0685)  time: 3.4375  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:22  loss: 1.1433 (1.0319)  acc1: 72.9167 (76.4790)  acc5: 93.7500 (94.1794)  time: 3.4380  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:21:48  loss: 1.0055 (1.0232)  acc1: 75.0000 (76.7287)  acc5: 95.8333 (94.2819)  time: 3.4410  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:14  loss: 0.9547 (1.0332)  acc1: 79.1667 (76.4073)  acc5: 94.7917 (94.2743)  time: 3.4440  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:40  loss: 1.0760 (1.0246)  acc1: 77.0833 (76.6887)  acc5: 94.7917 (94.3129)  time: 3.4446  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:06  loss: 0.8463 (1.0168)  acc1: 81.2500 (76.8640)  acc5: 94.7917 (94.3348)  time: 3.4445  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:32  loss: 0.8907 (1.0098)  acc1: 81.2500 (77.1179)  acc5: 94.7917 (94.4003)  time: 3.4456  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:18:57  loss: 0.9013 (1.0074)  acc1: 79.1667 (77.1161)  acc5: 94.7917 (94.4372)  time: 3.4484  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:23  loss: 1.0076 (1.0148)  acc1: 76.0417 (76.9486)  acc5: 93.7500 (94.3253)  time: 3.4502  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:49  loss: 1.0140 (1.0141)  acc1: 75.0000 (76.9994)  acc5: 92.7083 (94.2684)  time: 3.4521  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:15  loss: 1.2063 (1.0371)  acc1: 69.7917 (76.4753)  acc5: 90.6250 (93.9480)  time: 3.4526  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:41  loss: 1.3119 (1.0532)  acc1: 68.7500 (76.0552)  acc5: 89.5833 (93.7094)  time: 3.4493  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:06  loss: 1.3527 (1.0720)  acc1: 64.5833 (75.6051)  acc5: 89.5833 (93.4820)  time: 3.4477  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:32  loss: 1.4808 (1.0950)  acc1: 64.5833 (75.2158)  acc5: 87.5000 (93.1316)  time: 3.4489  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:58  loss: 1.4920 (1.1153)  acc1: 62.5000 (74.7126)  acc5: 83.3333 (92.8560)  time: 3.4520  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:23  loss: 1.4920 (1.1371)  acc1: 61.4583 (74.1236)  acc5: 87.5000 (92.6084)  time: 3.4544  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:49  loss: 1.4679 (1.1479)  acc1: 63.5417 (73.9101)  acc5: 87.5000 (92.4488)  time: 3.4535  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:15  loss: 1.4526 (1.1579)  acc1: 67.7083 (73.7257)  acc5: 86.4583 (92.2752)  time: 3.4530  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:40  loss: 1.1088 (1.1548)  acc1: 73.9583 (73.8684)  acc5: 91.6667 (92.2792)  time: 3.4535  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:06  loss: 1.1105 (1.1704)  acc1: 70.8333 (73.5095)  acc5: 91.6667 (92.0954)  time: 3.4538  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:32  loss: 1.4197 (1.1748)  acc1: 69.7917 (73.5040)  acc5: 88.5417 (92.0009)  time: 3.4550  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:57  loss: 1.3688 (1.1935)  acc1: 69.7917 (73.0488)  acc5: 90.6250 (91.7516)  time: 3.4559  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:23  loss: 1.6057 (1.2041)  acc1: 63.5417 (72.8311)  acc5: 85.4167 (91.5903)  time: 3.4566  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:49  loss: 1.6057 (1.2133)  acc1: 63.5417 (72.5487)  acc5: 85.4167 (91.5005)  time: 3.4561  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:14  loss: 1.5211 (1.2254)  acc1: 64.5833 (72.3194)  acc5: 85.4167 (91.3089)  time: 3.4577  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:40  loss: 1.5110 (1.2329)  acc1: 66.6667 (72.1474)  acc5: 86.4583 (91.2539)  time: 3.4586  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:05  loss: 1.4058 (1.2377)  acc1: 69.7917 (72.1484)  acc5: 89.5833 (91.1445)  time: 3.4575  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:31  loss: 1.4058 (1.2480)  acc1: 66.6667 (71.8750)  acc5: 88.5417 (90.9580)  time: 3.4581  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:57  loss: 1.5398 (1.2543)  acc1: 65.6250 (71.7893)  acc5: 85.4167 (90.8588)  time: 3.4609  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:22  loss: 1.4818 (1.2608)  acc1: 68.7500 (71.6900)  acc5: 85.4167 (90.7619)  time: 3.4623  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:48  loss: 1.4818 (1.2650)  acc1: 68.7500 (71.6746)  acc5: 85.4167 (90.6992)  time: 3.4593  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:13  loss: 1.6131 (1.2725)  acc1: 67.7083 (71.4835)  acc5: 88.5417 (90.6298)  time: 3.4609  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:39  loss: 1.7444 (1.2845)  acc1: 59.3750 (71.2042)  acc5: 84.3750 (90.4644)  time: 3.4623  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:04  loss: 1.5783 (1.2893)  acc1: 63.5417 (71.0989)  acc5: 85.4167 (90.3987)  time: 3.4602  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:30  loss: 1.4870 (1.2922)  acc1: 66.6667 (70.9960)  acc5: 87.5000 (90.3674)  time: 3.4606  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:55  loss: 1.4062 (1.2978)  acc1: 69.7917 (70.8953)  acc5: 89.5833 (90.2999)  time: 3.4620  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:21  loss: 1.4062 (1.3024)  acc1: 69.7917 (70.7489)  acc5: 90.6250 (90.2612)  time: 3.4614  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:46  loss: 1.1035 (1.2975)  acc1: 70.8333 (70.8333)  acc5: 92.7083 (90.3386)  time: 3.4600  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 1.0038 (1.2909)  acc1: 77.0833 (70.9851)  acc5: 93.7500 (90.4005)  time: 3.4622  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1576 (1.2960)  acc1: 71.8750 (70.8252)  acc5: 92.7083 (90.3518)  time: 3.4639  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1894 (1.2871)  acc1: 70.8333 (71.0520)  acc5: 91.6667 (90.4320)  time: 3.4398  data: 0.0001  max mem: 16226
Test: Total time: 0:29:57 (3.4492 s / it)
* Acc@1 71.052 Acc@5 90.432 loss 1.287
Accuracy of the network on the 50000 test images: 71.1%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=3.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 3.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:26  loss: 0.5456 (0.5456)  acc1: 89.5833 (89.5833)  acc5: 97.9167 (97.9167)  time: 5.1172  data: 1.0775  max mem: 16225
Test:  [ 10/521]  eta: 0:30:45  loss: 0.6001 (0.6856)  acc1: 90.6250 (86.4583)  acc5: 97.9167 (96.9697)  time: 3.6121  data: 0.0982  max mem: 16226
Test:  [ 20/521]  eta: 0:29:33  loss: 0.8326 (0.8642)  acc1: 82.2917 (81.6964)  acc5: 96.8750 (95.3373)  time: 3.4603  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:44  loss: 1.0780 (0.9732)  acc1: 75.0000 (78.5954)  acc5: 92.7083 (94.1196)  time: 3.4587  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:28:03  loss: 1.2232 (1.0458)  acc1: 68.7500 (76.0671)  acc5: 92.7083 (93.5976)  time: 3.4589  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:24  loss: 0.7662 (0.9734)  acc1: 85.4167 (78.4518)  acc5: 94.7917 (94.0972)  time: 3.4599  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:47  loss: 0.7351 (0.9658)  acc1: 85.4167 (79.0642)  acc5: 95.8333 (94.1257)  time: 3.4619  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:26:11  loss: 0.8378 (0.9672)  acc1: 83.3333 (79.1373)  acc5: 94.7917 (94.1021)  time: 3.4622  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:35  loss: 0.7275 (0.9406)  acc1: 83.3333 (79.8997)  acc5: 95.8333 (94.3544)  time: 3.4619  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:59  loss: 0.9977 (0.9784)  acc1: 77.0833 (78.6287)  acc5: 94.7917 (94.1506)  time: 3.4618  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:24:23  loss: 1.2503 (1.0135)  acc1: 69.7917 (77.4237)  acc5: 92.7083 (93.9563)  time: 3.4613  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:48  loss: 1.1684 (1.0187)  acc1: 70.8333 (77.1396)  acc5: 93.7500 (94.0409)  time: 3.4629  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:23:13  loss: 1.0321 (1.0258)  acc1: 76.0417 (77.0231)  acc5: 93.7500 (93.8275)  time: 3.4629  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:38  loss: 1.1151 (1.0334)  acc1: 73.9583 (76.4393)  acc5: 94.7917 (93.9249)  time: 3.4620  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:22:03  loss: 0.9902 (1.0243)  acc1: 75.0000 (76.6622)  acc5: 94.7917 (94.0160)  time: 3.4622  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:28  loss: 0.9184 (1.0329)  acc1: 80.2083 (76.4004)  acc5: 94.7917 (94.0121)  time: 3.4619  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:53  loss: 1.0610 (1.0245)  acc1: 80.2083 (76.6887)  acc5: 94.7917 (94.1059)  time: 3.4638  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:18  loss: 0.8167 (1.0168)  acc1: 82.2917 (76.8640)  acc5: 94.7917 (94.1277)  time: 3.4658  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:43  loss: 0.8962 (1.0100)  acc1: 80.2083 (77.0833)  acc5: 94.7917 (94.1701)  time: 3.4671  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:19:08  loss: 0.9051 (1.0071)  acc1: 79.1667 (77.1052)  acc5: 94.7917 (94.2408)  time: 3.4656  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:34  loss: 0.9623 (1.0142)  acc1: 78.1250 (76.9434)  acc5: 93.7500 (94.1594)  time: 3.4627  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:59  loss: 0.9623 (1.0135)  acc1: 78.1250 (76.9599)  acc5: 92.7083 (94.1153)  time: 3.4641  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:24  loss: 1.1667 (1.0366)  acc1: 67.7083 (76.4235)  acc5: 91.6667 (93.8443)  time: 3.4639  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:49  loss: 1.2912 (1.0516)  acc1: 66.6667 (75.9695)  acc5: 90.6250 (93.6373)  time: 3.4639  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:16:14  loss: 1.3905 (1.0705)  acc1: 66.6667 (75.5316)  acc5: 88.5417 (93.3999)  time: 3.4651  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:40  loss: 1.5041 (1.0929)  acc1: 66.6667 (75.1328)  acc5: 87.5000 (93.0569)  time: 3.4650  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:15:05  loss: 1.5865 (1.1144)  acc1: 62.5000 (74.6368)  acc5: 87.5000 (92.7802)  time: 3.4666  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:30  loss: 1.4967 (1.1353)  acc1: 60.4167 (74.0813)  acc5: 87.5000 (92.5469)  time: 3.4660  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:56  loss: 1.4829 (1.1463)  acc1: 64.5833 (73.8508)  acc5: 88.5417 (92.4192)  time: 3.4637  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:21  loss: 1.4783 (1.1566)  acc1: 66.6667 (73.6326)  acc5: 89.5833 (92.2716)  time: 3.4659  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:46  loss: 1.1099 (1.1537)  acc1: 76.0417 (73.7991)  acc5: 90.6250 (92.2550)  time: 3.4698  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:12:11  loss: 1.1168 (1.1693)  acc1: 71.8750 (73.4291)  acc5: 91.6667 (92.0552)  time: 3.4680  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:37  loss: 1.3571 (1.1738)  acc1: 66.6667 (73.3969)  acc5: 88.5417 (91.9555)  time: 3.4665  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:11:02  loss: 1.3552 (1.1923)  acc1: 68.7500 (72.9576)  acc5: 89.5833 (91.7013)  time: 3.4663  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:27  loss: 1.5955 (1.2028)  acc1: 65.6250 (72.7578)  acc5: 86.4583 (91.5598)  time: 3.4642  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:53  loss: 1.5928 (1.2116)  acc1: 65.6250 (72.4982)  acc5: 88.5417 (91.4945)  time: 3.4650  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:18  loss: 1.5133 (1.2239)  acc1: 64.5833 (72.2501)  acc5: 86.4583 (91.3118)  time: 3.4664  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:43  loss: 1.4804 (1.2312)  acc1: 64.5833 (72.0772)  acc5: 86.4583 (91.2511)  time: 3.4662  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:09  loss: 1.4248 (1.2361)  acc1: 68.7500 (72.0500)  acc5: 89.5833 (91.1499)  time: 3.4651  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:34  loss: 1.4936 (1.2468)  acc1: 66.6667 (71.7924)  acc5: 88.5417 (90.9660)  time: 3.4643  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:59  loss: 1.5466 (1.2527)  acc1: 67.7083 (71.7269)  acc5: 85.4167 (90.8822)  time: 3.4649  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:24  loss: 1.5466 (1.2597)  acc1: 67.7083 (71.6114)  acc5: 86.4583 (90.7644)  time: 3.4653  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:50  loss: 1.5599 (1.2638)  acc1: 66.6667 (71.5855)  acc5: 84.3750 (90.6770)  time: 3.4650  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:15  loss: 1.5724 (1.2714)  acc1: 66.6667 (71.4013)  acc5: 87.5000 (90.5936)  time: 3.4654  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:40  loss: 1.8197 (1.2844)  acc1: 59.3750 (71.0979)  acc5: 84.3750 (90.4148)  time: 3.4656  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:06  loss: 1.5742 (1.2890)  acc1: 61.4583 (70.9742)  acc5: 86.4583 (90.3663)  time: 3.4669  data: 0.0003  max mem: 16226
Test:  [460/521]  eta: 0:03:31  loss: 1.5286 (1.2917)  acc1: 67.7083 (70.8785)  acc5: 89.5833 (90.3448)  time: 3.4658  data: 0.0003  max mem: 16226
Test:  [470/521]  eta: 0:02:56  loss: 1.3774 (1.2974)  acc1: 67.7083 (70.7714)  acc5: 88.5417 (90.2601)  time: 3.4635  data: 0.0003  max mem: 16226
Test:  [480/521]  eta: 0:02:22  loss: 1.3764 (1.3016)  acc1: 66.6667 (70.6363)  acc5: 88.5417 (90.2330)  time: 3.4638  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:47  loss: 1.1048 (1.2968)  acc1: 71.8750 (70.7379)  acc5: 92.7083 (90.3068)  time: 3.4630  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 1.0191 (1.2898)  acc1: 78.1250 (70.9061)  acc5: 94.7917 (90.3776)  time: 3.4635  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:38  loss: 1.1399 (1.2946)  acc1: 72.9167 (70.7273)  acc5: 91.6667 (90.3253)  time: 3.4661  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1837 (1.2857)  acc1: 71.8750 (70.9640)  acc5: 90.6250 (90.4080)  time: 3.4412  data: 0.0001  max mem: 16226
Test: Total time: 0:30:06 (3.4666 s / it)
* Acc@1 70.964 Acc@5 90.408 loss 1.286
Accuracy of the network on the 50000 test images: 71.0%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=4.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 4.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:05  loss: 0.5077 (0.5077)  acc1: 91.6667 (91.6667)  acc5: 97.9167 (97.9167)  time: 5.0769  data: 1.0313  max mem: 16225
Test:  [ 10/521]  eta: 0:30:43  loss: 0.6122 (0.6844)  acc1: 89.5833 (86.1742)  acc5: 97.9167 (97.2538)  time: 3.6071  data: 0.0940  max mem: 16226
Test:  [ 20/521]  eta: 0:29:32  loss: 0.8633 (0.8687)  acc1: 82.2917 (81.5972)  acc5: 95.8333 (95.3869)  time: 3.4607  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:44  loss: 1.0512 (0.9767)  acc1: 75.0000 (78.5618)  acc5: 91.6667 (94.0860)  time: 3.4608  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:28:04  loss: 1.2326 (1.0463)  acc1: 68.7500 (75.9400)  acc5: 91.6667 (93.8008)  time: 3.4634  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:25  loss: 0.7873 (0.9746)  acc1: 85.4167 (78.4314)  acc5: 94.7917 (94.1789)  time: 3.4666  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:48  loss: 0.7396 (0.9662)  acc1: 85.4167 (79.0813)  acc5: 94.7917 (94.1940)  time: 3.4643  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:26:11  loss: 0.8460 (0.9666)  acc1: 83.3333 (79.1667)  acc5: 94.7917 (94.2048)  time: 3.4617  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:35  loss: 0.7286 (0.9417)  acc1: 83.3333 (79.9254)  acc5: 95.8333 (94.4573)  time: 3.4627  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:25:00  loss: 1.0360 (0.9793)  acc1: 77.0833 (78.5943)  acc5: 94.7917 (94.1850)  time: 3.4625  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:24  loss: 1.2413 (1.0127)  acc1: 68.7500 (77.4443)  acc5: 92.7083 (94.0388)  time: 3.4623  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:49  loss: 1.1813 (1.0182)  acc1: 69.7917 (77.2616)  acc5: 93.7500 (94.1441)  time: 3.4642  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:23:14  loss: 1.0498 (1.0247)  acc1: 76.0417 (77.1092)  acc5: 93.7500 (94.0083)  time: 3.4657  data: 0.0003  max mem: 16226
Test:  [130/521]  eta: 0:22:39  loss: 1.1153 (1.0313)  acc1: 73.9583 (76.5426)  acc5: 93.7500 (94.1317)  time: 3.4683  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:22:03  loss: 0.9928 (1.0220)  acc1: 77.0833 (76.7361)  acc5: 95.8333 (94.2376)  time: 3.4658  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:28  loss: 0.9377 (1.0314)  acc1: 78.1250 (76.4487)  acc5: 95.8333 (94.1915)  time: 3.4619  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:53  loss: 1.0164 (1.0221)  acc1: 78.1250 (76.7857)  acc5: 95.8333 (94.2676)  time: 3.4623  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:20:18  loss: 0.8180 (1.0146)  acc1: 83.3333 (76.9737)  acc5: 95.8333 (94.3104)  time: 3.4626  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:44  loss: 0.8883 (1.0090)  acc1: 82.2917 (77.1869)  acc5: 94.7917 (94.3600)  time: 3.4643  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:19:09  loss: 0.9444 (1.0063)  acc1: 82.2917 (77.2088)  acc5: 94.7917 (94.4045)  time: 3.4637  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:34  loss: 0.9773 (1.0133)  acc1: 77.0833 (77.0782)  acc5: 93.7500 (94.3201)  time: 3.4623  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:59  loss: 0.9937 (1.0125)  acc1: 77.0833 (77.0587)  acc5: 93.7500 (94.2536)  time: 3.4635  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:24  loss: 1.1470 (1.0348)  acc1: 68.7500 (76.5979)  acc5: 91.6667 (93.9762)  time: 3.4650  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:49  loss: 1.3490 (1.0507)  acc1: 66.6667 (76.1905)  acc5: 90.6250 (93.7771)  time: 3.4640  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:16:15  loss: 1.3490 (1.0692)  acc1: 64.5833 (75.7175)  acc5: 89.5833 (93.5382)  time: 3.4617  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:40  loss: 1.4478 (1.0915)  acc1: 65.6250 (75.3362)  acc5: 88.5417 (93.2105)  time: 3.4623  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:15:05  loss: 1.6296 (1.1126)  acc1: 64.5833 (74.8404)  acc5: 85.4167 (92.9318)  time: 3.4631  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:30  loss: 1.4978 (1.1339)  acc1: 64.5833 (74.3466)  acc5: 87.5000 (92.6661)  time: 3.4639  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:56  loss: 1.4964 (1.1455)  acc1: 64.5833 (74.0807)  acc5: 88.5417 (92.5156)  time: 3.4648  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:21  loss: 1.4843 (1.1559)  acc1: 66.6667 (73.8796)  acc5: 87.5000 (92.3396)  time: 3.4634  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:46  loss: 1.1325 (1.1531)  acc1: 73.9583 (73.9826)  acc5: 90.6250 (92.3242)  time: 3.4622  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:12:11  loss: 1.1325 (1.1687)  acc1: 71.8750 (73.6569)  acc5: 90.6250 (92.1155)  time: 3.4632  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:37  loss: 1.3622 (1.1731)  acc1: 70.8333 (73.6111)  acc5: 88.5417 (91.9879)  time: 3.4643  data: 0.0003  max mem: 16226
Test:  [330/521]  eta: 0:11:02  loss: 1.3622 (1.1925)  acc1: 70.8333 (73.1212)  acc5: 89.5833 (91.7328)  time: 3.4646  data: 0.0003  max mem: 16226
Test:  [340/521]  eta: 0:10:27  loss: 1.6042 (1.2027)  acc1: 62.5000 (72.9228)  acc5: 88.5417 (91.5995)  time: 3.4651  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:53  loss: 1.5952 (1.2121)  acc1: 65.6250 (72.6555)  acc5: 87.5000 (91.5153)  time: 3.4635  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:18  loss: 1.5391 (1.2247)  acc1: 64.5833 (72.3973)  acc5: 86.4583 (91.3233)  time: 3.4633  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:43  loss: 1.5170 (1.2316)  acc1: 65.6250 (72.2204)  acc5: 87.5000 (91.2820)  time: 3.4635  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:08  loss: 1.3836 (1.2362)  acc1: 68.7500 (72.1949)  acc5: 89.5833 (91.1827)  time: 3.4618  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:34  loss: 1.4472 (1.2476)  acc1: 62.5000 (71.9176)  acc5: 87.5000 (90.9900)  time: 3.4613  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:59  loss: 1.5418 (1.2535)  acc1: 65.6250 (71.8542)  acc5: 86.4583 (90.9004)  time: 3.4615  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:24  loss: 1.5217 (1.2604)  acc1: 67.7083 (71.7356)  acc5: 86.4583 (90.7796)  time: 3.4617  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:50  loss: 1.5351 (1.2642)  acc1: 67.7083 (71.7117)  acc5: 85.4167 (90.7215)  time: 3.4622  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:15  loss: 1.5679 (1.2719)  acc1: 66.6667 (71.5391)  acc5: 87.5000 (90.6443)  time: 3.4622  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:40  loss: 1.7473 (1.2843)  acc1: 57.2917 (71.2254)  acc5: 83.3333 (90.4667)  time: 3.4621  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:06  loss: 1.5983 (1.2886)  acc1: 62.5000 (71.1151)  acc5: 86.4583 (90.4218)  time: 3.4625  data: 0.0004  max mem: 16226
Test:  [460/521]  eta: 0:03:31  loss: 1.5198 (1.2915)  acc1: 65.6250 (71.0186)  acc5: 89.5833 (90.3968)  time: 3.4617  data: 0.0005  max mem: 16226
Test:  [470/521]  eta: 0:02:56  loss: 1.3884 (1.2971)  acc1: 68.7500 (70.9218)  acc5: 89.5833 (90.3286)  time: 3.4617  data: 0.0003  max mem: 16226
Test:  [480/521]  eta: 0:02:22  loss: 1.3884 (1.3016)  acc1: 68.7500 (70.7879)  acc5: 90.6250 (90.2958)  time: 3.4610  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:47  loss: 1.1843 (1.2968)  acc1: 68.7500 (70.8715)  acc5: 92.7083 (90.3831)  time: 3.4602  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 1.0246 (1.2901)  acc1: 78.1250 (71.0350)  acc5: 94.7917 (90.4566)  time: 3.4596  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:38  loss: 1.1739 (1.2952)  acc1: 72.9167 (70.8823)  acc5: 92.7083 (90.4089)  time: 3.4616  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1922 (1.2861)  acc1: 68.7500 (71.1200)  acc5: 91.6667 (90.4880)  time: 3.4395  data: 0.0001  max mem: 16226
Test: Total time: 0:30:05 (3.4653 s / it)
* Acc@1 71.120 Acc@5 90.488 loss 1.286
Accuracy of the network on the 50000 test images: 71.1%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=5.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 5.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:21  loss: 0.6140 (0.6140)  acc1: 87.5000 (87.5000)  acc5: 96.8750 (96.8750)  time: 5.1078  data: 1.0509  max mem: 16225
Test:  [ 10/521]  eta: 0:30:44  loss: 0.6140 (0.7010)  acc1: 89.5833 (86.1742)  acc5: 97.9167 (96.8750)  time: 3.6104  data: 0.0958  max mem: 16226
Test:  [ 20/521]  eta: 0:29:33  loss: 0.8476 (0.8732)  acc1: 83.3333 (81.4484)  acc5: 96.8750 (95.1389)  time: 3.4616  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:45  loss: 1.0368 (0.9757)  acc1: 73.9583 (78.6290)  acc5: 91.6667 (94.0860)  time: 3.4630  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:28:04  loss: 1.1869 (1.0499)  acc1: 69.7917 (76.1941)  acc5: 91.6667 (93.6230)  time: 3.4638  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:27:25  loss: 0.7367 (0.9752)  acc1: 85.4167 (78.6765)  acc5: 95.8333 (94.0972)  time: 3.4624  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:48  loss: 0.7247 (0.9668)  acc1: 84.3750 (79.2350)  acc5: 94.7917 (93.9891)  time: 3.4604  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:26:11  loss: 0.8256 (0.9674)  acc1: 82.2917 (79.1227)  acc5: 94.7917 (94.0141)  time: 3.4606  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:35  loss: 0.7689 (0.9420)  acc1: 81.2500 (79.7968)  acc5: 95.8333 (94.2773)  time: 3.4607  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:24:59  loss: 1.0047 (0.9818)  acc1: 76.0417 (78.5142)  acc5: 94.7917 (94.0362)  time: 3.4610  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:24  loss: 1.2710 (1.0139)  acc1: 67.7083 (77.5268)  acc5: 91.6667 (93.8635)  time: 3.4609  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:48  loss: 1.1639 (1.0187)  acc1: 71.8750 (77.3743)  acc5: 93.7500 (93.9752)  time: 3.4608  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:23:13  loss: 1.0437 (1.0248)  acc1: 76.0417 (77.2125)  acc5: 93.7500 (93.8189)  time: 3.4617  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:38  loss: 1.1247 (1.0321)  acc1: 73.9583 (76.5506)  acc5: 94.7917 (93.9170)  time: 3.4611  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:22:03  loss: 1.0023 (1.0232)  acc1: 77.0833 (76.7731)  acc5: 95.8333 (94.0381)  time: 3.4603  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:28  loss: 0.9136 (1.0325)  acc1: 80.2083 (76.4556)  acc5: 95.8333 (94.0259)  time: 3.4602  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:53  loss: 1.0646 (1.0235)  acc1: 79.1667 (76.7404)  acc5: 94.7917 (94.1123)  time: 3.4607  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:20:18  loss: 0.8357 (1.0155)  acc1: 80.2083 (76.9432)  acc5: 94.7917 (94.1520)  time: 3.4607  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:43  loss: 0.8993 (1.0090)  acc1: 79.1667 (77.1351)  acc5: 94.7917 (94.2047)  time: 3.4600  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:19:08  loss: 0.9250 (1.0064)  acc1: 79.1667 (77.1651)  acc5: 94.7917 (94.2354)  time: 3.4599  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:33  loss: 0.9900 (1.0139)  acc1: 77.0833 (77.0108)  acc5: 92.7083 (94.1180)  time: 3.4602  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:58  loss: 1.0018 (1.0134)  acc1: 77.0833 (77.0438)  acc5: 91.6667 (94.0413)  time: 3.4620  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:24  loss: 1.1896 (1.0354)  acc1: 70.8333 (76.5272)  acc5: 90.6250 (93.7783)  time: 3.4615  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:49  loss: 1.2970 (1.0513)  acc1: 66.6667 (76.1138)  acc5: 89.5833 (93.5426)  time: 3.4613  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:14  loss: 1.3900 (1.0706)  acc1: 65.6250 (75.6224)  acc5: 88.5417 (93.3178)  time: 3.4626  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:39  loss: 1.4974 (1.0928)  acc1: 64.5833 (75.2158)  acc5: 88.5417 (92.9988)  time: 3.4616  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:15:05  loss: 1.6060 (1.1139)  acc1: 63.5417 (74.7725)  acc5: 85.4167 (92.7482)  time: 3.4621  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:30  loss: 1.4696 (1.1348)  acc1: 62.5000 (74.2466)  acc5: 85.4167 (92.5008)  time: 3.4613  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:55  loss: 1.4583 (1.1459)  acc1: 64.5833 (74.0251)  acc5: 87.5000 (92.3451)  time: 3.4608  data: 0.0003  max mem: 16226
Test:  [290/521]  eta: 0:13:20  loss: 1.4955 (1.1558)  acc1: 67.7083 (73.8581)  acc5: 87.5000 (92.1857)  time: 3.4608  data: 0.0004  max mem: 16226
Test:  [300/521]  eta: 0:12:46  loss: 1.1314 (1.1529)  acc1: 72.9167 (73.9929)  acc5: 91.6667 (92.1789)  time: 3.4622  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:11  loss: 1.1791 (1.1687)  acc1: 72.9167 (73.5999)  acc5: 91.6667 (91.9949)  time: 3.4628  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:36  loss: 1.3267 (1.1733)  acc1: 64.5833 (73.5365)  acc5: 89.5833 (91.8873)  time: 3.4605  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:11:02  loss: 1.3267 (1.1925)  acc1: 67.7083 (73.0551)  acc5: 89.5833 (91.6289)  time: 3.4604  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:27  loss: 1.6321 (1.2031)  acc1: 62.5000 (72.8067)  acc5: 86.4583 (91.4864)  time: 3.4623  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:52  loss: 1.6066 (1.2120)  acc1: 62.5000 (72.5368)  acc5: 88.5417 (91.4144)  time: 3.4628  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:17  loss: 1.4806 (1.2243)  acc1: 65.6250 (72.3165)  acc5: 86.4583 (91.2252)  time: 3.4606  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:43  loss: 1.5302 (1.2320)  acc1: 65.6250 (72.1389)  acc5: 87.5000 (91.1529)  time: 3.4610  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:08  loss: 1.3569 (1.2364)  acc1: 66.6667 (72.1320)  acc5: 89.5833 (91.0570)  time: 3.4619  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:33  loss: 1.4189 (1.2471)  acc1: 63.5417 (71.8484)  acc5: 86.4583 (90.8781)  time: 3.4612  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:59  loss: 1.5657 (1.2530)  acc1: 64.5833 (71.7945)  acc5: 86.4583 (90.8068)  time: 3.4621  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:24  loss: 1.5405 (1.2599)  acc1: 67.7083 (71.6596)  acc5: 86.4583 (90.6884)  time: 3.4617  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:49  loss: 1.5200 (1.2640)  acc1: 68.7500 (71.6647)  acc5: 86.4583 (90.6225)  time: 3.4598  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:15  loss: 1.5200 (1.2717)  acc1: 68.7500 (71.4762)  acc5: 87.5000 (90.5549)  time: 3.4624  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:40  loss: 1.7306 (1.2840)  acc1: 60.4167 (71.2065)  acc5: 84.3750 (90.3723)  time: 3.4626  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:06  loss: 1.5701 (1.2882)  acc1: 64.5833 (71.1128)  acc5: 87.5000 (90.3294)  time: 3.4621  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:31  loss: 1.4283 (1.2912)  acc1: 66.6667 (71.0231)  acc5: 89.5833 (90.2974)  time: 3.4638  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:56  loss: 1.4084 (1.2965)  acc1: 69.7917 (70.9196)  acc5: 89.5833 (90.2291)  time: 3.4638  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:22  loss: 1.3971 (1.3011)  acc1: 68.7500 (70.7814)  acc5: 89.5833 (90.1897)  time: 3.4612  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:47  loss: 1.1522 (1.2964)  acc1: 70.8333 (70.8673)  acc5: 91.6667 (90.2643)  time: 3.4611  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 1.0168 (1.2896)  acc1: 78.1250 (71.0309)  acc5: 93.7500 (90.3422)  time: 3.4618  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:38  loss: 1.1474 (1.2943)  acc1: 72.9167 (70.8761)  acc5: 93.7500 (90.3131)  time: 3.4608  data: 0.0002  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1935 (1.2854)  acc1: 71.8750 (71.1160)  acc5: 91.6667 (90.4020)  time: 3.4375  data: 0.0001  max mem: 16226
Test: Total time: 0:30:04 (3.4638 s / it)
* Acc@1 71.116 Acc@5 90.402 loss 1.285
Accuracy of the network on the 50000 test images: 71.1%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=6.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 6.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:55  loss: 0.5590 (0.5590)  acc1: 88.5417 (88.5417)  acc5: 96.8750 (96.8750)  time: 5.1742  data: 1.2294  max mem: 16225
Test:  [ 10/521]  eta: 0:30:47  loss: 0.5648 (0.6824)  acc1: 90.6250 (86.1742)  acc5: 96.8750 (96.7803)  time: 3.6150  data: 0.1119  max mem: 16226
Test:  [ 20/521]  eta: 0:29:33  loss: 0.8915 (0.8644)  acc1: 81.2500 (81.2004)  acc5: 96.8750 (95.2877)  time: 3.4581  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:45  loss: 1.0361 (0.9679)  acc1: 72.9167 (78.4274)  acc5: 91.6667 (94.1532)  time: 3.4587  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:28:04  loss: 1.1888 (1.0429)  acc1: 69.7917 (75.9400)  acc5: 91.6667 (93.7500)  time: 3.4617  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:27:25  loss: 0.7747 (0.9696)  acc1: 85.4167 (78.4314)  acc5: 94.7917 (94.2198)  time: 3.4611  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:48  loss: 0.7438 (0.9612)  acc1: 85.4167 (79.0642)  acc5: 95.8333 (94.2794)  time: 3.4610  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:26:11  loss: 0.8476 (0.9636)  acc1: 82.2917 (79.0493)  acc5: 95.8333 (94.2782)  time: 3.4624  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:35  loss: 0.7325 (0.9391)  acc1: 82.2917 (79.7325)  acc5: 95.8333 (94.5345)  time: 3.4614  data: 0.0003  max mem: 16226
Test:  [ 90/521]  eta: 0:24:59  loss: 1.0209 (0.9776)  acc1: 78.1250 (78.4684)  acc5: 93.7500 (94.2308)  time: 3.4601  data: 0.0003  max mem: 16226
Test:  [100/521]  eta: 0:24:23  loss: 1.2329 (1.0106)  acc1: 67.7083 (77.3721)  acc5: 92.7083 (94.0697)  time: 3.4593  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:48  loss: 1.1022 (1.0172)  acc1: 71.8750 (77.2241)  acc5: 93.7500 (94.1817)  time: 3.4604  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:23:13  loss: 1.0335 (1.0232)  acc1: 73.9583 (77.1092)  acc5: 93.7500 (94.0599)  time: 3.4624  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:38  loss: 1.1532 (1.0308)  acc1: 71.8750 (76.5188)  acc5: 94.7917 (94.1714)  time: 3.4614  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:22:03  loss: 1.0056 (1.0226)  acc1: 76.0417 (76.7140)  acc5: 95.8333 (94.3115)  time: 3.4608  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:28  loss: 0.9735 (1.0332)  acc1: 78.1250 (76.3590)  acc5: 95.8333 (94.2536)  time: 3.4604  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:53  loss: 1.0521 (1.0235)  acc1: 78.1250 (76.6757)  acc5: 95.8333 (94.3582)  time: 3.4591  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:20:18  loss: 0.8394 (1.0155)  acc1: 83.3333 (76.7909)  acc5: 95.8333 (94.3835)  time: 3.4614  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:43  loss: 0.8577 (1.0087)  acc1: 80.2083 (77.0603)  acc5: 94.7917 (94.4176)  time: 3.4613  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:19:08  loss: 0.9136 (1.0064)  acc1: 80.2083 (77.0561)  acc5: 94.7917 (94.4263)  time: 3.4616  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:33  loss: 0.9897 (1.0140)  acc1: 78.1250 (76.9279)  acc5: 93.7500 (94.3356)  time: 3.4641  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:58  loss: 0.9897 (1.0131)  acc1: 78.1250 (76.9500)  acc5: 92.7083 (94.2931)  time: 3.4624  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:24  loss: 1.1655 (1.0352)  acc1: 69.7917 (76.4046)  acc5: 91.6667 (93.9998)  time: 3.4598  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:49  loss: 1.3197 (1.0512)  acc1: 67.7083 (75.9921)  acc5: 89.5833 (93.7590)  time: 3.4601  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:16:14  loss: 1.3197 (1.0700)  acc1: 66.6667 (75.5619)  acc5: 88.5417 (93.5166)  time: 3.4610  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:39  loss: 1.5132 (1.0928)  acc1: 66.6667 (75.1453)  acc5: 86.4583 (93.1399)  time: 3.4596  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:15:04  loss: 1.5558 (1.1139)  acc1: 61.4583 (74.6408)  acc5: 86.4583 (92.8839)  time: 3.4593  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:30  loss: 1.5099 (1.1345)  acc1: 61.4583 (74.1351)  acc5: 86.4583 (92.6315)  time: 3.4613  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:55  loss: 1.4705 (1.1457)  acc1: 64.5833 (73.9287)  acc5: 87.5000 (92.4637)  time: 3.4621  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:20  loss: 1.4705 (1.1558)  acc1: 67.7083 (73.7471)  acc5: 87.5000 (92.2752)  time: 3.4610  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:46  loss: 1.1361 (1.1530)  acc1: 75.0000 (73.8891)  acc5: 90.6250 (92.2688)  time: 3.4603  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:12:11  loss: 1.1571 (1.1692)  acc1: 72.9167 (73.5397)  acc5: 90.6250 (92.0385)  time: 3.4609  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:36  loss: 1.3318 (1.1734)  acc1: 68.7500 (73.5267)  acc5: 87.5000 (91.9100)  time: 3.4621  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:11:02  loss: 1.3318 (1.1922)  acc1: 69.7917 (73.0677)  acc5: 88.5417 (91.6698)  time: 3.4611  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:27  loss: 1.6339 (1.2027)  acc1: 61.4583 (72.8556)  acc5: 87.5000 (91.5353)  time: 3.4596  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:52  loss: 1.6275 (1.2118)  acc1: 64.5833 (72.5932)  acc5: 88.5417 (91.4649)  time: 3.4588  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:17  loss: 1.5128 (1.2241)  acc1: 65.6250 (72.3482)  acc5: 86.4583 (91.2916)  time: 3.4580  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:43  loss: 1.5089 (1.2314)  acc1: 66.6667 (72.2063)  acc5: 88.5417 (91.2287)  time: 3.4593  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:08  loss: 1.4451 (1.2363)  acc1: 68.7500 (72.1976)  acc5: 88.5417 (91.1253)  time: 3.4602  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:33  loss: 1.4625 (1.2473)  acc1: 63.5417 (71.9123)  acc5: 87.5000 (90.9420)  time: 3.4591  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:59  loss: 1.5513 (1.2534)  acc1: 63.5417 (71.8230)  acc5: 86.4583 (90.8640)  time: 3.4583  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:24  loss: 1.5159 (1.2599)  acc1: 68.7500 (71.7432)  acc5: 86.4583 (90.7593)  time: 3.4591  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:49  loss: 1.5159 (1.2641)  acc1: 67.7083 (71.7265)  acc5: 85.4167 (90.7067)  time: 3.4600  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:15  loss: 1.5237 (1.2715)  acc1: 67.7083 (71.5487)  acc5: 87.5000 (90.6274)  time: 3.4604  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:40  loss: 1.7339 (1.2839)  acc1: 59.3750 (71.2491)  acc5: 85.4167 (90.4573)  time: 3.4607  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:05  loss: 1.6054 (1.2880)  acc1: 62.5000 (71.1544)  acc5: 85.4167 (90.4102)  time: 3.4607  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:31  loss: 1.4917 (1.2910)  acc1: 66.6667 (71.0412)  acc5: 88.5417 (90.3855)  time: 3.4620  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:56  loss: 1.3972 (1.2966)  acc1: 66.6667 (70.9306)  acc5: 88.5417 (90.2977)  time: 3.4643  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:22  loss: 1.3611 (1.3012)  acc1: 68.7500 (70.7770)  acc5: 88.5417 (90.2633)  time: 3.4614  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:47  loss: 1.1262 (1.2963)  acc1: 69.7917 (70.8546)  acc5: 93.7500 (90.3513)  time: 3.4585  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 1.0110 (1.2899)  acc1: 76.0417 (71.0205)  acc5: 94.7917 (90.4192)  time: 3.4615  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:38  loss: 1.1482 (1.2950)  acc1: 72.9167 (70.8537)  acc5: 92.7083 (90.3661)  time: 3.4605  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1984 (1.2858)  acc1: 71.8750 (71.0860)  acc5: 91.6667 (90.4520)  time: 3.4370  data: 0.0001  max mem: 16226
Test: Total time: 0:30:04 (3.4632 s / it)
* Acc@1 71.086 Acc@5 90.452 loss 1.286
Accuracy of the network on the 50000 test images: 71.1%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=1, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=7.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
1
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 7.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:24  loss: 0.5667 (0.5667)  acc1: 88.5417 (88.5417)  acc5: 97.9167 (97.9167)  time: 5.1143  data: 1.0836  max mem: 16225
Test:  [ 10/521]  eta: 0:30:50  loss: 0.6133 (0.6799)  acc1: 89.5833 (86.8371)  acc5: 97.9167 (96.7803)  time: 3.6214  data: 0.0987  max mem: 16226
Test:  [ 20/521]  eta: 0:29:38  loss: 0.8631 (0.8594)  acc1: 80.2083 (81.7460)  acc5: 95.8333 (95.2381)  time: 3.4716  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:50  loss: 1.0399 (0.9702)  acc1: 75.0000 (79.1331)  acc5: 91.6667 (94.1196)  time: 3.4704  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:28:08  loss: 1.2197 (1.0411)  acc1: 71.8750 (76.3465)  acc5: 91.6667 (93.8008)  time: 3.4673  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:27:28  loss: 0.7509 (0.9694)  acc1: 86.4583 (78.6765)  acc5: 94.7917 (94.1993)  time: 3.4654  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:51  loss: 0.7390 (0.9615)  acc1: 85.4167 (79.3374)  acc5: 94.7917 (94.1428)  time: 3.4665  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:26:14  loss: 0.8636 (0.9623)  acc1: 83.3333 (79.3574)  acc5: 93.7500 (94.1168)  time: 3.4669  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:38  loss: 0.7218 (0.9377)  acc1: 83.3333 (79.9769)  acc5: 96.8750 (94.4187)  time: 3.4669  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:25:02  loss: 1.0305 (0.9764)  acc1: 77.0833 (78.5829)  acc5: 94.7917 (94.1392)  time: 3.4677  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:24:26  loss: 1.2042 (1.0089)  acc1: 66.6667 (77.5578)  acc5: 92.7083 (93.9563)  time: 3.4673  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:51  loss: 1.1510 (1.0160)  acc1: 70.8333 (77.2898)  acc5: 93.7500 (94.0503)  time: 3.4665  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:23:16  loss: 1.0164 (1.0222)  acc1: 72.9167 (77.1092)  acc5: 93.7500 (93.9222)  time: 3.4670  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:40  loss: 1.0951 (1.0299)  acc1: 72.9167 (76.5188)  acc5: 93.7500 (94.0283)  time: 3.4673  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:22:05  loss: 0.9870 (1.0216)  acc1: 75.0000 (76.6548)  acc5: 94.7917 (94.1194)  time: 3.4668  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:30  loss: 0.9372 (1.0324)  acc1: 78.1250 (76.3245)  acc5: 94.7917 (94.0880)  time: 3.4661  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:55  loss: 1.0319 (1.0235)  acc1: 78.1250 (76.5787)  acc5: 95.8333 (94.1706)  time: 3.4665  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:20  loss: 0.8329 (1.0152)  acc1: 81.2500 (76.8458)  acc5: 95.8333 (94.2191)  time: 3.4674  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:45  loss: 0.8982 (1.0088)  acc1: 80.2083 (77.0891)  acc5: 94.7917 (94.2622)  time: 3.4707  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:19:10  loss: 0.9109 (1.0059)  acc1: 80.2083 (77.2033)  acc5: 94.7917 (94.2845)  time: 3.4699  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:35  loss: 0.9763 (1.0127)  acc1: 77.0833 (77.0678)  acc5: 93.7500 (94.2061)  time: 3.4675  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:18:00  loss: 0.9962 (1.0119)  acc1: 76.0417 (77.0735)  acc5: 92.7083 (94.1647)  time: 3.4672  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:25  loss: 1.1359 (1.0346)  acc1: 69.7917 (76.5460)  acc5: 91.6667 (93.8773)  time: 3.4658  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:51  loss: 1.3331 (1.0506)  acc1: 66.6667 (76.1273)  acc5: 89.5833 (93.6688)  time: 3.4676  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:16  loss: 1.3331 (1.0689)  acc1: 66.6667 (75.6872)  acc5: 89.5833 (93.4431)  time: 3.4682  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:41  loss: 1.4769 (1.0917)  acc1: 66.6667 (75.2822)  acc5: 87.5000 (93.0777)  time: 3.4698  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:15:06  loss: 1.5989 (1.1125)  acc1: 62.5000 (74.8204)  acc5: 86.4583 (92.8081)  time: 3.4691  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:31  loss: 1.5113 (1.1342)  acc1: 61.4583 (74.2697)  acc5: 84.3750 (92.5392)  time: 3.4659  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:57  loss: 1.4794 (1.1454)  acc1: 61.4583 (73.9991)  acc5: 87.5000 (92.4081)  time: 3.4658  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:22  loss: 1.4696 (1.1554)  acc1: 69.7917 (73.8295)  acc5: 88.5417 (92.2215)  time: 3.4690  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:47  loss: 1.1128 (1.1522)  acc1: 75.0000 (74.0033)  acc5: 89.5833 (92.2238)  time: 3.4697  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:12  loss: 1.1128 (1.1680)  acc1: 71.8750 (73.6468)  acc5: 90.6250 (92.0150)  time: 3.4689  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:38  loss: 1.3576 (1.1723)  acc1: 70.8333 (73.6176)  acc5: 88.5417 (91.9230)  time: 3.4684  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:11:03  loss: 1.3979 (1.1912)  acc1: 70.8333 (73.1779)  acc5: 88.5417 (91.6572)  time: 3.4689  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:28  loss: 1.6295 (1.2019)  acc1: 62.5000 (72.9747)  acc5: 86.4583 (91.5139)  time: 3.4696  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:53  loss: 1.6176 (1.2111)  acc1: 65.6250 (72.7030)  acc5: 89.5833 (91.4678)  time: 3.4670  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:19  loss: 1.4976 (1.2232)  acc1: 65.6250 (72.4406)  acc5: 88.5417 (91.3060)  time: 3.4656  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:44  loss: 1.5248 (1.2307)  acc1: 65.6250 (72.2849)  acc5: 89.5833 (91.2455)  time: 3.4664  data: 0.0004  max mem: 16226
Test:  [380/521]  eta: 0:08:09  loss: 1.3512 (1.2351)  acc1: 68.7500 (72.2660)  acc5: 90.6250 (91.1445)  time: 3.4671  data: 0.0004  max mem: 16226
Test:  [390/521]  eta: 0:07:34  loss: 1.4341 (1.2460)  acc1: 63.5417 (71.9709)  acc5: 87.5000 (90.9580)  time: 3.4678  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:07:00  loss: 1.5478 (1.2519)  acc1: 67.7083 (71.9218)  acc5: 86.4583 (90.8770)  time: 3.4677  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:25  loss: 1.5087 (1.2589)  acc1: 68.7500 (71.8218)  acc5: 86.4583 (90.7745)  time: 3.4697  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:50  loss: 1.5087 (1.2627)  acc1: 65.6250 (71.7736)  acc5: 86.4583 (90.7067)  time: 3.4716  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:15  loss: 1.5190 (1.2707)  acc1: 65.6250 (71.5681)  acc5: 87.5000 (90.6250)  time: 3.4675  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:41  loss: 1.7831 (1.2833)  acc1: 62.5000 (71.2727)  acc5: 83.3333 (90.4526)  time: 3.4660  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:06  loss: 1.5634 (1.2871)  acc1: 63.5417 (71.1821)  acc5: 86.4583 (90.4033)  time: 3.4664  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:31  loss: 1.4676 (1.2901)  acc1: 67.7083 (71.0616)  acc5: 88.5417 (90.3719)  time: 3.4695  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:57  loss: 1.3579 (1.2957)  acc1: 68.7500 (70.9660)  acc5: 88.5417 (90.2977)  time: 3.4695  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:22  loss: 1.3579 (1.3002)  acc1: 69.7917 (70.8225)  acc5: 89.5833 (90.2698)  time: 3.4673  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:47  loss: 1.1483 (1.2955)  acc1: 70.8333 (70.9203)  acc5: 92.7083 (90.3513)  time: 3.4665  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 1.0008 (1.2889)  acc1: 79.1667 (71.0745)  acc5: 94.7917 (90.4192)  time: 3.4652  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:38  loss: 1.1546 (1.2939)  acc1: 72.9167 (70.9312)  acc5: 91.6667 (90.3641)  time: 3.4662  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1848 (1.2848)  acc1: 72.9167 (71.1680)  acc5: 90.6250 (90.4460)  time: 3.4433  data: 0.0001  max mem: 16226
Test: Total time: 0:30:07 (3.4702 s / it)
* Acc@1 71.168 Acc@5 90.446 loss 1.285
Accuracy of the network on the 50000 test images: 71.2%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:28  loss: 0.5551 (0.5551)  acc1: 89.5833 (89.5833)  acc5: 96.8750 (96.8750)  time: 5.1211  data: 1.0777  max mem: 16225
Test:  [ 10/521]  eta: 0:30:47  loss: 0.6131 (0.6938)  acc1: 89.5833 (86.6477)  acc5: 97.9167 (96.6856)  time: 3.6154  data: 0.0981  max mem: 16226
Test:  [ 20/521]  eta: 0:29:34  loss: 0.8386 (0.8655)  acc1: 83.3333 (81.9444)  acc5: 96.8750 (95.4365)  time: 3.4634  data: 0.0001  max mem: 16226
Test:  [ 30/521]  eta: 0:28:46  loss: 1.0412 (0.9725)  acc1: 72.9167 (79.2339)  acc5: 91.6667 (94.0860)  time: 3.4627  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:28:05  loss: 1.2520 (1.0426)  acc1: 71.8750 (76.7531)  acc5: 92.7083 (93.7246)  time: 3.4631  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:27:26  loss: 0.7782 (0.9714)  acc1: 86.4583 (79.1667)  acc5: 94.7917 (94.1381)  time: 3.4624  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:49  loss: 0.7358 (0.9587)  acc1: 86.4583 (79.7985)  acc5: 94.7917 (94.1598)  time: 3.4640  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:26:12  loss: 0.8338 (0.9575)  acc1: 83.3333 (79.7975)  acc5: 93.7500 (94.0728)  time: 3.4654  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:36  loss: 0.7051 (0.9320)  acc1: 83.3333 (80.4141)  acc5: 95.8333 (94.3416)  time: 3.4649  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:25:00  loss: 0.9948 (0.9712)  acc1: 75.0000 (79.0751)  acc5: 93.7500 (94.0934)  time: 3.4634  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:24:25  loss: 1.2373 (1.0045)  acc1: 66.6667 (78.0528)  acc5: 92.7083 (93.9563)  time: 3.4643  data: 0.0001  max mem: 16226
Test:  [110/521]  eta: 0:23:49  loss: 1.1539 (1.0117)  acc1: 69.7917 (77.6652)  acc5: 94.7917 (94.0597)  time: 3.4650  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:23:14  loss: 1.0496 (1.0185)  acc1: 76.0417 (77.5826)  acc5: 94.7917 (93.9566)  time: 3.4624  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:39  loss: 1.0989 (1.0262)  acc1: 73.9583 (77.0038)  acc5: 93.7500 (94.0681)  time: 3.4619  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:22:04  loss: 1.0083 (1.0191)  acc1: 75.0000 (77.1129)  acc5: 95.8333 (94.1637)  time: 3.4621  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:29  loss: 0.9873 (1.0296)  acc1: 76.0417 (76.7591)  acc5: 95.8333 (94.1294)  time: 3.4635  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:54  loss: 1.0917 (1.0210)  acc1: 76.0417 (77.0445)  acc5: 95.8333 (94.2094)  time: 3.4636  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:19  loss: 0.8222 (1.0131)  acc1: 80.2083 (77.1869)  acc5: 95.8333 (94.2617)  time: 3.4643  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:44  loss: 0.8834 (1.0054)  acc1: 80.2083 (77.4459)  acc5: 94.7917 (94.3198)  time: 3.4640  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:19:09  loss: 0.8834 (1.0024)  acc1: 80.2083 (77.4596)  acc5: 94.7917 (94.3663)  time: 3.4620  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:34  loss: 0.9729 (1.0099)  acc1: 78.1250 (77.2803)  acc5: 93.7500 (94.3045)  time: 3.4621  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:59  loss: 1.0047 (1.0084)  acc1: 78.1250 (77.2512)  acc5: 92.7083 (94.2684)  time: 3.4636  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:24  loss: 1.1611 (1.0316)  acc1: 69.7917 (76.7110)  acc5: 91.6667 (94.0092)  time: 3.4643  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:49  loss: 1.3097 (1.0483)  acc1: 63.5417 (76.2762)  acc5: 89.5833 (93.7365)  time: 3.4641  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:15  loss: 1.3763 (1.0666)  acc1: 64.5833 (75.7910)  acc5: 89.5833 (93.4863)  time: 3.4674  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:40  loss: 1.5307 (1.0899)  acc1: 64.5833 (75.3445)  acc5: 86.4583 (93.1109)  time: 3.4663  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:15:05  loss: 1.5843 (1.1114)  acc1: 61.4583 (74.8044)  acc5: 84.3750 (92.8520)  time: 3.4634  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:30  loss: 1.4527 (1.1322)  acc1: 61.4583 (74.2389)  acc5: 86.4583 (92.6276)  time: 3.4635  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:56  loss: 1.4279 (1.1428)  acc1: 61.4583 (73.9954)  acc5: 88.5417 (92.4859)  time: 3.4627  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:21  loss: 1.4583 (1.1525)  acc1: 69.7917 (73.8044)  acc5: 88.5417 (92.3289)  time: 3.4622  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:46  loss: 1.1056 (1.1497)  acc1: 75.0000 (73.9756)  acc5: 90.6250 (92.3069)  time: 3.4622  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:11  loss: 1.1834 (1.1650)  acc1: 75.0000 (73.6837)  acc5: 90.6250 (92.1423)  time: 3.4633  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:37  loss: 1.3789 (1.1698)  acc1: 67.7083 (73.6598)  acc5: 87.5000 (92.0236)  time: 3.4637  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:11:02  loss: 1.3789 (1.1884)  acc1: 68.7500 (73.2125)  acc5: 89.5833 (91.8083)  time: 3.4632  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:27  loss: 1.5934 (1.1989)  acc1: 63.5417 (72.9717)  acc5: 87.5000 (91.6545)  time: 3.4622  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:53  loss: 1.5934 (1.2080)  acc1: 63.5417 (72.7089)  acc5: 87.5000 (91.5865)  time: 3.4630  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:18  loss: 1.5582 (1.2197)  acc1: 63.5417 (72.4550)  acc5: 86.4583 (91.4329)  time: 3.4632  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:43  loss: 1.5582 (1.2272)  acc1: 65.6250 (72.2709)  acc5: 89.5833 (91.3634)  time: 3.4616  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:08:08  loss: 1.3073 (1.2322)  acc1: 67.7083 (72.2496)  acc5: 90.6250 (91.2648)  time: 3.4624  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:34  loss: 1.4436 (1.2428)  acc1: 63.5417 (71.9656)  acc5: 88.5417 (91.0939)  time: 3.4638  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:59  loss: 1.5147 (1.2487)  acc1: 64.5833 (71.8880)  acc5: 86.4583 (90.9861)  time: 3.4658  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:24  loss: 1.5139 (1.2554)  acc1: 67.7083 (71.7787)  acc5: 86.4583 (90.8506)  time: 3.4644  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:50  loss: 1.5139 (1.2597)  acc1: 67.7083 (71.7364)  acc5: 84.3750 (90.7809)  time: 3.4649  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:15  loss: 1.5650 (1.2672)  acc1: 66.6667 (71.5729)  acc5: 88.5417 (90.7120)  time: 3.4668  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:40  loss: 1.7251 (1.2799)  acc1: 60.4167 (71.2845)  acc5: 84.3750 (90.5234)  time: 3.4632  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:06  loss: 1.5889 (1.2844)  acc1: 63.5417 (71.1729)  acc5: 85.4167 (90.4564)  time: 3.4640  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:31  loss: 1.5007 (1.2878)  acc1: 66.6667 (71.0548)  acc5: 87.5000 (90.4442)  time: 3.4643  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:56  loss: 1.4275 (1.2933)  acc1: 69.7917 (70.9638)  acc5: 88.5417 (90.3662)  time: 3.4664  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:22  loss: 1.4275 (1.2982)  acc1: 69.7917 (70.8312)  acc5: 89.5833 (90.3456)  time: 3.4682  data: 0.0003  max mem: 16226
Test:  [490/521]  eta: 0:01:47  loss: 1.1406 (1.2938)  acc1: 71.8750 (70.9203)  acc5: 91.6667 (90.4044)  time: 3.4661  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 1.0056 (1.2870)  acc1: 76.0417 (71.0683)  acc5: 95.8333 (90.4815)  time: 3.4660  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:38  loss: 1.1317 (1.2916)  acc1: 75.0000 (70.8965)  acc5: 92.7083 (90.4415)  time: 3.4646  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2291 (1.2827)  acc1: 72.9167 (71.1000)  acc5: 91.6667 (90.5340)  time: 3.4434  data: 0.0001  max mem: 16226
Test: Total time: 0:30:06 (3.4664 s / it)
* Acc@1 71.100 Acc@5 90.534 loss 1.283
Accuracy of the network on the 50000 test images: 71.1%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=3.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 3.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:18  loss: 0.5518 (0.5518)  acc1: 88.5417 (88.5417)  acc5: 97.9167 (97.9167)  time: 5.1031  data: 1.0571  max mem: 16225
Test:  [ 10/521]  eta: 0:30:42  loss: 0.6232 (0.6886)  acc1: 88.5417 (86.5530)  acc5: 96.8750 (96.8750)  time: 3.6055  data: 0.0963  max mem: 16226
Test:  [ 20/521]  eta: 0:29:31  loss: 0.8152 (0.8627)  acc1: 82.2917 (81.3988)  acc5: 95.8333 (95.4861)  time: 3.4569  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:43  loss: 1.0486 (0.9696)  acc1: 73.9583 (78.6290)  acc5: 91.6667 (94.2876)  time: 3.4578  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:28:02  loss: 1.1805 (1.0401)  acc1: 70.8333 (76.3211)  acc5: 91.6667 (93.8008)  time: 3.4573  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:23  loss: 0.7414 (0.9648)  acc1: 86.4583 (78.7786)  acc5: 94.7917 (94.2606)  time: 3.4572  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:46  loss: 0.7159 (0.9545)  acc1: 85.4167 (79.3545)  acc5: 94.7917 (94.1769)  time: 3.4569  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:26:09  loss: 0.8192 (0.9539)  acc1: 82.2917 (79.3867)  acc5: 93.7500 (94.1901)  time: 3.4577  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:33  loss: 0.7236 (0.9293)  acc1: 83.3333 (80.0926)  acc5: 95.8333 (94.4316)  time: 3.4582  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:58  loss: 0.9932 (0.9673)  acc1: 76.0417 (78.8118)  acc5: 94.7917 (94.2308)  time: 3.4594  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:22  loss: 1.2355 (1.0003)  acc1: 66.6667 (77.7021)  acc5: 93.7500 (94.1213)  time: 3.4594  data: 0.0001  max mem: 16226
Test:  [110/521]  eta: 0:23:47  loss: 1.1379 (1.0076)  acc1: 67.7083 (77.3836)  acc5: 93.7500 (94.1723)  time: 3.4581  data: 0.0004  max mem: 16226
Test:  [120/521]  eta: 0:23:12  loss: 1.0233 (1.0145)  acc1: 72.9167 (77.2297)  acc5: 93.7500 (94.0771)  time: 3.4588  data: 0.0004  max mem: 16226
Test:  [130/521]  eta: 0:22:36  loss: 1.0820 (1.0230)  acc1: 71.8750 (76.6380)  acc5: 94.7917 (94.1714)  time: 3.4579  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:22:01  loss: 1.0141 (1.0156)  acc1: 77.0833 (76.7804)  acc5: 95.8333 (94.2819)  time: 3.4571  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:27  loss: 0.9683 (1.0276)  acc1: 77.0833 (76.3935)  acc5: 94.7917 (94.1984)  time: 3.4599  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:52  loss: 0.9976 (1.0185)  acc1: 73.9583 (76.7534)  acc5: 94.7917 (94.2805)  time: 3.4606  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:17  loss: 0.7971 (1.0118)  acc1: 83.3333 (76.8945)  acc5: 95.8333 (94.3409)  time: 3.4590  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:42  loss: 0.8825 (1.0054)  acc1: 82.2917 (77.1582)  acc5: 94.7917 (94.3946)  time: 3.4607  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:19:07  loss: 0.8979 (1.0013)  acc1: 80.2083 (77.2142)  acc5: 94.7917 (94.4426)  time: 3.4603  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:32  loss: 0.9552 (1.0089)  acc1: 78.1250 (77.0315)  acc5: 93.7500 (94.3615)  time: 3.4578  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:57  loss: 1.0048 (1.0077)  acc1: 77.0833 (77.0488)  acc5: 93.7500 (94.3128)  time: 3.4570  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:23  loss: 1.1259 (1.0316)  acc1: 68.7500 (76.5036)  acc5: 91.6667 (93.9951)  time: 3.4573  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:48  loss: 1.3120 (1.0479)  acc1: 66.6667 (76.0913)  acc5: 89.5833 (93.7771)  time: 3.4583  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:13  loss: 1.4033 (1.0662)  acc1: 63.5417 (75.6224)  acc5: 88.5417 (93.5123)  time: 3.4575  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:38  loss: 1.5065 (1.0895)  acc1: 63.5417 (75.1743)  acc5: 86.4583 (93.1607)  time: 3.4565  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:15:04  loss: 1.5826 (1.1095)  acc1: 63.5417 (74.7126)  acc5: 84.3750 (92.9159)  time: 3.4594  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:29  loss: 1.4845 (1.1305)  acc1: 63.5417 (74.1967)  acc5: 86.4583 (92.6737)  time: 3.4596  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:54  loss: 1.4812 (1.1421)  acc1: 64.5833 (73.9546)  acc5: 88.5417 (92.5267)  time: 3.4567  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:20  loss: 1.4312 (1.1522)  acc1: 68.7500 (73.7722)  acc5: 88.5417 (92.3790)  time: 3.4566  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:45  loss: 1.1032 (1.1494)  acc1: 71.8750 (73.9445)  acc5: 90.6250 (92.3657)  time: 3.4576  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:10  loss: 1.1825 (1.1655)  acc1: 70.8333 (73.6334)  acc5: 90.6250 (92.1657)  time: 3.4585  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:36  loss: 1.3418 (1.1698)  acc1: 69.7917 (73.6436)  acc5: 88.5417 (92.0658)  time: 3.4592  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:11:01  loss: 1.3418 (1.1892)  acc1: 69.7917 (73.1747)  acc5: 89.5833 (91.8209)  time: 3.4602  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:26  loss: 1.5960 (1.1994)  acc1: 62.5000 (72.9014)  acc5: 86.4583 (91.6667)  time: 3.4587  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:52  loss: 1.5960 (1.2086)  acc1: 64.5833 (72.6258)  acc5: 86.4583 (91.6014)  time: 3.4570  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:17  loss: 1.5043 (1.2210)  acc1: 63.5417 (72.3598)  acc5: 87.5000 (91.4272)  time: 3.4592  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:42  loss: 1.4950 (1.2284)  acc1: 66.6667 (72.2204)  acc5: 87.5000 (91.3691)  time: 3.4609  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:08  loss: 1.3693 (1.2335)  acc1: 69.7917 (72.2031)  acc5: 90.6250 (91.2566)  time: 3.4591  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:33  loss: 1.4796 (1.2443)  acc1: 65.6250 (71.8990)  acc5: 87.5000 (91.0912)  time: 3.4571  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:58  loss: 1.5111 (1.2503)  acc1: 65.6250 (71.8230)  acc5: 85.4167 (91.0069)  time: 3.4576  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:24  loss: 1.4652 (1.2571)  acc1: 68.7500 (71.7027)  acc5: 86.4583 (90.8835)  time: 3.4580  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:49  loss: 1.4652 (1.2610)  acc1: 68.7500 (71.6969)  acc5: 86.4583 (90.8081)  time: 3.4606  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:15  loss: 1.5083 (1.2688)  acc1: 66.6667 (71.4883)  acc5: 87.5000 (90.7289)  time: 3.4630  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:40  loss: 1.7408 (1.2813)  acc1: 58.3333 (71.1664)  acc5: 85.4167 (90.5400)  time: 3.4605  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:05  loss: 1.5945 (1.2856)  acc1: 64.5833 (71.0574)  acc5: 87.5000 (90.4934)  time: 3.4600  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:31  loss: 1.4332 (1.2889)  acc1: 65.6250 (70.9373)  acc5: 89.5833 (90.4601)  time: 3.4605  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:56  loss: 1.4270 (1.2948)  acc1: 66.6667 (70.8090)  acc5: 88.5417 (90.3773)  time: 3.4579  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:21  loss: 1.4270 (1.2994)  acc1: 69.7917 (70.6623)  acc5: 88.5417 (90.3478)  time: 3.4557  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:47  loss: 1.1251 (1.2946)  acc1: 72.9167 (70.7548)  acc5: 92.7083 (90.4256)  time: 3.4561  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 0.9900 (1.2876)  acc1: 78.1250 (70.9019)  acc5: 94.7917 (90.4961)  time: 3.4570  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:38  loss: 1.0754 (1.2928)  acc1: 73.9583 (70.7314)  acc5: 91.6667 (90.4680)  time: 3.4566  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2185 (1.2839)  acc1: 73.9583 (70.9420)  acc5: 92.7083 (90.5620)  time: 3.4360  data: 0.0001  max mem: 16226
Test: Total time: 0:30:03 (3.4609 s / it)
* Acc@1 70.942 Acc@5 90.562 loss 1.284
Accuracy of the network on the 50000 test images: 70.9%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=4.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 4.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:45:00  loss: 0.5639 (0.5639)  acc1: 89.5833 (89.5833)  acc5: 95.8333 (95.8333)  time: 5.1836  data: 1.1240  max mem: 16225
Test:  [ 10/521]  eta: 0:30:46  loss: 0.6114 (0.6855)  acc1: 89.5833 (86.9318)  acc5: 96.8750 (96.6856)  time: 3.6143  data: 0.1024  max mem: 16226
Test:  [ 20/521]  eta: 0:29:33  loss: 0.8521 (0.8693)  acc1: 81.2500 (81.4484)  acc5: 96.8750 (95.5357)  time: 3.4588  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:45  loss: 1.0643 (0.9773)  acc1: 76.0417 (78.8979)  acc5: 92.7083 (94.2204)  time: 3.4603  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:28:04  loss: 1.2358 (1.0462)  acc1: 71.8750 (76.4482)  acc5: 91.6667 (93.8516)  time: 3.4606  data: 0.0003  max mem: 16226
Test:  [ 50/521]  eta: 0:27:25  loss: 0.7561 (0.9735)  acc1: 87.5000 (78.8603)  acc5: 94.7917 (94.2198)  time: 3.4601  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:47  loss: 0.7511 (0.9637)  acc1: 85.4167 (79.4057)  acc5: 94.7917 (94.2281)  time: 3.4599  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:26:11  loss: 0.8036 (0.9621)  acc1: 83.3333 (79.5335)  acc5: 95.8333 (94.2342)  time: 3.4595  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:35  loss: 0.7203 (0.9346)  acc1: 84.3750 (80.2726)  acc5: 95.8333 (94.4959)  time: 3.4611  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:59  loss: 1.0153 (0.9725)  acc1: 76.0417 (78.9606)  acc5: 95.8333 (94.3223)  time: 3.4633  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:23  loss: 1.1820 (1.0035)  acc1: 67.7083 (77.8981)  acc5: 93.7500 (94.2554)  time: 3.4612  data: 0.0003  max mem: 16226
Test:  [110/521]  eta: 0:23:48  loss: 1.1150 (1.0106)  acc1: 67.7083 (77.5432)  acc5: 94.7917 (94.3412)  time: 3.4610  data: 0.0003  max mem: 16226
Test:  [120/521]  eta: 0:23:13  loss: 1.0363 (1.0175)  acc1: 76.0417 (77.3846)  acc5: 94.7917 (94.2407)  time: 3.4633  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:38  loss: 1.1112 (1.0253)  acc1: 73.9583 (76.7573)  acc5: 94.7917 (94.2907)  time: 3.4615  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:22:03  loss: 1.0008 (1.0181)  acc1: 76.0417 (76.8765)  acc5: 94.7917 (94.3780)  time: 3.4600  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:28  loss: 0.9667 (1.0301)  acc1: 78.1250 (76.4763)  acc5: 94.7917 (94.3019)  time: 3.4595  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:53  loss: 1.0493 (1.0211)  acc1: 75.0000 (76.7534)  acc5: 94.7917 (94.3582)  time: 3.4586  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:20:18  loss: 0.7870 (1.0129)  acc1: 81.2500 (76.9554)  acc5: 94.7917 (94.3957)  time: 3.4587  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:43  loss: 0.8599 (1.0060)  acc1: 80.2083 (77.1984)  acc5: 94.7917 (94.4233)  time: 3.4585  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:19:08  loss: 0.8902 (1.0027)  acc1: 79.1667 (77.2469)  acc5: 94.7917 (94.4699)  time: 3.4589  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:33  loss: 0.9735 (1.0103)  acc1: 77.0833 (77.0574)  acc5: 93.7500 (94.3926)  time: 3.4606  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:58  loss: 1.0324 (1.0095)  acc1: 76.0417 (77.0685)  acc5: 92.7083 (94.3325)  time: 3.4620  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:23  loss: 1.1375 (1.0324)  acc1: 68.7500 (76.5319)  acc5: 90.6250 (94.0564)  time: 3.4634  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:49  loss: 1.3403 (1.0492)  acc1: 66.6667 (76.0868)  acc5: 89.5833 (93.8131)  time: 3.4652  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:16:14  loss: 1.3555 (1.0684)  acc1: 63.5417 (75.5705)  acc5: 88.5417 (93.5728)  time: 3.4650  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:39  loss: 1.5357 (1.0910)  acc1: 63.5417 (75.0996)  acc5: 87.5000 (93.2229)  time: 3.4636  data: 0.0003  max mem: 16226
Test:  [260/521]  eta: 0:15:05  loss: 1.5689 (1.1114)  acc1: 62.5000 (74.6209)  acc5: 85.4167 (92.9757)  time: 3.4621  data: 0.0003  max mem: 16226
Test:  [270/521]  eta: 0:14:30  loss: 1.4840 (1.1321)  acc1: 62.5000 (74.1082)  acc5: 87.5000 (92.7583)  time: 3.4611  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:55  loss: 1.4792 (1.1433)  acc1: 63.5417 (73.8842)  acc5: 87.5000 (92.6045)  time: 3.4601  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:20  loss: 1.4745 (1.1528)  acc1: 65.6250 (73.7042)  acc5: 87.5000 (92.4721)  time: 3.4594  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:46  loss: 1.1140 (1.1499)  acc1: 73.9583 (73.8718)  acc5: 91.6667 (92.4419)  time: 3.4596  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:11  loss: 1.1294 (1.1658)  acc1: 73.9583 (73.5330)  acc5: 89.5833 (92.2461)  time: 3.4603  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:36  loss: 1.3210 (1.1702)  acc1: 67.7083 (73.5138)  acc5: 88.5417 (92.1372)  time: 3.4606  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:11:02  loss: 1.3210 (1.1895)  acc1: 67.7083 (73.0457)  acc5: 89.5833 (91.8901)  time: 3.4603  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:27  loss: 1.5969 (1.1997)  acc1: 61.4583 (72.7700)  acc5: 88.5417 (91.7369)  time: 3.4605  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:52  loss: 1.5849 (1.2088)  acc1: 61.4583 (72.5101)  acc5: 88.5417 (91.6845)  time: 3.4622  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:17  loss: 1.5121 (1.2208)  acc1: 62.5000 (72.2501)  acc5: 87.5000 (91.5022)  time: 3.4642  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:43  loss: 1.5040 (1.2282)  acc1: 64.5833 (72.0996)  acc5: 88.5417 (91.4224)  time: 3.4635  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:08:08  loss: 1.3452 (1.2334)  acc1: 70.8333 (72.1047)  acc5: 89.5833 (91.3030)  time: 3.4618  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:33  loss: 1.4282 (1.2444)  acc1: 66.6667 (71.7738)  acc5: 87.5000 (91.1418)  time: 3.4628  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:59  loss: 1.5436 (1.2500)  acc1: 65.6250 (71.7217)  acc5: 86.4583 (91.0484)  time: 3.4632  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:24  loss: 1.4734 (1.2569)  acc1: 66.6667 (71.5911)  acc5: 86.4583 (90.9139)  time: 3.4643  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:50  loss: 1.4635 (1.2608)  acc1: 67.7083 (71.5632)  acc5: 86.4583 (90.8650)  time: 3.4638  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:15  loss: 1.4635 (1.2686)  acc1: 67.7083 (71.3965)  acc5: 86.4583 (90.7773)  time: 3.4644  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:40  loss: 1.7424 (1.2812)  acc1: 60.4167 (71.1050)  acc5: 84.3750 (90.5990)  time: 3.4649  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:06  loss: 1.5759 (1.2856)  acc1: 63.5417 (70.9904)  acc5: 86.4583 (90.5419)  time: 3.4624  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:31  loss: 1.4501 (1.2886)  acc1: 63.5417 (70.8537)  acc5: 88.5417 (90.5188)  time: 3.4628  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:56  loss: 1.4150 (1.2939)  acc1: 68.7500 (70.7714)  acc5: 88.5417 (90.4414)  time: 3.4623  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:22  loss: 1.4150 (1.2988)  acc1: 69.7917 (70.6276)  acc5: 89.5833 (90.4193)  time: 3.4625  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:47  loss: 1.1403 (1.2941)  acc1: 69.7917 (70.6848)  acc5: 91.6667 (90.5041)  time: 3.4621  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 0.9925 (1.2870)  acc1: 75.0000 (70.8624)  acc5: 95.8333 (90.5751)  time: 3.4640  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:38  loss: 1.1178 (1.2919)  acc1: 73.9583 (70.7131)  acc5: 93.7500 (90.5353)  time: 3.4640  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2241 (1.2829)  acc1: 69.7917 (70.9340)  acc5: 91.6667 (90.6240)  time: 3.4393  data: 0.0001  max mem: 16226
Test: Total time: 0:30:04 (3.4642 s / it)
* Acc@1 70.934 Acc@5 90.624 loss 1.283
Accuracy of the network on the 50000 test images: 70.9%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=5.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 5.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:45:39  loss: 0.5633 (0.5633)  acc1: 88.5417 (88.5417)  acc5: 97.9167 (97.9167)  time: 5.2579  data: 1.2472  max mem: 16225
Test:  [ 10/521]  eta: 0:30:54  loss: 0.5789 (0.6790)  acc1: 88.5417 (86.6477)  acc5: 97.9167 (96.9697)  time: 3.6284  data: 0.1136  max mem: 16226
Test:  [ 20/521]  eta: 0:29:38  loss: 0.8667 (0.8624)  acc1: 81.2500 (81.5476)  acc5: 96.8750 (95.5357)  time: 3.4648  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:49  loss: 1.0456 (0.9682)  acc1: 72.9167 (79.0659)  acc5: 91.6667 (94.1196)  time: 3.4639  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:28:07  loss: 1.2121 (1.0377)  acc1: 72.9167 (76.5752)  acc5: 92.7083 (93.8516)  time: 3.4648  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:28  loss: 0.7419 (0.9687)  acc1: 85.4167 (78.9216)  acc5: 95.8333 (94.3015)  time: 3.4656  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:50  loss: 0.7399 (0.9563)  acc1: 86.4583 (79.6790)  acc5: 95.8333 (94.2965)  time: 3.4650  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:26:14  loss: 0.8391 (0.9583)  acc1: 83.3333 (79.5628)  acc5: 93.7500 (94.2195)  time: 3.4674  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:38  loss: 0.7201 (0.9328)  acc1: 82.2917 (80.1698)  acc5: 95.8333 (94.5087)  time: 3.4698  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:25:02  loss: 0.9824 (0.9725)  acc1: 73.9583 (78.8805)  acc5: 93.7500 (94.2651)  time: 3.4678  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:24:26  loss: 1.2426 (1.0045)  acc1: 64.5833 (77.7847)  acc5: 92.7083 (94.0800)  time: 3.4666  data: 0.0001  max mem: 16226
Test:  [110/521]  eta: 0:23:51  loss: 1.0997 (1.0118)  acc1: 70.8333 (77.4212)  acc5: 93.7500 (94.1441)  time: 3.4669  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:23:15  loss: 1.0412 (1.0182)  acc1: 75.0000 (77.2211)  acc5: 93.7500 (94.0599)  time: 3.4664  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:40  loss: 1.1112 (1.0260)  acc1: 72.9167 (76.6619)  acc5: 94.7917 (94.1396)  time: 3.4690  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:22:05  loss: 1.0027 (1.0186)  acc1: 77.0833 (76.8322)  acc5: 95.8333 (94.2376)  time: 3.4701  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:30  loss: 0.9626 (1.0301)  acc1: 78.1250 (76.4625)  acc5: 94.7917 (94.1846)  time: 3.4685  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:55  loss: 1.0871 (1.0208)  acc1: 78.1250 (76.7728)  acc5: 94.7917 (94.2611)  time: 3.4674  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:20  loss: 0.8192 (1.0135)  acc1: 83.3333 (76.9189)  acc5: 95.8333 (94.3226)  time: 3.4661  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:45  loss: 0.8539 (1.0058)  acc1: 81.2500 (77.1697)  acc5: 94.7917 (94.3485)  time: 3.4673  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:19:10  loss: 0.8759 (1.0031)  acc1: 81.2500 (77.2142)  acc5: 94.7917 (94.3608)  time: 3.4673  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:35  loss: 0.9781 (1.0100)  acc1: 78.1250 (77.0833)  acc5: 93.7500 (94.2890)  time: 3.4680  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:18:00  loss: 0.9970 (1.0092)  acc1: 77.0833 (77.0784)  acc5: 93.7500 (94.2042)  time: 3.4695  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:26  loss: 1.1188 (1.0321)  acc1: 68.7500 (76.5224)  acc5: 91.6667 (93.9197)  time: 3.4691  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:51  loss: 1.3234 (1.0488)  acc1: 66.6667 (76.0732)  acc5: 89.5833 (93.6824)  time: 3.4692  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:16:16  loss: 1.3503 (1.0669)  acc1: 65.6250 (75.6613)  acc5: 89.5833 (93.4474)  time: 3.4719  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:41  loss: 1.4988 (1.0893)  acc1: 64.5833 (75.2407)  acc5: 86.4583 (93.1067)  time: 3.4707  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:15:06  loss: 1.6423 (1.1108)  acc1: 61.4583 (74.7366)  acc5: 84.3750 (92.8360)  time: 3.4693  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:32  loss: 1.5174 (1.1320)  acc1: 62.5000 (74.2236)  acc5: 87.5000 (92.5892)  time: 3.4698  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:57  loss: 1.4687 (1.1434)  acc1: 64.5833 (74.0065)  acc5: 87.5000 (92.4526)  time: 3.4692  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:22  loss: 1.4478 (1.1527)  acc1: 69.7917 (73.8295)  acc5: 87.5000 (92.3182)  time: 3.4710  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:47  loss: 1.0632 (1.1498)  acc1: 72.9167 (73.9687)  acc5: 90.6250 (92.2861)  time: 3.4711  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:12:13  loss: 1.1426 (1.1656)  acc1: 71.8750 (73.6267)  acc5: 90.6250 (92.1054)  time: 3.4714  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:38  loss: 1.3687 (1.1702)  acc1: 67.7083 (73.5981)  acc5: 88.5417 (91.9944)  time: 3.4715  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:11:03  loss: 1.3687 (1.1896)  acc1: 67.7083 (73.1401)  acc5: 89.5833 (91.7391)  time: 3.4693  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:28  loss: 1.6149 (1.2004)  acc1: 61.4583 (72.8892)  acc5: 86.4583 (91.5842)  time: 3.4684  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:53  loss: 1.5906 (1.2097)  acc1: 62.5000 (72.6169)  acc5: 87.5000 (91.5213)  time: 3.4706  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:19  loss: 1.5318 (1.2219)  acc1: 62.5000 (72.3338)  acc5: 87.5000 (91.3579)  time: 3.4711  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:44  loss: 1.5378 (1.2294)  acc1: 63.5417 (72.1839)  acc5: 88.5417 (91.2960)  time: 3.4710  data: 0.0003  max mem: 16226
Test:  [380/521]  eta: 0:08:09  loss: 1.3419 (1.2347)  acc1: 67.7083 (72.1566)  acc5: 90.6250 (91.1964)  time: 3.4714  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:34  loss: 1.4673 (1.2457)  acc1: 65.6250 (71.8350)  acc5: 88.5417 (91.0086)  time: 3.4693  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:07:00  loss: 1.5438 (1.2516)  acc1: 66.6667 (71.7815)  acc5: 86.4583 (90.9185)  time: 3.4688  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:25  loss: 1.4380 (1.2585)  acc1: 69.7917 (71.6722)  acc5: 86.4583 (90.7897)  time: 3.4717  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:50  loss: 1.4487 (1.2628)  acc1: 67.7083 (71.6325)  acc5: 85.4167 (90.7215)  time: 3.4713  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:16  loss: 1.5207 (1.2703)  acc1: 65.6250 (71.4351)  acc5: 87.5000 (90.6516)  time: 3.4702  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:41  loss: 1.6966 (1.2831)  acc1: 61.4583 (71.1617)  acc5: 85.4167 (90.4620)  time: 3.4708  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:06  loss: 1.5719 (1.2874)  acc1: 64.5833 (71.0458)  acc5: 87.5000 (90.4241)  time: 3.4718  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:31  loss: 1.4627 (1.2906)  acc1: 67.7083 (70.9282)  acc5: 89.5833 (90.3877)  time: 3.4711  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:57  loss: 1.3909 (1.2960)  acc1: 67.7083 (70.8201)  acc5: 88.5417 (90.3198)  time: 3.4713  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:22  loss: 1.3909 (1.3006)  acc1: 68.7500 (70.6623)  acc5: 89.5833 (90.2980)  time: 3.4713  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:47  loss: 1.1638 (1.2962)  acc1: 69.7917 (70.7315)  acc5: 91.6667 (90.3747)  time: 3.4685  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 1.0218 (1.2896)  acc1: 75.0000 (70.8728)  acc5: 94.7917 (90.4483)  time: 3.4707  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:38  loss: 1.1608 (1.2944)  acc1: 75.0000 (70.7069)  acc5: 91.6667 (90.4069)  time: 3.4702  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2179 (1.2855)  acc1: 72.9167 (70.9380)  acc5: 91.6667 (90.4900)  time: 3.4452  data: 0.0001  max mem: 16226
Test: Total time: 0:30:08 (3.4718 s / it)
* Acc@1 70.938 Acc@5 90.490 loss 1.285
Accuracy of the network on the 50000 test images: 70.9%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=6.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 6.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:42  loss: 0.5339 (0.5339)  acc1: 88.5417 (88.5417)  acc5: 97.9167 (97.9167)  time: 5.1492  data: 1.0860  max mem: 16225
Test:  [ 10/521]  eta: 0:30:49  loss: 0.6144 (0.6814)  acc1: 88.5417 (86.2689)  acc5: 97.9167 (96.9697)  time: 3.6194  data: 0.0989  max mem: 16226
Test:  [ 20/521]  eta: 0:29:36  loss: 0.8250 (0.8584)  acc1: 82.2917 (81.4484)  acc5: 96.8750 (95.4861)  time: 3.4662  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:48  loss: 1.0447 (0.9652)  acc1: 73.9583 (78.7634)  acc5: 92.7083 (94.1868)  time: 3.4665  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:28:07  loss: 1.2090 (1.0400)  acc1: 71.8750 (76.2957)  acc5: 91.6667 (93.5976)  time: 3.4701  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:29  loss: 0.7634 (0.9680)  acc1: 85.4167 (78.8194)  acc5: 95.8333 (94.0768)  time: 3.4712  data: 0.0003  max mem: 16226
Test:  [ 60/521]  eta: 0:26:51  loss: 0.7483 (0.9568)  acc1: 85.4167 (79.4911)  acc5: 95.8333 (94.0745)  time: 3.4696  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:26:15  loss: 0.8404 (0.9560)  acc1: 84.3750 (79.6215)  acc5: 93.7500 (94.0728)  time: 3.4705  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:38  loss: 0.7092 (0.9317)  acc1: 85.4167 (80.3241)  acc5: 95.8333 (94.2901)  time: 3.4696  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:25:03  loss: 0.9709 (0.9719)  acc1: 77.0833 (79.0179)  acc5: 94.7917 (94.0705)  time: 3.4682  data: 0.0004  max mem: 16226
Test:  [100/521]  eta: 0:24:27  loss: 1.1983 (1.0038)  acc1: 67.7083 (77.9909)  acc5: 92.7083 (93.9563)  time: 3.4701  data: 0.0004  max mem: 16226
Test:  [110/521]  eta: 0:23:51  loss: 1.1241 (1.0096)  acc1: 70.8333 (77.7027)  acc5: 93.7500 (94.0972)  time: 3.4695  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:23:16  loss: 1.0377 (1.0168)  acc1: 75.0000 (77.5568)  acc5: 94.7917 (93.9824)  time: 3.4672  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:41  loss: 1.0993 (1.0234)  acc1: 72.9167 (76.9720)  acc5: 94.7917 (94.0601)  time: 3.4689  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:22:06  loss: 0.9901 (1.0163)  acc1: 75.0000 (77.1055)  acc5: 94.7917 (94.1563)  time: 3.4696  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:31  loss: 0.9383 (1.0274)  acc1: 77.0833 (76.6832)  acc5: 94.7917 (94.1294)  time: 3.4679  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:55  loss: 1.0617 (1.0192)  acc1: 77.0833 (76.9669)  acc5: 94.7917 (94.2158)  time: 3.4676  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:20:20  loss: 0.8348 (1.0127)  acc1: 81.2500 (77.0955)  acc5: 94.7917 (94.2739)  time: 3.4688  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:45  loss: 0.9115 (1.0063)  acc1: 81.2500 (77.3366)  acc5: 94.7917 (94.3082)  time: 3.4678  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:19:11  loss: 0.9115 (1.0033)  acc1: 81.2500 (77.3778)  acc5: 94.7917 (94.3226)  time: 3.4671  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:36  loss: 0.9882 (1.0101)  acc1: 77.0833 (77.2025)  acc5: 93.7500 (94.2527)  time: 3.4683  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:18:01  loss: 0.9940 (1.0082)  acc1: 77.0833 (77.1969)  acc5: 92.7083 (94.1993)  time: 3.4669  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:26  loss: 1.1147 (1.0316)  acc1: 68.7500 (76.6591)  acc5: 91.6667 (93.9197)  time: 3.4674  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:51  loss: 1.3137 (1.0474)  acc1: 66.6667 (76.2220)  acc5: 90.6250 (93.7229)  time: 3.4704  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:16:16  loss: 1.3515 (1.0661)  acc1: 65.6250 (75.7391)  acc5: 90.6250 (93.4907)  time: 3.4694  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:41  loss: 1.5338 (1.0887)  acc1: 62.5000 (75.3237)  acc5: 88.5417 (93.1399)  time: 3.4672  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:15:06  loss: 1.6402 (1.1101)  acc1: 59.3750 (74.7965)  acc5: 85.4167 (92.8760)  time: 3.4670  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:32  loss: 1.5012 (1.1312)  acc1: 61.4583 (74.2543)  acc5: 86.4583 (92.6353)  time: 3.4668  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:57  loss: 1.4827 (1.1422)  acc1: 64.5833 (73.9991)  acc5: 87.5000 (92.4933)  time: 3.4668  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:22  loss: 1.4827 (1.1522)  acc1: 68.7500 (73.8116)  acc5: 87.5000 (92.3074)  time: 3.4677  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:47  loss: 1.0969 (1.1491)  acc1: 73.9583 (73.9653)  acc5: 89.5833 (92.2965)  time: 3.4670  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:12  loss: 1.0969 (1.1649)  acc1: 71.8750 (73.6033)  acc5: 91.6667 (92.0987)  time: 3.4675  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:38  loss: 1.3481 (1.1691)  acc1: 68.7500 (73.5884)  acc5: 88.5417 (91.9814)  time: 3.4680  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:11:03  loss: 1.3248 (1.1884)  acc1: 68.7500 (73.1338)  acc5: 90.6250 (91.7516)  time: 3.4666  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:28  loss: 1.6015 (1.1989)  acc1: 60.4167 (72.8464)  acc5: 86.4583 (91.6025)  time: 3.4651  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:53  loss: 1.6015 (1.2080)  acc1: 62.5000 (72.5873)  acc5: 86.4583 (91.5509)  time: 3.4687  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:19  loss: 1.5417 (1.2204)  acc1: 63.5417 (72.3194)  acc5: 89.5833 (91.3810)  time: 3.4705  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:44  loss: 1.4893 (1.2280)  acc1: 64.5833 (72.1502)  acc5: 89.5833 (91.3269)  time: 3.4666  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:09  loss: 1.3782 (1.2331)  acc1: 67.7083 (72.1074)  acc5: 90.6250 (91.2292)  time: 3.4664  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:34  loss: 1.4174 (1.2439)  acc1: 65.6250 (71.8324)  acc5: 88.5417 (91.0513)  time: 3.4664  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:07:00  loss: 1.5408 (1.2499)  acc1: 65.6250 (71.7919)  acc5: 86.4583 (90.9549)  time: 3.4657  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:25  loss: 1.4751 (1.2569)  acc1: 66.6667 (71.6722)  acc5: 86.4583 (90.8404)  time: 3.4651  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:50  loss: 1.4751 (1.2612)  acc1: 67.7083 (71.6226)  acc5: 87.5000 (90.7834)  time: 3.4655  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:15  loss: 1.5032 (1.2689)  acc1: 67.7083 (71.4400)  acc5: 89.5833 (90.7289)  time: 3.4657  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:41  loss: 1.7172 (1.2815)  acc1: 60.4167 (71.1262)  acc5: 85.4167 (90.5636)  time: 3.4649  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:06  loss: 1.6303 (1.2863)  acc1: 63.5417 (71.0043)  acc5: 87.5000 (90.5118)  time: 3.4644  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:31  loss: 1.4878 (1.2895)  acc1: 66.6667 (70.8853)  acc5: 87.5000 (90.4872)  time: 3.4632  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:57  loss: 1.4015 (1.2949)  acc1: 68.7500 (70.7626)  acc5: 89.5833 (90.4215)  time: 3.4648  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:22  loss: 1.4015 (1.2994)  acc1: 66.6667 (70.6103)  acc5: 89.5833 (90.3998)  time: 3.4664  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:47  loss: 1.1440 (1.2948)  acc1: 68.7500 (70.6870)  acc5: 92.7083 (90.4659)  time: 3.4671  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 1.0274 (1.2880)  acc1: 77.0833 (70.8437)  acc5: 93.7500 (90.5273)  time: 3.4669  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:38  loss: 1.1072 (1.2927)  acc1: 71.8750 (70.6682)  acc5: 92.7083 (90.4680)  time: 3.4656  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2332 (1.2835)  acc1: 68.7500 (70.8840)  acc5: 90.6250 (90.5540)  time: 3.4428  data: 0.0001  max mem: 16226
Test: Total time: 0:30:07 (3.4699 s / it)
* Acc@1 70.884 Acc@5 90.554 loss 1.283
Accuracy of the network on the 50000 test images: 70.9%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=2, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=7.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
2
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 7.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:31  loss: 0.5739 (0.5739)  acc1: 89.5833 (89.5833)  acc5: 97.9167 (97.9167)  time: 5.1271  data: 1.0845  max mem: 16225
Test:  [ 10/521]  eta: 0:30:46  loss: 0.6042 (0.6862)  acc1: 88.5417 (85.9849)  acc5: 97.9167 (97.1591)  time: 3.6127  data: 0.0987  max mem: 16226
Test:  [ 20/521]  eta: 0:29:34  loss: 0.8490 (0.8615)  acc1: 82.2917 (81.1508)  acc5: 96.8750 (95.5853)  time: 3.4626  data: 0.0001  max mem: 16226
Test:  [ 30/521]  eta: 0:28:47  loss: 1.0555 (0.9747)  acc1: 71.8750 (78.3602)  acc5: 91.6667 (94.1532)  time: 3.4678  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:28:06  loss: 1.2546 (1.0446)  acc1: 68.7500 (75.9909)  acc5: 92.7083 (93.6992)  time: 3.4692  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:27  loss: 0.7385 (0.9729)  acc1: 85.4167 (78.4518)  acc5: 95.8333 (94.0768)  time: 3.4664  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:50  loss: 0.7434 (0.9624)  acc1: 84.3750 (79.1325)  acc5: 95.8333 (94.1086)  time: 3.4665  data: 0.0003  max mem: 16226
Test:  [ 70/521]  eta: 0:26:13  loss: 0.8447 (0.9597)  acc1: 82.2917 (79.3134)  acc5: 94.7917 (94.1168)  time: 3.4661  data: 0.0003  max mem: 16226
Test:  [ 80/521]  eta: 0:25:37  loss: 0.7312 (0.9349)  acc1: 84.3750 (79.9769)  acc5: 95.8333 (94.3287)  time: 3.4658  data: 0.0003  max mem: 16226
Test:  [ 90/521]  eta: 0:25:01  loss: 0.9735 (0.9736)  acc1: 76.0417 (78.7088)  acc5: 93.7500 (94.0820)  time: 3.4654  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:25  loss: 1.2580 (1.0057)  acc1: 66.6667 (77.7331)  acc5: 91.6667 (93.9253)  time: 3.4644  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:50  loss: 1.1446 (1.0125)  acc1: 69.7917 (77.3836)  acc5: 93.7500 (94.0409)  time: 3.4661  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:23:15  loss: 1.0330 (1.0192)  acc1: 76.0417 (77.2986)  acc5: 94.7917 (93.8705)  time: 3.4665  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:40  loss: 1.1269 (1.0271)  acc1: 72.9167 (76.6778)  acc5: 94.7917 (93.9806)  time: 3.4653  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:22:04  loss: 1.0012 (1.0201)  acc1: 76.0417 (76.8174)  acc5: 95.8333 (94.0603)  time: 3.4645  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:29  loss: 0.9482 (1.0308)  acc1: 78.1250 (76.4487)  acc5: 94.7917 (94.0466)  time: 3.4653  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:54  loss: 1.0553 (1.0214)  acc1: 77.0833 (76.7663)  acc5: 95.8333 (94.1576)  time: 3.4675  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:20:19  loss: 0.8222 (1.0147)  acc1: 82.2917 (76.8945)  acc5: 95.8333 (94.2130)  time: 3.4658  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:44  loss: 0.8784 (1.0074)  acc1: 81.2500 (77.1639)  acc5: 94.7917 (94.2564)  time: 3.4634  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:19:09  loss: 0.8784 (1.0037)  acc1: 80.2083 (77.2415)  acc5: 94.7917 (94.2954)  time: 3.4641  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:35  loss: 0.9738 (1.0106)  acc1: 78.1250 (77.0937)  acc5: 93.7500 (94.2268)  time: 3.4645  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:18:00  loss: 1.0170 (1.0094)  acc1: 77.0833 (77.0636)  acc5: 93.7500 (94.1696)  time: 3.4638  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:25  loss: 1.1772 (1.0328)  acc1: 69.7917 (76.5743)  acc5: 90.6250 (93.8678)  time: 3.4665  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:50  loss: 1.3113 (1.0493)  acc1: 67.7083 (76.1003)  acc5: 89.5833 (93.6373)  time: 3.4669  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:15  loss: 1.3496 (1.0678)  acc1: 64.5833 (75.6570)  acc5: 88.5417 (93.3869)  time: 3.4652  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:40  loss: 1.5076 (1.0911)  acc1: 65.6250 (75.1826)  acc5: 86.4583 (93.0403)  time: 3.4663  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:15:06  loss: 1.6100 (1.1118)  acc1: 62.5000 (74.7047)  acc5: 86.4583 (92.7842)  time: 3.4656  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:31  loss: 1.5099 (1.1327)  acc1: 62.5000 (74.1813)  acc5: 87.5000 (92.5546)  time: 3.4669  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:56  loss: 1.5099 (1.1442)  acc1: 64.5833 (73.9472)  acc5: 88.5417 (92.4266)  time: 3.4667  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:21  loss: 1.4625 (1.1538)  acc1: 67.7083 (73.7650)  acc5: 89.5833 (92.2824)  time: 3.4641  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:47  loss: 1.0897 (1.1511)  acc1: 71.8750 (73.9099)  acc5: 90.6250 (92.2619)  time: 3.4642  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:12:12  loss: 1.1735 (1.1669)  acc1: 68.7500 (73.5363)  acc5: 90.6250 (92.0753)  time: 3.4644  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:37  loss: 1.3204 (1.1711)  acc1: 68.7500 (73.5267)  acc5: 89.5833 (91.9652)  time: 3.4648  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:11:02  loss: 1.3204 (1.1894)  acc1: 68.7500 (73.1307)  acc5: 90.6250 (91.7233)  time: 3.4672  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:28  loss: 1.6039 (1.1999)  acc1: 61.4583 (72.9014)  acc5: 86.4583 (91.5720)  time: 3.4688  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:53  loss: 1.6039 (1.2092)  acc1: 61.4583 (72.6288)  acc5: 88.5417 (91.5213)  time: 3.4690  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:18  loss: 1.5678 (1.2215)  acc1: 61.4583 (72.3425)  acc5: 87.5000 (91.3550)  time: 3.4681  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:44  loss: 1.5024 (1.2292)  acc1: 64.5833 (72.1951)  acc5: 87.5000 (91.2792)  time: 3.4665  data: 0.0003  max mem: 16226
Test:  [380/521]  eta: 0:08:09  loss: 1.3472 (1.2344)  acc1: 69.7917 (72.1539)  acc5: 89.5833 (91.1663)  time: 3.4663  data: 0.0004  max mem: 16226
Test:  [390/521]  eta: 0:07:34  loss: 1.4581 (1.2451)  acc1: 63.5417 (71.8670)  acc5: 88.5417 (91.0166)  time: 3.4655  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:59  loss: 1.4929 (1.2504)  acc1: 65.6250 (71.8360)  acc5: 87.5000 (90.9133)  time: 3.4660  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:25  loss: 1.4550 (1.2569)  acc1: 67.7083 (71.7077)  acc5: 86.4583 (90.8049)  time: 3.4656  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:50  loss: 1.4550 (1.2607)  acc1: 68.7500 (71.6969)  acc5: 86.4583 (90.7462)  time: 3.4674  data: 0.0003  max mem: 16226
Test:  [430/521]  eta: 0:05:15  loss: 1.4599 (1.2683)  acc1: 68.7500 (71.5028)  acc5: 87.5000 (90.6733)  time: 3.4681  data: 0.0003  max mem: 16226
Test:  [440/521]  eta: 0:04:41  loss: 1.7073 (1.2810)  acc1: 60.4167 (71.2018)  acc5: 85.4167 (90.5022)  time: 3.4648  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:06  loss: 1.6179 (1.2858)  acc1: 60.4167 (71.0989)  acc5: 86.4583 (90.4472)  time: 3.4632  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:31  loss: 1.4785 (1.2890)  acc1: 65.6250 (70.9825)  acc5: 88.5417 (90.4194)  time: 3.4623  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:56  loss: 1.3605 (1.2943)  acc1: 67.7083 (70.8577)  acc5: 88.5417 (90.3331)  time: 3.4628  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:22  loss: 1.3605 (1.2987)  acc1: 67.7083 (70.7164)  acc5: 89.5833 (90.3153)  time: 3.4634  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:47  loss: 1.1229 (1.2939)  acc1: 70.8333 (70.8142)  acc5: 91.6667 (90.3916)  time: 3.4634  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 1.0164 (1.2872)  acc1: 75.0000 (70.9685)  acc5: 94.7917 (90.4732)  time: 3.4647  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:38  loss: 1.1314 (1.2921)  acc1: 71.8750 (70.8068)  acc5: 92.7083 (90.4212)  time: 3.4647  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2285 (1.2829)  acc1: 70.8333 (71.0220)  acc5: 90.6250 (90.5100)  time: 3.4430  data: 0.0001  max mem: 16226
Test: Total time: 0:30:06 (3.4681 s / it)
* Acc@1 71.022 Acc@5 90.510 loss 1.283
Accuracy of the network on the 50000 test images: 71.0%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=2.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 2.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:26  loss: 0.5646 (0.5646)  acc1: 88.5417 (88.5417)  acc5: 96.8750 (96.8750)  time: 5.1173  data: 1.0679  max mem: 16225
Test:  [ 10/521]  eta: 0:30:48  loss: 0.5872 (0.6746)  acc1: 88.5417 (86.7424)  acc5: 97.9167 (96.7803)  time: 3.6167  data: 0.0974  max mem: 16226
Test:  [ 20/521]  eta: 0:29:36  loss: 0.8329 (0.8635)  acc1: 83.3333 (81.9940)  acc5: 95.8333 (95.3373)  time: 3.4673  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:48  loss: 1.0776 (0.9754)  acc1: 75.0000 (79.0995)  acc5: 91.6667 (94.1868)  time: 3.4675  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:28:07  loss: 1.2185 (1.0424)  acc1: 72.9167 (76.5498)  acc5: 91.6667 (93.8008)  time: 3.4668  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:27:28  loss: 0.7353 (0.9680)  acc1: 86.4583 (78.8807)  acc5: 94.7917 (94.1381)  time: 3.4686  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:50  loss: 0.7282 (0.9581)  acc1: 86.4583 (79.4399)  acc5: 94.7917 (94.0915)  time: 3.4679  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:26:14  loss: 0.8509 (0.9610)  acc1: 83.3333 (79.2840)  acc5: 93.7500 (93.9554)  time: 3.4658  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:38  loss: 0.7410 (0.9352)  acc1: 83.3333 (79.9254)  acc5: 95.8333 (94.2387)  time: 3.4676  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:25:02  loss: 0.9969 (0.9731)  acc1: 77.0833 (78.6859)  acc5: 94.7917 (94.0820)  time: 3.4674  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:24:26  loss: 1.2318 (1.0080)  acc1: 66.6667 (77.4649)  acc5: 92.7083 (93.8944)  time: 3.4669  data: 0.0001  max mem: 16226
Test:  [110/521]  eta: 0:23:51  loss: 1.1523 (1.0145)  acc1: 68.7500 (77.2241)  acc5: 93.7500 (93.9471)  time: 3.4680  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:23:15  loss: 1.0369 (1.0216)  acc1: 76.0417 (77.1006)  acc5: 93.7500 (93.8533)  time: 3.4678  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:40  loss: 1.0986 (1.0289)  acc1: 71.8750 (76.5108)  acc5: 94.7917 (93.9647)  time: 3.4677  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:22:05  loss: 0.9730 (1.0212)  acc1: 78.1250 (76.6770)  acc5: 95.8333 (94.0677)  time: 3.4695  data: 0.0003  max mem: 16226
Test:  [150/521]  eta: 0:21:30  loss: 0.9689 (1.0320)  acc1: 78.1250 (76.3314)  acc5: 94.7917 (94.0673)  time: 3.4692  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:55  loss: 1.0583 (1.0237)  acc1: 77.0833 (76.6046)  acc5: 94.7917 (94.1706)  time: 3.4666  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:20  loss: 0.8458 (1.0158)  acc1: 82.2917 (76.7361)  acc5: 95.8333 (94.2434)  time: 3.4669  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:45  loss: 0.8741 (1.0105)  acc1: 80.2083 (76.9625)  acc5: 94.7917 (94.2795)  time: 3.4703  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:19:10  loss: 0.9071 (1.0084)  acc1: 79.1667 (76.9906)  acc5: 94.7917 (94.3172)  time: 3.4711  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:35  loss: 0.9667 (1.0159)  acc1: 77.0833 (76.8398)  acc5: 93.7500 (94.2527)  time: 3.4679  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:18:00  loss: 0.9990 (1.0146)  acc1: 77.0833 (76.8908)  acc5: 93.7500 (94.2091)  time: 3.4662  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:26  loss: 1.1358 (1.0366)  acc1: 70.8333 (76.3810)  acc5: 92.7083 (93.9385)  time: 3.4660  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:51  loss: 1.3308 (1.0533)  acc1: 68.7500 (75.9560)  acc5: 89.5833 (93.7410)  time: 3.4664  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:16  loss: 1.4315 (1.0717)  acc1: 66.6667 (75.5100)  acc5: 89.5833 (93.4691)  time: 3.4663  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:41  loss: 1.4724 (1.0937)  acc1: 66.6667 (75.1038)  acc5: 87.5000 (93.1316)  time: 3.4675  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:15:06  loss: 1.6191 (1.1144)  acc1: 62.5000 (74.6408)  acc5: 85.4167 (92.8560)  time: 3.4677  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:31  loss: 1.5067 (1.1350)  acc1: 62.5000 (74.1275)  acc5: 86.4583 (92.6046)  time: 3.4676  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:57  loss: 1.4457 (1.1462)  acc1: 64.5833 (73.9139)  acc5: 86.4583 (92.4414)  time: 3.4687  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:22  loss: 1.4457 (1.1557)  acc1: 67.7083 (73.7507)  acc5: 86.4583 (92.2967)  time: 3.4677  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:47  loss: 1.0828 (1.1528)  acc1: 73.9583 (73.8960)  acc5: 90.6250 (92.2861)  time: 3.4676  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:12  loss: 1.1345 (1.1682)  acc1: 69.7917 (73.5330)  acc5: 90.6250 (92.0987)  time: 3.4695  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:38  loss: 1.3890 (1.1727)  acc1: 69.7917 (73.5235)  acc5: 88.5417 (91.9879)  time: 3.4708  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:11:03  loss: 1.3995 (1.1919)  acc1: 69.7917 (73.0457)  acc5: 89.5833 (91.7548)  time: 3.4685  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:28  loss: 1.5673 (1.2018)  acc1: 58.3333 (72.7975)  acc5: 86.4583 (91.6239)  time: 3.4669  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:53  loss: 1.5559 (1.2110)  acc1: 58.3333 (72.5368)  acc5: 88.5417 (91.5539)  time: 3.4681  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:19  loss: 1.5283 (1.2229)  acc1: 62.5000 (72.2790)  acc5: 88.5417 (91.3752)  time: 3.4699  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:44  loss: 1.5463 (1.2304)  acc1: 64.5833 (72.1502)  acc5: 88.5417 (91.3157)  time: 3.4686  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:08:09  loss: 1.3630 (1.2352)  acc1: 71.8750 (72.1429)  acc5: 89.5833 (91.2074)  time: 3.4664  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:34  loss: 1.4508 (1.2461)  acc1: 63.5417 (71.8244)  acc5: 88.5417 (91.0246)  time: 3.4669  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:07:00  loss: 1.5426 (1.2520)  acc1: 65.6250 (71.7477)  acc5: 85.4167 (90.9315)  time: 3.4700  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:25  loss: 1.5272 (1.2586)  acc1: 67.7083 (71.6368)  acc5: 85.4167 (90.8075)  time: 3.4709  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:50  loss: 1.5002 (1.2628)  acc1: 68.7500 (71.6127)  acc5: 86.4583 (90.7314)  time: 3.4663  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:15  loss: 1.5002 (1.2700)  acc1: 67.7083 (71.4376)  acc5: 87.5000 (90.6588)  time: 3.4644  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:41  loss: 1.7491 (1.2822)  acc1: 59.3750 (71.1475)  acc5: 85.4167 (90.4762)  time: 3.4655  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:06  loss: 1.5756 (1.2868)  acc1: 64.5833 (71.0504)  acc5: 86.4583 (90.4218)  time: 3.4678  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:31  loss: 1.4601 (1.2904)  acc1: 64.5833 (70.9079)  acc5: 87.5000 (90.3945)  time: 3.4678  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:57  loss: 1.3953 (1.2962)  acc1: 66.6667 (70.8179)  acc5: 89.5833 (90.3132)  time: 3.4664  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:22  loss: 1.3953 (1.3006)  acc1: 70.8333 (70.6947)  acc5: 89.5833 (90.2763)  time: 3.4664  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:47  loss: 1.1078 (1.2961)  acc1: 71.8750 (70.7633)  acc5: 90.6250 (90.3428)  time: 3.4669  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 0.9970 (1.2898)  acc1: 76.0417 (70.9123)  acc5: 94.7917 (90.4171)  time: 3.4681  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:38  loss: 1.1306 (1.2948)  acc1: 71.8750 (70.7579)  acc5: 92.7083 (90.3641)  time: 3.4677  data: 0.0002  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2418 (1.2860)  acc1: 70.8333 (70.9720)  acc5: 91.6667 (90.4560)  time: 3.4450  data: 0.0001  max mem: 16226
Test: Total time: 0:30:07 (3.4702 s / it)
* Acc@1 70.972 Acc@5 90.456 loss 1.286
Accuracy of the network on the 50000 test images: 71.0%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=3.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 3.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:45:21  loss: 0.5739 (0.5739)  acc1: 89.5833 (89.5833)  acc5: 97.9167 (97.9167)  time: 5.2245  data: 1.1616  max mem: 16225
Test:  [ 10/521]  eta: 0:30:52  loss: 0.6006 (0.6836)  acc1: 89.5833 (86.8371)  acc5: 97.9167 (96.8750)  time: 3.6248  data: 0.1058  max mem: 16226
Test:  [ 20/521]  eta: 0:29:38  loss: 0.8589 (0.8643)  acc1: 83.3333 (82.1429)  acc5: 96.8750 (95.2877)  time: 3.4658  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:48  loss: 1.0756 (0.9747)  acc1: 76.0417 (79.5363)  acc5: 91.6667 (94.0524)  time: 3.4637  data: 0.0002  max mem: 16226
Test:  [ 40/521]  eta: 0:28:06  loss: 1.1912 (1.0476)  acc1: 69.7917 (76.7276)  acc5: 91.6667 (93.7246)  time: 3.4601  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:27:27  loss: 0.7561 (0.9716)  acc1: 87.5000 (79.0850)  acc5: 94.7917 (94.1381)  time: 3.4616  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:49  loss: 0.7372 (0.9590)  acc1: 86.4583 (79.7814)  acc5: 95.8333 (94.2281)  time: 3.4633  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:26:13  loss: 0.8518 (0.9605)  acc1: 82.2917 (79.6655)  acc5: 94.7917 (94.1461)  time: 3.4641  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:37  loss: 0.7248 (0.9352)  acc1: 84.3750 (80.3627)  acc5: 96.8750 (94.4187)  time: 3.4661  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:25:01  loss: 0.9737 (0.9765)  acc1: 77.0833 (78.9377)  acc5: 94.7917 (94.1163)  time: 3.4641  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:25  loss: 1.2241 (1.0096)  acc1: 66.6667 (77.7743)  acc5: 92.7083 (93.9872)  time: 3.4622  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:49  loss: 1.1404 (1.0160)  acc1: 67.7083 (77.5150)  acc5: 93.7500 (94.0972)  time: 3.4616  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:23:14  loss: 1.0133 (1.0225)  acc1: 75.0000 (77.3158)  acc5: 93.7500 (93.9652)  time: 3.4618  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:39  loss: 1.1271 (1.0308)  acc1: 70.8333 (76.6619)  acc5: 93.7500 (94.0760)  time: 3.4613  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:22:04  loss: 1.0101 (1.0220)  acc1: 73.9583 (76.8100)  acc5: 95.8333 (94.2080)  time: 3.4605  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:29  loss: 0.9696 (1.0328)  acc1: 79.1667 (76.4280)  acc5: 95.8333 (94.1846)  time: 3.4630  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:54  loss: 1.0594 (1.0243)  acc1: 77.0833 (76.7275)  acc5: 94.7917 (94.2611)  time: 3.4651  data: 0.0002  max mem: 16226
Test:  [170/521]  eta: 0:20:19  loss: 0.8152 (1.0174)  acc1: 82.2917 (76.8640)  acc5: 94.7917 (94.3104)  time: 3.4664  data: 0.0003  max mem: 16226
Test:  [180/521]  eta: 0:19:44  loss: 0.9023 (1.0113)  acc1: 80.2083 (77.1179)  acc5: 94.7917 (94.3428)  time: 3.4646  data: 0.0003  max mem: 16226
Test:  [190/521]  eta: 0:19:09  loss: 0.9084 (1.0084)  acc1: 79.1667 (77.1161)  acc5: 94.7917 (94.3990)  time: 3.4621  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:34  loss: 0.9578 (1.0155)  acc1: 77.0833 (76.9952)  acc5: 93.7500 (94.3201)  time: 3.4624  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:17:59  loss: 0.9914 (1.0141)  acc1: 78.1250 (77.0093)  acc5: 92.7083 (94.2536)  time: 3.4613  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:17:24  loss: 1.1774 (1.0369)  acc1: 67.7083 (76.4800)  acc5: 90.6250 (94.0045)  time: 3.4606  data: 0.0002  max mem: 16226
Test:  [230/521]  eta: 0:16:49  loss: 1.3462 (1.0528)  acc1: 66.6667 (76.0913)  acc5: 89.5833 (93.7861)  time: 3.4612  data: 0.0002  max mem: 16226
Test:  [240/521]  eta: 0:16:15  loss: 1.3891 (1.0709)  acc1: 65.6250 (75.6483)  acc5: 89.5833 (93.5641)  time: 3.4625  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:40  loss: 1.5097 (1.0927)  acc1: 64.5833 (75.2449)  acc5: 86.4583 (93.1939)  time: 3.4624  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:15:05  loss: 1.5733 (1.1131)  acc1: 63.5417 (74.7486)  acc5: 86.4583 (92.9518)  time: 3.4607  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:30  loss: 1.5283 (1.1340)  acc1: 62.5000 (74.2082)  acc5: 86.4583 (92.6930)  time: 3.4622  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:56  loss: 1.4825 (1.1454)  acc1: 63.5417 (73.9806)  acc5: 87.5000 (92.5489)  time: 3.4652  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:21  loss: 1.4825 (1.1550)  acc1: 66.6667 (73.7758)  acc5: 88.5417 (92.4005)  time: 3.4641  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:46  loss: 1.1030 (1.1520)  acc1: 76.0417 (73.9307)  acc5: 91.6667 (92.3900)  time: 3.4629  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:12:11  loss: 1.1064 (1.1675)  acc1: 73.9583 (73.5999)  acc5: 91.6667 (92.1858)  time: 3.4626  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:37  loss: 1.3255 (1.1719)  acc1: 68.7500 (73.5722)  acc5: 87.5000 (92.0755)  time: 3.4609  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:11:02  loss: 1.3585 (1.1906)  acc1: 69.7917 (73.1307)  acc5: 89.5833 (91.8209)  time: 3.4619  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:10:27  loss: 1.5879 (1.2017)  acc1: 59.3750 (72.8617)  acc5: 88.5417 (91.6728)  time: 3.4617  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:52  loss: 1.5846 (1.2109)  acc1: 60.4167 (72.6169)  acc5: 88.5417 (91.5925)  time: 3.4601  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:18  loss: 1.4984 (1.2229)  acc1: 64.5833 (72.3713)  acc5: 88.5417 (91.4243)  time: 3.4604  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:43  loss: 1.5499 (1.2302)  acc1: 65.6250 (72.2428)  acc5: 89.5833 (91.3775)  time: 3.4608  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:08:08  loss: 1.3796 (1.2353)  acc1: 70.8333 (72.2058)  acc5: 89.5833 (91.2812)  time: 3.4611  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:34  loss: 1.4305 (1.2459)  acc1: 62.5000 (71.8883)  acc5: 87.5000 (91.0939)  time: 3.4639  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:59  loss: 1.5224 (1.2517)  acc1: 63.5417 (71.8205)  acc5: 85.4167 (91.0043)  time: 3.4650  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:24  loss: 1.5224 (1.2586)  acc1: 67.7083 (71.7153)  acc5: 86.4583 (90.8784)  time: 3.4605  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:50  loss: 1.5201 (1.2630)  acc1: 65.6250 (71.6696)  acc5: 86.4583 (90.8081)  time: 3.4579  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:15  loss: 1.5201 (1.2705)  acc1: 65.6250 (71.4593)  acc5: 87.5000 (90.7265)  time: 3.4598  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:40  loss: 1.7156 (1.2832)  acc1: 57.2917 (71.1546)  acc5: 83.3333 (90.5352)  time: 3.4606  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:06  loss: 1.5679 (1.2876)  acc1: 63.5417 (71.0481)  acc5: 85.4167 (90.5026)  time: 3.4613  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:31  loss: 1.4856 (1.2909)  acc1: 65.6250 (70.9305)  acc5: 88.5417 (90.4623)  time: 3.4638  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:56  loss: 1.4243 (1.2969)  acc1: 66.6667 (70.8201)  acc5: 88.5417 (90.3707)  time: 3.4632  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:22  loss: 1.3776 (1.3009)  acc1: 69.7917 (70.7229)  acc5: 89.5833 (90.3565)  time: 3.4619  data: 0.0002  max mem: 16226
Test:  [490/521]  eta: 0:01:47  loss: 1.1162 (1.2960)  acc1: 70.8333 (70.7951)  acc5: 92.7083 (90.4426)  time: 3.4614  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 0.9907 (1.2897)  acc1: 73.9583 (70.9311)  acc5: 94.7917 (90.5065)  time: 3.4611  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:38  loss: 1.1306 (1.2945)  acc1: 71.8750 (70.7681)  acc5: 93.7500 (90.4578)  time: 3.4603  data: 0.0002  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2137 (1.2859)  acc1: 69.7917 (70.9820)  acc5: 93.7500 (90.5460)  time: 3.4364  data: 0.0001  max mem: 16226
Test: Total time: 0:30:05 (3.4649 s / it)
* Acc@1 70.982 Acc@5 90.546 loss 1.286
Accuracy of the network on the 50000 test images: 71.0%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=4.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 4.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:45  loss: 0.5620 (0.5620)  acc1: 88.5417 (88.5417)  acc5: 96.8750 (96.8750)  time: 5.1545  data: 1.1936  max mem: 16225
Test:  [ 10/521]  eta: 0:30:45  loss: 0.5946 (0.6818)  acc1: 89.5833 (87.0265)  acc5: 96.8750 (96.6856)  time: 3.6123  data: 0.1087  max mem: 16226
Test:  [ 20/521]  eta: 0:29:33  loss: 0.8375 (0.8670)  acc1: 83.3333 (81.7956)  acc5: 95.8333 (95.1885)  time: 3.4586  data: 0.0002  max mem: 16226
Test:  [ 30/521]  eta: 0:28:45  loss: 1.0562 (0.9725)  acc1: 76.0417 (79.1667)  acc5: 91.6667 (94.0188)  time: 3.4618  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:28:04  loss: 1.1942 (1.0448)  acc1: 71.8750 (76.4228)  acc5: 91.6667 (93.6230)  time: 3.4617  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:25  loss: 0.7461 (0.9709)  acc1: 86.4583 (78.8603)  acc5: 94.7917 (94.0155)  time: 3.4595  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:47  loss: 0.7188 (0.9613)  acc1: 85.4167 (79.3887)  acc5: 94.7917 (94.0403)  time: 3.4598  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:26:11  loss: 0.8511 (0.9618)  acc1: 83.3333 (79.4014)  acc5: 94.7917 (94.0434)  time: 3.4603  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:35  loss: 0.7028 (0.9341)  acc1: 84.3750 (80.1055)  acc5: 96.8750 (94.3158)  time: 3.4600  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:24:59  loss: 0.9803 (0.9743)  acc1: 77.0833 (78.6859)  acc5: 94.7917 (94.1049)  time: 3.4589  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:24:23  loss: 1.2222 (1.0081)  acc1: 64.5833 (77.5474)  acc5: 92.7083 (93.9769)  time: 3.4596  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:48  loss: 1.1915 (1.0148)  acc1: 69.7917 (77.2898)  acc5: 93.7500 (94.0315)  time: 3.4600  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:23:13  loss: 1.0263 (1.0205)  acc1: 76.0417 (77.1608)  acc5: 93.7500 (93.9222)  time: 3.4601  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:37  loss: 1.1218 (1.0291)  acc1: 73.9583 (76.5267)  acc5: 93.7500 (94.0283)  time: 3.4604  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:22:02  loss: 0.9927 (1.0206)  acc1: 76.0417 (76.6770)  acc5: 95.8333 (94.1268)  time: 3.4601  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:27  loss: 0.9705 (1.0309)  acc1: 79.1667 (76.2969)  acc5: 94.7917 (94.1225)  time: 3.4627  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:53  loss: 1.0624 (1.0225)  acc1: 76.0417 (76.5657)  acc5: 94.7917 (94.2223)  time: 3.4650  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:18  loss: 0.8145 (1.0147)  acc1: 82.2917 (76.7422)  acc5: 94.7917 (94.3043)  time: 3.4633  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:43  loss: 0.8772 (1.0080)  acc1: 81.2500 (77.0028)  acc5: 94.7917 (94.3370)  time: 3.4613  data: 0.0002  max mem: 16226
Test:  [190/521]  eta: 0:19:08  loss: 0.9141 (1.0057)  acc1: 80.2083 (77.0179)  acc5: 95.8333 (94.4154)  time: 3.4624  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:33  loss: 0.9690 (1.0127)  acc1: 76.0417 (76.8449)  acc5: 93.7500 (94.3408)  time: 3.4615  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:58  loss: 1.0028 (1.0117)  acc1: 76.0417 (76.8908)  acc5: 92.7083 (94.2634)  time: 3.4588  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:24  loss: 1.1803 (1.0352)  acc1: 69.7917 (76.3999)  acc5: 91.6667 (93.9668)  time: 3.4610  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:49  loss: 1.3198 (1.0510)  acc1: 65.6250 (76.0056)  acc5: 89.5833 (93.7500)  time: 3.4635  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:14  loss: 1.3491 (1.0695)  acc1: 66.6667 (75.5446)  acc5: 89.5833 (93.5123)  time: 3.4642  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:39  loss: 1.5044 (1.0914)  acc1: 66.6667 (75.1577)  acc5: 88.5417 (93.1731)  time: 3.4628  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:15:05  loss: 1.5366 (1.1115)  acc1: 62.5000 (74.7007)  acc5: 84.3750 (92.9159)  time: 3.4607  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:30  loss: 1.5177 (1.1326)  acc1: 62.5000 (74.2005)  acc5: 86.4583 (92.6545)  time: 3.4601  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:13:55  loss: 1.4774 (1.1435)  acc1: 64.5833 (73.9769)  acc5: 87.5000 (92.5304)  time: 3.4602  data: 0.0002  max mem: 16226
Test:  [290/521]  eta: 0:13:20  loss: 1.4675 (1.1532)  acc1: 67.7083 (73.7829)  acc5: 87.5000 (92.3862)  time: 3.4624  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:46  loss: 1.0968 (1.1505)  acc1: 76.0417 (73.9272)  acc5: 91.6667 (92.3623)  time: 3.4620  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:11  loss: 1.1528 (1.1660)  acc1: 71.8750 (73.6167)  acc5: 90.6250 (92.1691)  time: 3.4615  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:36  loss: 1.3364 (1.1705)  acc1: 66.6667 (73.5949)  acc5: 89.5833 (92.0691)  time: 3.4615  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:11:02  loss: 1.3507 (1.1893)  acc1: 68.7500 (73.1653)  acc5: 89.5833 (91.8146)  time: 3.4595  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:27  loss: 1.5915 (1.2004)  acc1: 59.3750 (72.8892)  acc5: 86.4583 (91.6789)  time: 3.4620  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:09:52  loss: 1.5912 (1.2093)  acc1: 61.4583 (72.6941)  acc5: 86.4583 (91.6162)  time: 3.4629  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:17  loss: 1.4786 (1.2214)  acc1: 65.6250 (72.4636)  acc5: 87.5000 (91.4301)  time: 3.4602  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:43  loss: 1.5332 (1.2291)  acc1: 67.7083 (72.3299)  acc5: 87.5000 (91.3494)  time: 3.4615  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:08:08  loss: 1.4064 (1.2342)  acc1: 70.8333 (72.3097)  acc5: 88.5417 (91.2347)  time: 3.4617  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:33  loss: 1.4597 (1.2454)  acc1: 63.5417 (72.0029)  acc5: 87.5000 (91.0699)  time: 3.4577  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:59  loss: 1.5341 (1.2517)  acc1: 63.5417 (71.8776)  acc5: 86.4583 (90.9835)  time: 3.4558  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:24  loss: 1.5402 (1.2589)  acc1: 64.5833 (71.7584)  acc5: 86.4583 (90.8607)  time: 3.4526  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:49  loss: 1.5369 (1.2634)  acc1: 66.6667 (71.7142)  acc5: 86.4583 (90.7908)  time: 3.4477  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:15  loss: 1.4761 (1.2711)  acc1: 66.6667 (71.4811)  acc5: 87.5000 (90.7144)  time: 3.4426  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:40  loss: 1.7677 (1.2832)  acc1: 54.1667 (71.1876)  acc5: 85.4167 (90.5518)  time: 3.4408  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:05  loss: 1.5694 (1.2875)  acc1: 64.5833 (71.0989)  acc5: 86.4583 (90.4980)  time: 3.4422  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:31  loss: 1.4827 (1.2909)  acc1: 65.6250 (70.9779)  acc5: 89.5833 (90.4646)  time: 3.4386  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:02:56  loss: 1.3900 (1.2969)  acc1: 66.6667 (70.8798)  acc5: 88.5417 (90.3640)  time: 3.4363  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:21  loss: 1.3900 (1.3012)  acc1: 68.7500 (70.7445)  acc5: 88.5417 (90.3500)  time: 3.4382  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:47  loss: 1.1481 (1.2965)  acc1: 70.8333 (70.8503)  acc5: 92.7083 (90.4298)  time: 3.4388  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 1.0181 (1.2899)  acc1: 77.0833 (71.0059)  acc5: 94.7917 (90.5023)  time: 3.4381  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:38  loss: 1.1201 (1.2948)  acc1: 70.8333 (70.8313)  acc5: 93.7500 (90.4640)  time: 3.4392  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2441 (1.2857)  acc1: 69.7917 (71.0620)  acc5: 92.7083 (90.5540)  time: 3.4146  data: 0.0001  max mem: 16226
Test: Total time: 0:30:01 (3.4586 s / it)
* Acc@1 71.062 Acc@5 90.554 loss 1.286
Accuracy of the network on the 50000 test images: 71.1%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=5.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 5.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:43  loss: 0.5609 (0.5609)  acc1: 88.5417 (88.5417)  acc5: 96.8750 (96.8750)  time: 5.1509  data: 1.1314  max mem: 16225
Test:  [ 10/521]  eta: 0:30:36  loss: 0.5911 (0.6824)  acc1: 88.5417 (86.2689)  acc5: 97.9167 (97.2538)  time: 3.5933  data: 0.1030  max mem: 16226
Test:  [ 20/521]  eta: 0:29:22  loss: 0.8303 (0.8641)  acc1: 83.3333 (81.3988)  acc5: 95.8333 (95.4861)  time: 3.4370  data: 0.0001  max mem: 16226
Test:  [ 30/521]  eta: 0:28:35  loss: 1.0793 (0.9720)  acc1: 76.0417 (79.0323)  acc5: 91.6667 (94.3884)  time: 3.4377  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:27:53  loss: 1.1479 (1.0430)  acc1: 71.8750 (76.4736)  acc5: 90.6250 (93.9278)  time: 3.4383  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:27:15  loss: 0.7318 (0.9666)  acc1: 87.5000 (78.8399)  acc5: 94.7917 (94.4036)  time: 3.4400  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:38  loss: 0.7186 (0.9557)  acc1: 84.3750 (79.4911)  acc5: 94.7917 (94.3648)  time: 3.4397  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:26:01  loss: 0.8588 (0.9586)  acc1: 83.3333 (79.4454)  acc5: 94.7917 (94.2782)  time: 3.4367  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:25  loss: 0.7205 (0.9322)  acc1: 83.3333 (80.1055)  acc5: 96.8750 (94.5473)  time: 3.4369  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:24:49  loss: 0.9897 (0.9710)  acc1: 79.1667 (78.7660)  acc5: 94.7917 (94.3452)  time: 3.4385  data: 0.0002  max mem: 16226
Test:  [100/521]  eta: 0:24:14  loss: 1.2077 (1.0035)  acc1: 67.7083 (77.7743)  acc5: 93.7500 (94.2657)  time: 3.4377  data: 0.0002  max mem: 16226
Test:  [110/521]  eta: 0:23:39  loss: 1.1525 (1.0101)  acc1: 69.7917 (77.4587)  acc5: 93.7500 (94.2568)  time: 3.4353  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:23:04  loss: 1.0338 (1.0172)  acc1: 75.0000 (77.3244)  acc5: 93.7500 (94.0944)  time: 3.4359  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:29  loss: 1.1276 (1.0255)  acc1: 72.9167 (76.6062)  acc5: 93.7500 (94.1953)  time: 3.4388  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:54  loss: 0.9613 (1.0169)  acc1: 77.0833 (76.7657)  acc5: 95.8333 (94.2967)  time: 3.4384  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:19  loss: 0.9613 (1.0275)  acc1: 78.1250 (76.3728)  acc5: 94.7917 (94.2467)  time: 3.4348  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:44  loss: 1.0806 (1.0187)  acc1: 77.0833 (76.6434)  acc5: 94.7917 (94.3194)  time: 3.4354  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:10  loss: 0.8138 (1.0115)  acc1: 83.3333 (76.8397)  acc5: 94.7917 (94.3896)  time: 3.4364  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:35  loss: 0.8644 (1.0054)  acc1: 82.2917 (77.1006)  acc5: 94.7917 (94.4061)  time: 3.4347  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:19:00  loss: 0.8765 (1.0028)  acc1: 82.2917 (77.1379)  acc5: 94.7917 (94.4699)  time: 3.4336  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:25  loss: 1.0042 (1.0103)  acc1: 77.0833 (76.9641)  acc5: 93.7500 (94.3615)  time: 3.4334  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:51  loss: 1.0042 (1.0090)  acc1: 77.0833 (77.0142)  acc5: 93.7500 (94.3128)  time: 3.4329  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:16  loss: 1.1308 (1.0318)  acc1: 68.7500 (76.4894)  acc5: 91.6667 (94.0422)  time: 3.4333  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:42  loss: 1.2958 (1.0474)  acc1: 65.6250 (76.1454)  acc5: 89.5833 (93.8357)  time: 3.4366  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:07  loss: 1.3756 (1.0656)  acc1: 68.7500 (75.7305)  acc5: 88.5417 (93.5728)  time: 3.4367  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:15:33  loss: 1.4755 (1.0874)  acc1: 66.6667 (75.3569)  acc5: 86.4583 (93.2395)  time: 3.4336  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:14:58  loss: 1.5639 (1.1077)  acc1: 63.5417 (74.8603)  acc5: 84.3750 (92.9757)  time: 3.4331  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:14:24  loss: 1.5080 (1.1290)  acc1: 63.5417 (74.3350)  acc5: 87.5000 (92.7391)  time: 3.4330  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:49  loss: 1.4928 (1.1402)  acc1: 65.6250 (74.1140)  acc5: 88.5417 (92.6045)  time: 3.4356  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:15  loss: 1.4731 (1.1498)  acc1: 68.7500 (73.9154)  acc5: 87.5000 (92.4506)  time: 3.4347  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:12:40  loss: 1.0749 (1.1470)  acc1: 75.0000 (74.0760)  acc5: 91.6667 (92.4315)  time: 3.4333  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:12:06  loss: 1.1039 (1.1623)  acc1: 72.9167 (73.7641)  acc5: 91.6667 (92.2394)  time: 3.4378  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:11:31  loss: 1.3350 (1.1671)  acc1: 68.7500 (73.7604)  acc5: 89.5833 (92.1404)  time: 3.4367  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:10:57  loss: 1.3350 (1.1856)  acc1: 69.7917 (73.3163)  acc5: 89.5833 (91.8901)  time: 3.4336  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:22  loss: 1.6133 (1.1963)  acc1: 60.4167 (73.0786)  acc5: 87.5000 (91.7400)  time: 3.4330  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:48  loss: 1.6133 (1.2053)  acc1: 63.5417 (72.8603)  acc5: 87.5000 (91.6696)  time: 3.4325  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:13  loss: 1.4917 (1.2176)  acc1: 64.5833 (72.5848)  acc5: 86.4583 (91.4935)  time: 3.4329  data: 0.0001  max mem: 16226
Test:  [370/521]  eta: 0:08:39  loss: 1.5688 (1.2251)  acc1: 64.5833 (72.4450)  acc5: 88.5417 (91.4252)  time: 3.4315  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:08:05  loss: 1.3359 (1.2301)  acc1: 72.9167 (72.4163)  acc5: 89.5833 (91.3276)  time: 3.4300  data: 0.0002  max mem: 16226
Test:  [390/521]  eta: 0:07:30  loss: 1.4063 (1.2409)  acc1: 63.5417 (72.0961)  acc5: 88.5417 (91.1765)  time: 3.4350  data: 0.0002  max mem: 16226
Test:  [400/521]  eta: 0:06:56  loss: 1.5077 (1.2468)  acc1: 63.5417 (72.0231)  acc5: 87.5000 (91.0874)  time: 3.4369  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:21  loss: 1.5205 (1.2542)  acc1: 67.7083 (71.8953)  acc5: 86.4583 (90.9393)  time: 3.4353  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:05:47  loss: 1.5168 (1.2587)  acc1: 65.6250 (71.8404)  acc5: 85.4167 (90.8502)  time: 3.4339  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:12  loss: 1.5168 (1.2662)  acc1: 64.5833 (71.6406)  acc5: 86.4583 (90.7845)  time: 3.4305  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:38  loss: 1.7843 (1.2788)  acc1: 56.2500 (71.3624)  acc5: 85.4167 (90.6014)  time: 3.4309  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:04  loss: 1.5821 (1.2834)  acc1: 64.5833 (71.2583)  acc5: 86.4583 (90.5511)  time: 3.4308  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:29  loss: 1.4949 (1.2868)  acc1: 66.6667 (71.1655)  acc5: 87.5000 (90.5120)  time: 3.4312  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:55  loss: 1.4082 (1.2924)  acc1: 66.6667 (71.0545)  acc5: 87.5000 (90.4237)  time: 3.4319  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.3935 (1.2966)  acc1: 70.8333 (70.9373)  acc5: 89.5833 (90.4063)  time: 3.4303  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:46  loss: 1.1256 (1.2922)  acc1: 71.8750 (71.0094)  acc5: 92.7083 (90.4807)  time: 3.4325  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 1.0184 (1.2859)  acc1: 77.0833 (71.1660)  acc5: 94.7917 (90.5439)  time: 3.4335  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1170 (1.2908)  acc1: 73.9583 (70.9923)  acc5: 91.6667 (90.5047)  time: 3.4310  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1631 (1.2819)  acc1: 70.8333 (71.2060)  acc5: 91.6667 (90.5960)  time: 3.4079  data: 0.0001  max mem: 16226
Test: Total time: 0:29:50 (3.4372 s / it)
* Acc@1 71.206 Acc@5 90.596 loss 1.282
Accuracy of the network on the 50000 test images: 71.2%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=6.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 6.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:32  loss: 0.5604 (0.5604)  acc1: 89.5833 (89.5833)  acc5: 97.9167 (97.9167)  time: 5.1287  data: 1.1153  max mem: 16225
Test:  [ 10/521]  eta: 0:30:32  loss: 0.5888 (0.6785)  acc1: 89.5833 (87.1212)  acc5: 97.9167 (97.1591)  time: 3.5864  data: 0.1016  max mem: 16226
Test:  [ 20/521]  eta: 0:29:19  loss: 0.7981 (0.8634)  acc1: 82.2917 (82.2421)  acc5: 95.8333 (95.3373)  time: 3.4310  data: 0.0001  max mem: 16226
Test:  [ 30/521]  eta: 0:28:30  loss: 1.1189 (0.9742)  acc1: 76.0417 (79.1667)  acc5: 91.6667 (94.3548)  time: 3.4287  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:27:49  loss: 1.2236 (1.0479)  acc1: 71.8750 (76.5244)  acc5: 91.6667 (93.9024)  time: 3.4283  data: 0.0001  max mem: 16226
Test:  [ 50/521]  eta: 0:27:11  loss: 0.7425 (0.9719)  acc1: 84.3750 (78.8194)  acc5: 94.7917 (94.3219)  time: 3.4307  data: 0.0001  max mem: 16226
Test:  [ 60/521]  eta: 0:26:34  loss: 0.7385 (0.9634)  acc1: 84.3750 (79.2521)  acc5: 95.8333 (94.2623)  time: 3.4324  data: 0.0001  max mem: 16226
Test:  [ 70/521]  eta: 0:25:57  loss: 0.8669 (0.9645)  acc1: 82.2917 (79.1813)  acc5: 94.7917 (94.2048)  time: 3.4310  data: 0.0002  max mem: 16226
Test:  [ 80/521]  eta: 0:25:22  loss: 0.7299 (0.9381)  acc1: 84.3750 (79.9897)  acc5: 95.8333 (94.4830)  time: 3.4299  data: 0.0002  max mem: 16226
Test:  [ 90/521]  eta: 0:24:46  loss: 0.9863 (0.9758)  acc1: 77.0833 (78.6973)  acc5: 94.7917 (94.2766)  time: 3.4295  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:24:11  loss: 1.2219 (1.0090)  acc1: 66.6667 (77.7125)  acc5: 91.6667 (94.0903)  time: 3.4291  data: 0.0001  max mem: 16226
Test:  [110/521]  eta: 0:23:36  loss: 1.1339 (1.0152)  acc1: 71.8750 (77.3836)  acc5: 93.7500 (94.1254)  time: 3.4320  data: 0.0001  max mem: 16226
Test:  [120/521]  eta: 0:23:01  loss: 1.0553 (1.0212)  acc1: 75.0000 (77.2986)  acc5: 93.7500 (94.0427)  time: 3.4334  data: 0.0001  max mem: 16226
Test:  [130/521]  eta: 0:22:26  loss: 1.1320 (1.0294)  acc1: 73.9583 (76.6937)  acc5: 93.7500 (94.1078)  time: 3.4323  data: 0.0001  max mem: 16226
Test:  [140/521]  eta: 0:21:51  loss: 0.9934 (1.0215)  acc1: 80.2083 (76.8248)  acc5: 95.8333 (94.2228)  time: 3.4317  data: 0.0001  max mem: 16226
Test:  [150/521]  eta: 0:21:16  loss: 0.9662 (1.0317)  acc1: 77.0833 (76.4349)  acc5: 94.7917 (94.2122)  time: 3.4299  data: 0.0001  max mem: 16226
Test:  [160/521]  eta: 0:20:42  loss: 1.1092 (1.0236)  acc1: 75.0000 (76.7145)  acc5: 94.7917 (94.2805)  time: 3.4300  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:07  loss: 0.8170 (1.0157)  acc1: 83.3333 (76.8701)  acc5: 94.7917 (94.3348)  time: 3.4311  data: 0.0001  max mem: 16226
Test:  [180/521]  eta: 0:19:33  loss: 0.8874 (1.0098)  acc1: 81.2500 (77.1121)  acc5: 94.7917 (94.3658)  time: 3.4299  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:18:58  loss: 0.9356 (1.0070)  acc1: 80.2083 (77.1542)  acc5: 94.7917 (94.4263)  time: 3.4290  data: 0.0001  max mem: 16226
Test:  [200/521]  eta: 0:18:23  loss: 0.9577 (1.0143)  acc1: 77.0833 (77.0522)  acc5: 93.7500 (94.3771)  time: 3.4293  data: 0.0001  max mem: 16226
Test:  [210/521]  eta: 0:17:49  loss: 0.9938 (1.0125)  acc1: 78.1250 (77.1031)  acc5: 92.7083 (94.3079)  time: 3.4294  data: 0.0001  max mem: 16226
Test:  [220/521]  eta: 0:17:14  loss: 1.1710 (1.0353)  acc1: 69.7917 (76.5554)  acc5: 90.6250 (94.0234)  time: 3.4291  data: 0.0001  max mem: 16226
Test:  [230/521]  eta: 0:16:40  loss: 1.2742 (1.0507)  acc1: 66.6667 (76.1724)  acc5: 89.5833 (93.8176)  time: 3.4297  data: 0.0001  max mem: 16226
Test:  [240/521]  eta: 0:16:05  loss: 1.4326 (1.0687)  acc1: 67.7083 (75.7002)  acc5: 89.5833 (93.5771)  time: 3.4310  data: 0.0001  max mem: 16226
Test:  [250/521]  eta: 0:15:31  loss: 1.4738 (1.0902)  acc1: 63.5417 (75.3071)  acc5: 86.4583 (93.2312)  time: 3.4295  data: 0.0001  max mem: 16226
Test:  [260/521]  eta: 0:14:57  loss: 1.5830 (1.1099)  acc1: 61.4583 (74.8603)  acc5: 84.3750 (93.0037)  time: 3.4291  data: 0.0001  max mem: 16226
Test:  [270/521]  eta: 0:14:22  loss: 1.5389 (1.1315)  acc1: 61.4583 (74.3273)  acc5: 87.5000 (92.7737)  time: 3.4309  data: 0.0001  max mem: 16226
Test:  [280/521]  eta: 0:13:48  loss: 1.4986 (1.1427)  acc1: 62.5000 (74.0918)  acc5: 89.5833 (92.6416)  time: 3.4305  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:13:13  loss: 1.4717 (1.1525)  acc1: 68.7500 (73.9118)  acc5: 89.5833 (92.4900)  time: 3.4312  data: 0.0001  max mem: 16226
Test:  [300/521]  eta: 0:12:39  loss: 1.0681 (1.1497)  acc1: 73.9583 (74.0483)  acc5: 91.6667 (92.4799)  time: 3.4315  data: 0.0001  max mem: 16226
Test:  [310/521]  eta: 0:12:04  loss: 1.1523 (1.1655)  acc1: 72.9167 (73.7038)  acc5: 91.6667 (92.2796)  time: 3.4300  data: 0.0001  max mem: 16226
Test:  [320/521]  eta: 0:11:30  loss: 1.3438 (1.1701)  acc1: 66.6667 (73.6565)  acc5: 89.5833 (92.1729)  time: 3.4281  data: 0.0001  max mem: 16226
Test:  [330/521]  eta: 0:10:56  loss: 1.3795 (1.1887)  acc1: 66.6667 (73.1558)  acc5: 89.5833 (91.9247)  time: 3.4274  data: 0.0001  max mem: 16226
Test:  [340/521]  eta: 0:10:21  loss: 1.6130 (1.1998)  acc1: 59.3750 (72.8861)  acc5: 86.4583 (91.7644)  time: 3.4278  data: 0.0001  max mem: 16226
Test:  [350/521]  eta: 0:09:47  loss: 1.5800 (1.2091)  acc1: 63.5417 (72.6347)  acc5: 87.5000 (91.6845)  time: 3.4308  data: 0.0002  max mem: 16226
Test:  [360/521]  eta: 0:09:13  loss: 1.5214 (1.2215)  acc1: 64.5833 (72.3886)  acc5: 88.5417 (91.4791)  time: 3.4319  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:08:38  loss: 1.4960 (1.2281)  acc1: 65.6250 (72.2765)  acc5: 88.5417 (91.4252)  time: 3.4303  data: 0.0001  max mem: 16226
Test:  [380/521]  eta: 0:08:04  loss: 1.3535 (1.2328)  acc1: 71.8750 (72.2605)  acc5: 89.5833 (91.3167)  time: 3.4299  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:07:29  loss: 1.4171 (1.2435)  acc1: 65.6250 (71.9629)  acc5: 88.5417 (91.1392)  time: 3.4282  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:06:55  loss: 1.5310 (1.2495)  acc1: 65.6250 (71.9036)  acc5: 85.4167 (91.0588)  time: 3.4279  data: 0.0001  max mem: 16226
Test:  [410/521]  eta: 0:06:21  loss: 1.5356 (1.2563)  acc1: 66.6667 (71.7838)  acc5: 85.4167 (90.9165)  time: 3.4307  data: 0.0001  max mem: 16226
Test:  [420/521]  eta: 0:05:46  loss: 1.5291 (1.2606)  acc1: 67.7083 (71.7612)  acc5: 85.4167 (90.8526)  time: 3.4299  data: 0.0001  max mem: 16226
Test:  [430/521]  eta: 0:05:12  loss: 1.5291 (1.2683)  acc1: 66.6667 (71.5463)  acc5: 87.5000 (90.7748)  time: 3.4299  data: 0.0001  max mem: 16226
Test:  [440/521]  eta: 0:04:38  loss: 1.7879 (1.2805)  acc1: 56.2500 (71.2538)  acc5: 84.3750 (90.6014)  time: 3.4336  data: 0.0001  max mem: 16226
Test:  [450/521]  eta: 0:04:03  loss: 1.5549 (1.2849)  acc1: 67.7083 (71.1659)  acc5: 86.4583 (90.5511)  time: 3.4308  data: 0.0001  max mem: 16226
Test:  [460/521]  eta: 0:03:29  loss: 1.4696 (1.2885)  acc1: 68.7500 (71.0977)  acc5: 88.5417 (90.5098)  time: 3.4287  data: 0.0001  max mem: 16226
Test:  [470/521]  eta: 0:02:55  loss: 1.4234 (1.2944)  acc1: 69.7917 (70.9815)  acc5: 88.5417 (90.4215)  time: 3.4298  data: 0.0001  max mem: 16226
Test:  [480/521]  eta: 0:02:20  loss: 1.4163 (1.2991)  acc1: 67.7083 (70.8463)  acc5: 88.5417 (90.3846)  time: 3.4283  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:46  loss: 1.1748 (1.2947)  acc1: 70.8333 (70.9203)  acc5: 91.6667 (90.4616)  time: 3.4277  data: 0.0001  max mem: 16226
Test:  [500/521]  eta: 0:01:12  loss: 0.9975 (1.2884)  acc1: 73.9583 (71.0641)  acc5: 94.7917 (90.5210)  time: 3.4296  data: 0.0001  max mem: 16226
Test:  [510/521]  eta: 0:00:37  loss: 1.1280 (1.2931)  acc1: 72.9167 (70.9128)  acc5: 92.7083 (90.4843)  time: 3.4292  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.2349 (1.2842)  acc1: 69.7917 (71.1300)  acc5: 91.6667 (90.5660)  time: 3.4047  data: 0.0001  max mem: 16226
Test: Total time: 0:29:48 (3.4325 s / it)
* Acc@1 71.130 Acc@5 90.566 loss 1.284
Accuracy of the network on the 50000 test images: 71.1%
Not using distributed mode
Namespace(batch_size=64, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', attn_only=False, data_path='/home/usr1/zixuan/ImageNet/data', data_set='IMNET', inat_category='name', output_dir='', device='cuda', seed=3, resume='./results/deit_tiny_4bit/4w4a_bs512_baselr5e-4_weightdecay1e-8_ft300_headwise1_noise_i_0.015_o_0.025_linear_noise/best_checkpoint.pth', start_epoch=0, eval=True, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=4, abits=4, headwise=True, offset=False, input_noise_std=0.03, output_noise_std=0.05, phase_noise_std=7.0, enable_wdm_noise=True, enable_linear_noise=True, num_wavelength=12, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)
3
Creating model: deit_tiny_patch16_224_quant
Noise param:
** Enable linear noise True
** Input noise std 0.03
** Output noise std 0.05
** Phase noise std 7.0
** Enable WDM noise for coupler True
** Num of wavelength 12
** Channel spacing 0.4
Use 4 bit weights.
Use 4 bit activations.
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int4 quantization
Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization
number of params: 5717696
Test:  [  0/521]  eta: 0:44:32  loss: 0.5592 (0.5592)  acc1: 90.6250 (90.6250)  acc5: 97.9167 (97.9167)  time: 5.1294  data: 1.1732  max mem: 16225
Test:  [ 10/521]  eta: 0:30:29  loss: 0.6002 (0.6853)  acc1: 88.5417 (86.7424)  acc5: 97.9167 (96.9697)  time: 3.5804  data: 0.1068  max mem: 16226
Test:  [ 20/521]  eta: 0:29:17  loss: 0.8459 (0.8675)  acc1: 80.2083 (81.2996)  acc5: 95.8333 (95.1885)  time: 3.4265  data: 0.0001  max mem: 16226
Test:  [ 30/521]  eta: 0:28:29  loss: 1.0800 (0.9764)  acc1: 75.0000 (78.8642)  acc5: 91.6667 (93.9180)  time: 3.4287  data: 0.0001  max mem: 16226
Test:  [ 40/521]  eta: 0:27:48  loss: 1.1696 (1.0470)  acc1: 71.8750 (76.2957)  acc5: 91.6667 (93.5976)  time: 3.4283  data: 0.0002  max mem: 16226
Test:  [ 50/521]  eta: 0:27:09  loss: 0.7375 (0.9716)  acc1: 85.4167 (78.6152)  acc5: 95.8333 (94.0972)  time: 3.4263  data: 0.0002  max mem: 16226
Test:  [ 60/521]  eta: 0:26:32  loss: 0.7146 (0.9590)  acc1: 85.4167 (79.3545)  acc5: 95.8333 (94.0915)  time: 3.4269  data: 0.0002  max mem: 16226
Test:  [ 70/521]  eta: 0:25:56  loss: 0.8503 (0.9595)  acc1: 82.2917 (79.3867)  acc5: 94.7917 (94.0141)  time: 3.4278  data: 0.0001  max mem: 16226
Test:  [ 80/521]  eta: 0:25:20  loss: 0.7206 (0.9339)  acc1: 84.3750 (80.0540)  acc5: 95.8333 (94.3030)  time: 3.4271  data: 0.0001  max mem: 16226
Test:  [ 90/521]  eta: 0:24:45  loss: 1.0096 (0.9735)  acc1: 76.0417 (78.7202)  acc5: 95.8333 (94.0591)  time: 3.4281  data: 0.0001  max mem: 16226
Test:  [100/521]  eta: 0:24:10  loss: 1.2061 (1.0060)  acc1: 67.7083 (77.6506)  acc5: 92.7083 (93.9253)  time: 3.4284  data: 0.0001  max mem: 16226
Test:  [110/521]  eta: 0:23:34  loss: 1.1240 (1.0115)  acc1: 68.7500 (77.4681)  acc5: 93.7500 (94.0128)  time: 3.4271  data: 0.0002  max mem: 16226
Test:  [120/521]  eta: 0:23:00  loss: 1.0205 (1.0185)  acc1: 78.1250 (77.2986)  acc5: 94.7917 (93.8964)  time: 3.4276  data: 0.0002  max mem: 16226
Test:  [130/521]  eta: 0:22:25  loss: 1.1531 (1.0264)  acc1: 72.9167 (76.6619)  acc5: 94.7917 (93.9886)  time: 3.4284  data: 0.0002  max mem: 16226
Test:  [140/521]  eta: 0:21:50  loss: 1.0024 (1.0183)  acc1: 77.0833 (76.7878)  acc5: 95.8333 (94.0972)  time: 3.4282  data: 0.0002  max mem: 16226
Test:  [150/521]  eta: 0:21:15  loss: 0.9767 (1.0284)  acc1: 79.1667 (76.4625)  acc5: 94.7917 (94.1087)  time: 3.4297  data: 0.0002  max mem: 16226
Test:  [160/521]  eta: 0:20:41  loss: 1.0476 (1.0193)  acc1: 79.1667 (76.8051)  acc5: 94.7917 (94.1770)  time: 3.4296  data: 0.0001  max mem: 16226
Test:  [170/521]  eta: 0:20:06  loss: 0.7978 (1.0114)  acc1: 83.3333 (76.9920)  acc5: 94.7917 (94.2251)  time: 3.4302  data: 0.0002  max mem: 16226
Test:  [180/521]  eta: 0:19:32  loss: 0.8443 (1.0057)  acc1: 81.2500 (77.2560)  acc5: 94.7917 (94.2564)  time: 3.4339  data: 0.0001  max mem: 16226
Test:  [190/521]  eta: 0:19:14  loss: 0.9179 (1.0037)  acc1: 81.2500 (77.2851)  acc5: 94.7917 (94.2845)  time: 3.9253  data: 0.0002  max mem: 16226
Test:  [200/521]  eta: 0:18:51  loss: 0.9671 (1.0107)  acc1: 76.0417 (77.1041)  acc5: 93.7500 (94.2475)  time: 4.3114  data: 0.0002  max mem: 16226
Test:  [210/521]  eta: 0:18:29  loss: 1.0170 (1.0095)  acc1: 76.0417 (77.1278)  acc5: 93.7500 (94.1993)  time: 4.3275  data: 0.0002  max mem: 16226
Test:  [220/521]  eta: 0:18:02  loss: 1.1627 (1.0327)  acc1: 71.8750 (76.5979)  acc5: 90.6250 (93.9197)  time: 4.3275  data: 0.0009  max mem: 16226
Test:  [230/521]  eta: 0:17:35  loss: 1.3315 (1.0487)  acc1: 65.6250 (76.2220)  acc5: 89.5833 (93.7094)  time: 4.2266  data: 0.0010  max mem: 16226
Test:  [240/521]  eta: 0:17:07  loss: 1.4060 (1.0665)  acc1: 65.6250 (75.7650)  acc5: 88.5417 (93.4734)  time: 4.3293  data: 0.0002  max mem: 16226
Test:  [250/521]  eta: 0:16:37  loss: 1.5000 (1.0884)  acc1: 64.5833 (75.3652)  acc5: 86.4583 (93.1067)  time: 4.3141  data: 0.0002  max mem: 16226
Test:  [260/521]  eta: 0:16:08  loss: 1.5435 (1.1093)  acc1: 62.5000 (74.8723)  acc5: 85.4167 (92.8440)  time: 4.3337  data: 0.0002  max mem: 16226
Test:  [270/521]  eta: 0:15:35  loss: 1.5307 (1.1303)  acc1: 61.4583 (74.3504)  acc5: 86.4583 (92.5930)  time: 4.3326  data: 0.0002  max mem: 16226
Test:  [280/521]  eta: 0:15:03  loss: 1.4678 (1.1416)  acc1: 64.5833 (74.1214)  acc5: 86.4583 (92.4451)  time: 4.2407  data: 0.0001  max mem: 16226
Test:  [290/521]  eta: 0:14:30  loss: 1.4555 (1.1511)  acc1: 68.7500 (73.9190)  acc5: 86.4583 (92.2895)  time: 4.3386  data: 0.0002  max mem: 16226
Test:  [300/521]  eta: 0:13:54  loss: 1.0831 (1.1488)  acc1: 71.8750 (74.0552)  acc5: 91.6667 (92.2723)  time: 4.1716  data: 0.0002  max mem: 16226
Test:  [310/521]  eta: 0:13:14  loss: 1.1341 (1.1645)  acc1: 71.8750 (73.7172)  acc5: 91.6667 (92.0719)  time: 3.6934  data: 0.0002  max mem: 16226
Test:  [320/521]  eta: 0:12:34  loss: 1.3448 (1.1685)  acc1: 67.7083 (73.6533)  acc5: 89.5833 (92.0042)  time: 3.4511  data: 0.0002  max mem: 16226
Test:  [330/521]  eta: 0:11:55  loss: 1.3503 (1.1872)  acc1: 67.7083 (73.1653)  acc5: 90.6250 (91.7642)  time: 3.4518  data: 0.0002  max mem: 16226
Test:  [340/521]  eta: 0:11:16  loss: 1.6155 (1.1983)  acc1: 59.3750 (72.9075)  acc5: 85.4167 (91.6086)  time: 3.4558  data: 0.0002  max mem: 16226
Test:  [350/521]  eta: 0:10:37  loss: 1.6155 (1.2073)  acc1: 61.4583 (72.6703)  acc5: 85.4167 (91.5183)  time: 3.4585  data: 0.0001  max mem: 16226
Test:  [360/521]  eta: 0:09:59  loss: 1.5002 (1.2194)  acc1: 63.5417 (72.4088)  acc5: 86.4583 (91.3320)  time: 3.4618  data: 0.0002  max mem: 16226
Test:  [370/521]  eta: 0:09:21  loss: 1.5377 (1.2270)  acc1: 63.5417 (72.2793)  acc5: 88.5417 (91.2652)  time: 3.4617  data: 0.0002  max mem: 16226
Test:  [380/521]  eta: 0:08:42  loss: 1.3601 (1.2322)  acc1: 72.9167 (72.2496)  acc5: 89.5833 (91.1718)  time: 3.4584  data: 0.0001  max mem: 16226
Test:  [390/521]  eta: 0:08:05  loss: 1.4655 (1.2431)  acc1: 64.5833 (71.9523)  acc5: 87.5000 (91.0060)  time: 3.4566  data: 0.0001  max mem: 16226
Test:  [400/521]  eta: 0:07:27  loss: 1.5416 (1.2493)  acc1: 66.6667 (71.8698)  acc5: 86.4583 (90.8978)  time: 3.4549  data: 0.0002  max mem: 16226
Test:  [410/521]  eta: 0:06:49  loss: 1.5393 (1.2564)  acc1: 67.7083 (71.7407)  acc5: 85.4167 (90.7593)  time: 3.4513  data: 0.0002  max mem: 16226
Test:  [420/521]  eta: 0:06:12  loss: 1.5054 (1.2602)  acc1: 66.6667 (71.7142)  acc5: 85.4167 (90.6992)  time: 3.4471  data: 0.0002  max mem: 16226
Test:  [430/521]  eta: 0:05:34  loss: 1.5054 (1.2679)  acc1: 66.6667 (71.5076)  acc5: 87.5000 (90.6153)  time: 3.4470  data: 0.0002  max mem: 16226
Test:  [440/521]  eta: 0:04:57  loss: 1.7397 (1.2806)  acc1: 56.2500 (71.2207)  acc5: 83.3333 (90.4313)  time: 3.4465  data: 0.0002  max mem: 16226
Test:  [450/521]  eta: 0:04:20  loss: 1.6194 (1.2849)  acc1: 64.5833 (71.1290)  acc5: 84.3750 (90.3686)  time: 3.4453  data: 0.0002  max mem: 16226
Test:  [460/521]  eta: 0:03:43  loss: 1.4777 (1.2884)  acc1: 66.6667 (71.0209)  acc5: 87.5000 (90.3313)  time: 3.4442  data: 0.0002  max mem: 16226
Test:  [470/521]  eta: 0:03:06  loss: 1.4058 (1.2946)  acc1: 69.7917 (70.8997)  acc5: 88.5417 (90.2402)  time: 3.4417  data: 0.0002  max mem: 16226
Test:  [480/521]  eta: 0:02:29  loss: 1.4058 (1.2992)  acc1: 70.8333 (70.7619)  acc5: 89.5833 (90.2027)  time: 3.4401  data: 0.0001  max mem: 16226
Test:  [490/521]  eta: 0:01:53  loss: 1.1298 (1.2951)  acc1: 70.8333 (70.8503)  acc5: 92.7083 (90.2686)  time: 3.4397  data: 0.0002  max mem: 16226
Test:  [500/521]  eta: 0:01:16  loss: 1.0452 (1.2885)  acc1: 77.0833 (71.0038)  acc5: 94.7917 (90.3339)  time: 3.4380  data: 0.0002  max mem: 16226
Test:  [510/521]  eta: 0:00:40  loss: 1.1347 (1.2937)  acc1: 71.8750 (70.8130)  acc5: 91.6667 (90.2907)  time: 3.4347  data: 0.0001  max mem: 16226
Test:  [520/521]  eta: 0:00:03  loss: 1.1965 (1.2848)  acc1: 68.7500 (71.0380)  acc5: 90.6250 (90.3780)  time: 3.4117  data: 0.0001  max mem: 16226
Test: Total time: 0:31:34 (3.6369 s / it)
* Acc@1 71.038 Acc@5 90.378 loss 1.285
Accuracy of the network on the 50000 test images: 71.0%
